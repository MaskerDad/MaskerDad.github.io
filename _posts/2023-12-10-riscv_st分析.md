---
title: riscv_st分析

date: 2023-12-10 17:00:00 +0800

categories: [linux内核学习, kvm]

tags: [virt, qemu, kvm]

description: 
---

# 0 资料

 https://github.com/riscv-non-isa/riscv-sbidoc/blob/master/src/ext-steal-time.adoc

https://github.com/jones-drew/linux/commits/kvm/steal-time-v2

[[PATCH v3 00/13\] RISC-V: Add steal-time support (kernel.org)](https://lore.kernel.org/linux-riscv/20231217204019.36492-15-ajones@ventanamicro.com/T/#mc1ddddb6acf9a7aac6375ed873a8c10b556a7497)

[opensbi入门 - LightningStar - 博客园 (cnblogs.com)](https://www.cnblogs.com/harrypotterjackson/p/17558399.html)

[【RISC-V 理论篇】SBI 规范解读（上）-CSDN博客](https://blog.csdn.net/BillyThe/article/details/129195552)

[RISCV基础开发（四） | 南京养鸡二厂 (databusworld.cn)](http://www.databusworld.cn/10507.html)



# 1 关于steal-time

## 1.1 CPU超分配与超卖

虚拟化的技能之一就是随心所欲的操控CPU, 比如，在一台服务器有2个CPU，每CPU 4个计算核心，虚拟化能在这台服务器上创建10台甚至20台8个计算核心的虚拟服务器。想象一下，原来一台服务器给1个系统使用，现在一台服务器可以给10个甚至20个系统使用，这台服务器的使用率不高才怪。如果我们把物理服务器想象成一条公路，以前这条路上只有一辆车，虚拟化其实并没有拓宽公路，只是允许更多的车能在这条公路上同时行进，但是，如果这条公路的车太多了，出现拥堵也就是很正常的事情。

公有云卖通用型云服务器的时候，一般都会超卖CPU，比如一台物理服务器有32个物理核心，在这台服务器上可能会最多创建128个1 VCPU的虚拟机，这时候的CPU超卖比就是1：4。一般来说，同厂商同规格的云服务器越便宜，超卖比越高。

> ***CPU超分配会带来什么影响？***

如果物理服务器的资源还有大量闲置，CPU超分配一般不会对运行在虚拟机中的业务产生明显的影响，但是如果这些超分配的VCPU大部分都处于高压力状态，物理服务器的PCPU已经接近饱和，那么各虚拟机的VCPU就要相互竞争，相互等待，从而各虚拟机中业务的延时增加，吞吐量下降。

各云服务商肯定不会对外公布他们的的CPU超卖方案，但是对于买家来讲，我们肯定不希望选到一个云服务器，结果它成天都在和其它的云服务器之间成天相互竞争，相互伤害，影响业务。

## 1.2 CPU Steal Time

Linux的top和iostat命令，提供了Steal Time （st） 指标，用来衡量被Hypervisor偷去给其它虚拟机使用的CPU时间所占的比例，这个值越高，说明这台物理服务器的资源竞争越激烈，购买需谨慎。

![img](https://pic4.zhimg.com/v2-b9d6adf321765821baa4c89343f183d7_b.jpg)

![img](https://pic1.zhimg.com/v2-0d3dc26b1496206e9d5139e26a198408_b.jpg)

# 2 SBI STA Extension

 https://github.com/riscv-non-isa/riscv-sbidoc/blob/master/src/ext-steal-time.adoc

| Function Name            | SBI Version | FID  | EID      |
| ------------------------ | ----------- | ---- | -------- |
| sbi_steal_time_set_shmem | 2.0         | 0    | 0x535441 |

```c
struct sbiret sbi_steal_time_set_shmem(unsigned long shmem_phys_lo,
                                       unsigned long shmem_phys_hi,
                                       unsigned long flags)
```

 STA Shared Memory Structure：

| Name      | Offset | Size | Description                                                  |
| --------- | ------ | ---- | ------------------------------------------------------------ |
| sequence  | 0      | 4    | The SBI implementation MUST increment this field to an odd value before writing the `steal` field, and increment it again to an even value after writing `steal` (i.e. an odd sequence number indicates an in-progress update). The SBI implementation SHOULD ensure that the sequence field remains odd for only very short periods of time.  The supervisor-mode software MUST check this field before and after reading the `steal` field, and repeat the read if it is different or odd.  *This sequence field enables the value of the steal field to be read by supervisor-mode software executing in a 32-bit environment.* |
| flags     | 4      | 4    | Always zero.  *Future extensions of the SBI call might allow the supervisor-mode software to write to some of the fields of the shared memory. Such extensions will not be enabled as long as a zero value is used for the flags argument to the SBI call.* |
| steal     | 8      | 8    | The amount of time in which this virtual hart was not idle and scheduled out, in nanoseconds. The time during which the virtual hart is idle will not be reported as steal-time. |
| preempted | 16     | 1    | An advisory flag indicating whether the virtual hart which registered this structure is running or not. A non-zero value MAY be written by the SBI implementation if the virtual hart has been preempted (i.e. while the `steal` field is increasing), while a zero value MUST be written before the virtual hart starts to run again.  *This preempted field can, for example, be used by the supervisor-mode software to check if a lock holder has been preempted, and, in that case, disable optimistic spinning.* |
| pad       | 17     | 47   | Pad with zeros to a 64 byte boundary.                        |

`sbiret.value` is set to zero and the possible error codes returned in `sbiret.error` are shown in below.

| Error code              | Description                                                  |
| ----------------------- | ------------------------------------------------------------ |
| SBI_SUCCESS             | The steal-time shared memory physical base address was set or cleared successfully. |
| SBI_ERR_INVALID_PARAM   | The `flags` parameter is not zero or the `shmem_phys_lo` is not 64-byte aligned. |
| SBI_ERR_INVALID_ADDRESS | The shared memory pointed to by the `shmem_phys_lo` and `shmem_phys_hi` parameters is not writable or does not satisfy other requirements of [[_shared_memory_physical_address_range_parameter\]](https://github.com/riscv-non-isa/riscv-sbi-doc/blob/master/src/ext-steal-time.adoc#_shared_memory_physical_address_range_parameter). |
| SBI_ERR_FAILED          | The request failed for unspecified or unknown other reasons. |

# 3 linux riscv对ST的支持

## 3.1 概述







## 3.2 Add paravirt support to riscv

### Add skeleton for pv-time support

```c
// arch/riscv/kernel/paravirt.c

struct static_key paravirt_steal_enabled;
struct static_key paravirt_steal_rq_enabled;

DEFINE_STATIC_CALL(pv_steal_clock, native_steal_clock);
static bool steal_acc = true;

static u64 native_steal_clock(int cpu);
static int __init parse_no_stealacc(char *arg)
{
	steal_acc = false;
	return 0;
}

early_param("no-steal-acc", parse_no_stealacc);

static bool __init has_pv_steal_clock(void);
static int pv_time_cpu_online(unsigned int cpu);
static int pv_time_cpu_down_prepare(unsigned int cpu);
static u64 pv_time_steal_clock(int cpu);

+int __init pv_time_init(void)
+{
+	int ret;
+
+	if (!has_pv_steal_clock())
+		return 0;
+
+	ret = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN,
+				"riscv/pv_time:online",
+				pv_time_cpu_online,
+				pv_time_cpu_down_prepare);
+	if (ret < 0)
+		return ret;
+
+	static_call_update(pv_steal_clock, pv_time_steal_clock);
+
+	static_key_slow_inc(&paravirt_steal_enabled);
+	if (steal_acc)
+		static_key_slow_inc(&paravirt_steal_rq_enabled);
+
+	pr_info("using paravirt steal-time\n");
+
+	return 0;
+}

void __init time_init(void)
 	timer_probe();
 
 	tick_setup_hrtimer_broadcast();
+
+	pv_time_init();
 }
```



### Add SBI STA extension definitions

 https://github.com/riscv-non-isa/riscv-sbidoc/blob/master/src/ext-steal-time.adoc

```c
//arch/riscv/include/asm/sbi.h

@@ -31,6 +31,7 @@ enum sbi_ext_id {
 	SBI_EXT_SRST = 0x53525354,
 	SBI_EXT_PMU = 0x504D55,
 	SBI_EXT_DBCN = 0x4442434E,
+	SBI_EXT_STA = 0x535441,
 
 	/* Experimentals extensions must lie within this range */
 	SBI_EXT_EXPERIMENTAL_START = 0x08000000,
@@ -243,6 +244,22 @@ enum sbi_ext_dbcn_fid {
 	SBI_EXT_DBCN_CONSOLE_WRITE_BYTE = 2,
 };
 
+/* SBI STA (steal-time accounting) extension */
+enum sbi_ext_sta_fid {
+	SBI_EXT_STA_STEAL_TIME_SET_SHMEM = 0,
+};
+
+struct sbi_sta_struct {
+	__le32 sequence;
+	__le32 flags;
+	__le64 steal;
+	u8 preempted;
+	u8 pad[47];
+} __packed;
+
+#define SBI_STA_SHMEM_DISABLE		-1

```



### Implement steal-time support

当存在SBI STA扩展时，我们可以使用它来实现虚拟化偷取时间的支持。使用SBI STA实现填充空的pv-time函数，并添加Kconfig开关以允许其启用。

```c
static u64 pv_time_steal_clock(int cpu)
 {
-	return 0;
+	struct sbi_sta_struct *st = per_cpu_ptr(&steal_time, cpu);
+	u32 sequence;
+	u64 steal;
+
+	/*
+	 * Check the sequence field before and after reading the steal
+	 * field. Repeat the read if it is different or odd.
+	 */
+	do {
+		sequence = READ_ONCE(st->sequence);
+		virt_rmb();
+		steal = READ_ONCE(st->steal);
+		virt_rmb();
+	} while ((le32_to_cpu(sequence) & 1) ||
+		 sequence != READ_ONCE(st->sequence));
+
+	return le64_to_cpu(steal);
 }
```



## 3.3 Implement SBI STA in KVM

### Add SBI STA extension skeleton

```c
// arch/riscv/include/asm/kvm_vcpu_sbi.h
extern const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_sta;

// arch/riscv/include/uapi/asm/kvm.h
enum KVM_RISCV_SBI_EXT_ID {
	KVM_RISCV_SBI_EXT_STA,
};

// arch/riscv/kvm/Makefile
kvm-y += vcpu_sbi_sta.o

// arch/riscv/kvm/vcpu_sbi.c
static const struct kvm_riscv_sbi_extension_entry sbi_ext[] = {
	{
		.ext_idx = KVM_RISCV_SBI_EXT_STA,
		.ext_ptr = &vcpu_sbi_ext_sta,
	},
}

// arch/riscv/kvm/vcpu_sbi_sta.c
static int kvm_sbi_sta_steal_time_set_shmem(struct kvm_vcpu *vcpu)
{
	//...
}

static int kvm_sbi_ext_sta_handler(struct kvm_vcpu *vcpu, struct kvm_run *run,
				   struct kvm_vcpu_sbi_return *retdata)
{
	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
	unsigned long funcid = cp->a6;
	int ret;

	switch (funcid) {
	case SBI_EXT_STA_STEAL_TIME_SET_SHMEM:
		ret = kvm_sbi_sta_steal_time_set_shmem(vcpu);
		break;
	default:
		ret = SBI_ERR_NOT_SUPPORTED;
		break;
	}

	retdata->err_val = ret;

	return 0;
}

static unsigned long kvm_sbi_ext_sta_probe(struct kvm_vcpu *vcpu)
{
	//...
}  
    
const struct kvm_vcpu_sbi_extension vcpu_sbi_ext_sta = {
	.extid_start = SBI_EXT_STA,
	.extid_end = SBI_EXT_STA,
	.handler = kvm_sbi_ext_sta_handler,
	.probe = kvm_sbi_ext_sta_probe,
};
```



### Add steal-update vcpu request

添加一个新的vcpu请求，通知vcpu应记录其偷取时间信息。每次检测到vcpu任务在一段时间内未分配cpu时，通过从vcpu-load发出请求很容易实现。目前，记录函数仅是一个存根，并将在后续补丁中填充其余的偷取时间支持函数。

```c
// arch/riscv/include/asm/kvm_host.h
#define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(6)

void kvm_riscv_vcpu_record_steal_time(struct kvm_vcpu *vcpu);

// arch/riscv/kvm/vcpu.c
void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
{
	kvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);
}

static void kvm_riscv_check_vcpu_requests(struct kvm_vcpu *vcpu)
{
    if (kvm_check_request(KVM_REQ_STEAL_UPDATE, vcpu))
        kvm_riscv_vcpu_record_steal_time(vcpu);
}


```

> kvm_guest 每次更新st的时机：vcpu_load函数会发起st更新请求，在guest enter之前调用kvm_riscv_vcpu_record_steal_time函数更新。



### Add SBI STA info to vcpu_arch

KVM的SBI STA实现需要跟踪每个VCPU的偷取时间共享内存区域的地址以及被偷取的时间量。在vcpu_arch中添加一个结构来包含这个状态，并确保在vcpu重置时地址始终设置为INVALID_GPA。当共享内存地址无效时，当然要确保KVM不会尝试更新偷取时间。

```c
// arch/riscv/include/asm/kvm_host.h
struct kvm_vcpu_arch {
	/* SBI steal-time accounting */
	struct {
		gpa_t shmem;
		u64 last_steal;
	} sta;
};

void kvm_riscv_vcpu_sbi_sta_reset(struct kvm_vcpu *vcpu);

// arch/riscv/kvm/vcpu.c
static void kvm_riscv_reset_vcpu(struct kvm_vcpu *vcpu)
{
    kvm_riscv_vcpu_sbi_sta_reset(vcpu);
}

// arch/riscv/kvm/vcpu_sbi_sta.c
void kvm_riscv_vcpu_sbi_sta_reset(struct kvm_vcpu *vcpu)
{
	vcpu->arch.sta.shmem = INVALID_GPA;
	vcpu->arch.sta.last_steal = 0;
}

void kvm_riscv_vcpu_record_steal_time(struct kvm_vcpu *vcpu)
{
	gpa_t shmem = vcpu->arch.sta.shmem;

	if (shmem == INVALID_GPA)
		return;
}

```

> vcpu结构体中新增了sta来记录guest st信息，包括：shmem为host与guest的一块共享内存，last_steal为上一次的st (为什么要记录这个？)



### Add support for SBI extension registers

一些SBI扩展具有在迁移虚拟机时需要保存/恢复的状态。为SBI扩展寄存器提供一个get/set-one-reg寄存器类型。使用此类型的每个SBI扩展将具有自己的子类型。目前尚未定义任何子类型。下一个补丁引入了第一个子类型。

```c
// arch/riscv/include/asm/kvm_vcpu_sbi.h
int kvm_riscv_vcpu_set_reg_sbi(struct kvm_vcpu *vcpu,
			       const struct kvm_one_reg *reg);
int kvm_riscv_vcpu_get_reg_sbi(struct kvm_vcpu *vcpu,
			       const struct kvm_one_reg *reg);

// arch/riscv/include/uapi/asm/kvm.h
/* Registers for specific SBI extensions are mapped as type 10 */
#define KVM_REG_RISCV_SBI		(0x0a << KVM_REG_RISCV_TYPE_SHIFT)

// arch/riscv/kvm/vcpu_onereg.c
+static inline unsigned long num_sbi_regs(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
+static int copy_sbi_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)
+{
+	int n = num_sbi_regs(vcpu);
+
+	for (int i = 0; i < n; i++) {
+		u64 reg = KVM_REG_RISCV | KVM_REG_SIZE_U64 |
+			  KVM_REG_RISCV_SBI | i;
+
+		if (uindices) {
+			if (put_user(reg, uindices))
+				return -EFAULT;
+			uindices++;
+		}
+	}
+
+	return n;
+}

unsigned long kvm_riscv_vcpu_num_regs(struct kvm_vcpu *vcpu)
{
 	res += num_vector_regs(vcpu);
 	res += num_isa_ext_regs(vcpu);
 	res += num_sbi_ext_regs(vcpu);
+	res += num_sbi_regs(vcpu);
 
 	return res;
}

int kvm_riscv_vcpu_copy_reg_indices(struct kvm_vcpu *vcpu, ...)
{
 	ret = copy_sbi_ext_reg_indices(vcpu, uindices);
 	if (ret < 0)
 		return ret;
+	uindices += ret;
+
+	ret = copy_sbi_reg_indices(vcpu, uindices);
+	if (ret < 0)
+		return ret;
+	uindices += ret;
 
 	return 0;
 }

int kvm_riscv_vcpu_set_reg(struct kvm_vcpu *vcpu,
 	case KVM_REG_RISCV_FP_D:
 		return kvm_riscv_vcpu_set_reg_fp(vcpu, reg,
 						 KVM_REG_RISCV_FP_D);
+	case KVM_REG_RISCV_VECTOR:
+		return kvm_riscv_vcpu_set_reg_vector(vcpu, reg);
 	case KVM_REG_RISCV_ISA_EXT:
 		return kvm_riscv_vcpu_set_reg_isa_ext(vcpu, reg);
 	case KVM_REG_RISCV_SBI_EXT:
 		return kvm_riscv_vcpu_set_reg_sbi_ext(vcpu, reg);
-	case KVM_REG_RISCV_VECTOR:
-		return kvm_riscv_vcpu_set_reg_vector(vcpu, reg);
+	case KVM_REG_RISCV_SBI:
+		return kvm_riscv_vcpu_set_reg_sbi(vcpu, reg);
 	default:
 		break;
 	}
                           
int kvm_riscv_vcpu_get_reg(struct kvm_vcpu *vcpu,
 	case KVM_REG_RISCV_FP_D:
 		return kvm_riscv_vcpu_get_reg_fp(vcpu, reg,
 						 KVM_REG_RISCV_FP_D);
+	case KVM_REG_RISCV_VECTOR:
+		return kvm_riscv_vcpu_get_reg_vector(vcpu, reg);
 	case KVM_REG_RISCV_ISA_EXT:
 		return kvm_riscv_vcpu_get_reg_isa_ext(vcpu, reg);
 	case KVM_REG_RISCV_SBI_EXT:
 		return kvm_riscv_vcpu_get_reg_sbi_ext(vcpu, reg);
-	case KVM_REG_RISCV_VECTOR:
-		return kvm_riscv_vcpu_get_reg_vector(vcpu, reg);
+	case KVM_REG_RISCV_SBI:
+		return kvm_riscv_vcpu_get_reg_sbi(vcpu, reg);
 	default:
 		break;
 	}

// arch/riscv/kvm/vcpu_sbi.c
+int kvm_riscv_vcpu_set_reg_sbi(struct kvm_vcpu *vcpu,
+			       const struct kvm_one_reg *reg)
+{
+	unsigned long __user *uaddr =
+			(unsigned long __user *)(unsigned long)reg->addr;
+	unsigned long reg_num = reg->id & ~(KVM_REG_ARCH_MASK |
+					    KVM_REG_SIZE_MASK |
+					    KVM_REG_RISCV_SBI);
+	unsigned long reg_subtype, reg_val;
+
+	if (KVM_REG_SIZE(reg->id) != sizeof(unsigned long))
+		return -EINVAL;
+
+	if (copy_from_user(&reg_val, uaddr, KVM_REG_SIZE(reg->id)))
+		return -EFAULT;
+
+	reg_subtype = reg_num & KVM_REG_RISCV_SUBTYPE_MASK;
+	reg_num &= ~KVM_REG_RISCV_SUBTYPE_MASK;
+
+	switch (reg_subtype) {
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
                           
+int kvm_riscv_vcpu_get_reg_sbi(struct kvm_vcpu *vcpu,
+			       const struct kvm_one_reg *reg)
+{
+	unsigned long __user *uaddr =
+			(unsigned long __user *)(unsigned long)reg->addr;
+	unsigned long reg_num = reg->id & ~(KVM_REG_ARCH_MASK |
+					    KVM_REG_SIZE_MASK |
+					    KVM_REG_RISCV_SBI);
+	unsigned long reg_subtype, reg_val;
+	int ret;
+
+	if (KVM_REG_SIZE(reg->id) != sizeof(unsigned long))
+		return -EINVAL;
+
+	reg_subtype = reg_num & KVM_REG_RISCV_SUBTYPE_MASK;
+	reg_num &= ~KVM_REG_RISCV_SUBTYPE_MASK;
+
+	switch (reg_subtype) {
+	default:
+		return -EINVAL;
+	}
+
+	if (ret)
+		return ret;
+
+	if (copy_to_user(uaddr, &reg_val, KVM_REG_SIZE(reg->id)))
+		return -EFAULT;
+
+	return 0;
+}

```

> get/set_one_reg 用于虚拟机热迁移，整体流程不清楚。但具体使用方式应该是：qemu调用get_one_reg获取当前虚拟机的一些寄存器值，然后调用set_one_reg设置下去。



### Add support for SBI STA registers

KVM用户空间需要能够保存和恢复steal-time共享内存地址。通过get/set-onereg接口提供地址，使用两个ulong大小的SBI STA扩展寄存器（lo和hi）。64位KVM用户空间不得将hi寄存器设置为非零值，并且可以完全忽略保存/恢复hi寄存器的操作。

```c
// arch/riscv/include/asm/kvm_vcpu_sbi.h
int kvm_riscv_vcpu_get_reg_sbi_sta(struct kvm_vcpu *vcpu, unsigned long reg_num,
				   unsigned long *reg_val);
int kvm_riscv_vcpu_set_reg_sbi_sta(struct kvm_vcpu *vcpu, unsigned long reg_num,
				   unsigned long reg_val);

// arch/riscv/include/uapi/asm/kvm.h
+/* SBI STA extension registers for KVM_GET_ONE_REG and KVM_SET_ONE_REG */
+struct kvm_riscv_sbi_sta {
+	unsigned long shmem_lo;
+	unsigned long shmem_hi;
+};

+#define KVM_REG_RISCV_SBI_STA		(0x0 << KVM_REG_RISCV_SUBTYPE_SHIFT)
+#define KVM_REG_RISCV_SBI_STA_REG(name)		\
+		(offsetof(struct kvm_riscv_sbi_sta, name) / sizeof(unsigned long))

// arch/riscv/kvm/vcpu_onereg.c
 static int copy_sbi_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)
 {
-	int n = num_sbi_regs(vcpu);
+	struct kvm_vcpu_sbi_context *scontext = &vcpu->arch.sbi_context;
+	int total = 0;
 
-	for (int i = 0; i < n; i++) {
-		u64 reg = KVM_REG_RISCV | KVM_REG_SIZE_U64 |
-			  KVM_REG_RISCV_SBI | i;
+	if (scontext->ext_status[KVM_RISCV_SBI_EXT_STA] == KVM_RISCV_SBI_EXT_STATUS_ENABLED) {
+		u64 size = IS_ENABLED(CONFIG_32BIT) ? KVM_REG_SIZE_U32 : KVM_REG_SIZE_U64;
+		int n = sizeof(struct kvm_riscv_sbi_sta) / sizeof(unsigned long);
 
-		if (uindices) {
-			if (put_user(reg, uindices))
-				return -EFAULT;
-			uindices++;
+		for (int i = 0; i < n; i++) {
+			u64 reg = KVM_REG_RISCV | size |
+				  KVM_REG_RISCV_SBI | KVM_REG_RISCV_SBI_STA | i;
+
+			if (uindices) {
+				if (put_user(reg, uindices))
+					return -EFAULT;
+				uindices++;
+			}
 		}
+
+		total += n;
 	}
 
-	return n;
+	return total;
+}
+
+static inline unsigned long num_sbi_regs(struct kvm_vcpu *vcpu)
+{
+	return copy_sbi_reg_indices(vcpu, NULL);
 }

    
// arch/riscv/kvm/vcpu_sbi.c
int kvm_riscv_vcpu_set_reg_sbi(struct kvm_vcpu *vcpu,
 	reg_num &= ~KVM_REG_RISCV_SUBTYPE_MASK;
 
 	switch (reg_subtype) {
+	case KVM_REG_RISCV_SBI_STA:
+		return kvm_riscv_vcpu_set_reg_sbi_sta(vcpu, reg_num, reg_val);
 	default:
 		return -EINVAL;
 	}
                               
//arch/riscv/kvm/vcpu_sbi_sta.c
+int kvm_riscv_vcpu_get_reg_sbi_sta(struct kvm_vcpu *vcpu,
+				   unsigned long reg_num,
+				   unsigned long *reg_val)
+{
+	switch (reg_num) {
+	case KVM_REG_RISCV_SBI_STA_REG(shmem_lo):
+		*reg_val = (unsigned long)vcpu->arch.sta.shmem;
+		break;
+	case KVM_REG_RISCV_SBI_STA_REG(shmem_hi):
+		if (IS_ENABLED(CONFIG_32BIT))
+			*reg_val = upper_32_bits(vcpu->arch.sta.shmem);
+		else
+			*reg_val = 0;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}

+int kvm_riscv_vcpu_set_reg_sbi_sta(struct kvm_vcpu *vcpu,
+				   unsigned long reg_num,
+				   unsigned long reg_val)
+{
+	switch (reg_num) {
+	case KVM_REG_RISCV_SBI_STA_REG(shmem_lo):
+		if (IS_ENABLED(CONFIG_32BIT)) {
+			gpa_t hi = upper_32_bits(vcpu->arch.sta.shmem);
+
+			vcpu->arch.sta.shmem = reg_val;
+			vcpu->arch.sta.shmem |= hi << 32;
+		} else {
+			vcpu->arch.sta.shmem = reg_val;
+		}
+		break;
+	case KVM_REG_RISCV_SBI_STA_REG(shmem_hi):
+		if (IS_ENABLED(CONFIG_32BIT)) {
+			gpa_t lo = lower_32_bits(vcpu->arch.sta.shmem);
+
+			vcpu->arch.sta.shmem = ((gpa_t)reg_val << 32);
+			vcpu->arch.sta.shmem |= lo;
+		} else if (reg_val != 0) {
+			return -EINVAL;
+		}
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
```

> 上个PATCH基于riscv虚拟机热迁移场景，增加了一个sbi_reg的get/set基本框架，主要是针对riscv-non-isa的寄存器 (不直接以硬件形式存在)，之后各种subtype寄存器类型可直接添加，例如 `kvm_riscv_vcpu_set_reg_sbi_(*type)`



### Implement SBI STA extension

在KVM配置中添加一个选择SCHED_INFO，以获取run_delay信息。然后实现SBI STA的set-steal-time-shmem函数和kvm_riscv_vcpu_record_steal_time()函数，向客户提供steal-time信息。

```c
// arch/riscv/kvm/Kconfig
config KVM
 	select KVM_XFER_TO_GUEST_WORK
 	select MMU_NOTIFIER
 	select PREEMPT_NOTIFIERS
+	select SCHED_INFO
 	help
 	  Support hosting virtualized guest machines.

// arch/riscv/kvm/vcpu_sbi_sta.c
void kvm_riscv_vcpu_record_steal_time(struct kvm_vcpu *vcpu)
 {
 	gpa_t shmem = vcpu->arch.sta.shmem;
+	u64 last_steal = vcpu->arch.sta.last_steal;
+	u32 *sequence_ptr, sequence;
+	u64 *steal_ptr, steal;
+	unsigned long hva;
+	gfn_t gfn;
 
 	if (shmem == INVALID_GPA)
 		return;
+
+	/*
+	 * shmem is 64-byte aligned (see the enforcement in
+	 * kvm_sbi_sta_steal_time_set_shmem()) and the size of sbi_sta_struct
+	 * is 64 bytes, so we know all its offsets are in the same page.
+	 */
+	gfn = shmem >> PAGE_SHIFT;
+	hva = kvm_vcpu_gfn_to_hva(vcpu, gfn);
+
+	if (WARN_ON(kvm_is_error_hva(hva))) {
+		vcpu->arch.sta.shmem = INVALID_GPA;
+		return;
+	}
+
+	sequence_ptr = (u32 *)(hva + offset_in_page(shmem) +
+			       offsetof(struct sbi_sta_struct, sequence));
+	steal_ptr = (u64 *)(hva + offset_in_page(shmem) +
+			    offsetof(struct sbi_sta_struct, steal));
+
+	if (WARN_ON(get_user(sequence, sequence_ptr)))
+		return;
+
+	sequence = le32_to_cpu(sequence);
+	sequence += 1;
+
+	if (WARN_ON(put_user(cpu_to_le32(sequence), sequence_ptr)))
+		return;
+
+	if (!WARN_ON(get_user(steal, steal_ptr))) {
+		steal = le64_to_cpu(steal);
+		vcpu->arch.sta.last_steal = READ_ONCE(current->sched_info.run_delay);
+		steal += vcpu->arch.sta.last_steal - last_steal;
+		WARN_ON(put_user(cpu_to_le64(steal), steal_ptr));
+	}
+
+	sequence += 1;
+	WARN_ON(put_user(cpu_to_le32(sequence), sequence_ptr));
+
+	kvm_vcpu_mark_page_dirty(vcpu, gfn);
 }

static int kvm_sbi_sta_steal_time_set_shmem(struct kvm_vcpu *vcpu)
 {
-	return SBI_ERR_FAILURE;
+	struct kvm_cpu_context *cp = &vcpu->arch.guest_context;
+	unsigned long shmem_phys_lo = cp->a0;
+	unsigned long shmem_phys_hi = cp->a1;
+	u32 flags = cp->a2;
+	struct sbi_sta_struct zero_sta = {0};
+	unsigned long hva;
+	bool writable;
+	gpa_t shmem;
+	int ret;
+
+	if (flags != 0)
+		return SBI_ERR_INVALID_PARAM;
+
+	if (shmem_phys_lo == SBI_STA_SHMEM_DISABLE &&
+	    shmem_phys_hi == SBI_STA_SHMEM_DISABLE) {
+		vcpu->arch.sta.shmem = INVALID_GPA;
+		return 0;
+	}
+
+	if (shmem_phys_lo & (SZ_64 - 1))
+		return SBI_ERR_INVALID_PARAM;
+
+	shmem = shmem_phys_lo;
+
+	if (shmem_phys_hi != 0) {
+		if (IS_ENABLED(CONFIG_32BIT))
+			shmem |= ((gpa_t)shmem_phys_hi << 32);
+		else
+			return SBI_ERR_INVALID_ADDRESS;
+	}
+
+	hva = kvm_vcpu_gfn_to_hva_prot(vcpu, shmem >> PAGE_SHIFT, &writable);
+	if (kvm_is_error_hva(hva) || !writable)
+		return SBI_ERR_INVALID_ADDRESS;
+
+	ret = kvm_vcpu_write_guest(vcpu, shmem, &zero_sta, sizeof(zero_sta));
+	if (ret)
+		return SBI_ERR_FAILURE;
+
+	vcpu->arch.sta.shmem = shmem;
+	vcpu->arch.sta.last_steal = current->sched_info.run_delay;
+
+	return 0;
 }

static unsigned long kvm_sbi_ext_sta_probe(struct kvm_vcpu *vcpu)
 {
-	return 0;
+	return !!sched_info_on();
 }
```

>st的计算：
>
>```c
>+	if (!WARN_ON(get_user(steal, steal_ptr))) {
>+		steal = le64_to_cpu(steal);
>+		vcpu->arch.sta.last_steal = READ_ONCE(current->sched_info.run_delay);
>+		steal += vcpu->arch.sta.last_steal - last_steal;
>+		WARN_ON(put_user(cpu_to_le64(steal), steal_ptr));
>+	}
>```
>
>`run_delay` 指的是当前vcpu线程在等待队列的时间，这段时间在虚拟化场景下指的是其它vcpu线程抢占它的时间。
>
>|———wait———|———run———|———wait———|———run———|
>
>t0              			   t1						   t2					   	 t3					 	   t4
>
>**<font color='red'>疑问：sched_info.run_delay算的是总和？</font>**



## 3.4 Add RISC-V support to the KVM selftests

### Move sbi_ecall to processor.c

sbi_ecall()不是ucall特定的，其原型已经在processor.h中。将其实现移至processor.c。

```c
// tools/testing/selftests/kvm/lib/riscv/processor.c
+struct sbiret sbi_ecall(int ext, int fid, unsigned long arg0,
+			unsigned long arg1, unsigned long arg2,
+			unsigned long arg3, unsigned long arg4,
+			unsigned long arg5)
+{
+	register uintptr_t a0 asm ("a0") = (uintptr_t)(arg0);
+	register uintptr_t a1 asm ("a1") = (uintptr_t)(arg1);
+	register uintptr_t a2 asm ("a2") = (uintptr_t)(arg2);
+	register uintptr_t a3 asm ("a3") = (uintptr_t)(arg3);
+	register uintptr_t a4 asm ("a4") = (uintptr_t)(arg4);
+	register uintptr_t a5 asm ("a5") = (uintptr_t)(arg5);
+	register uintptr_t a6 asm ("a6") = (uintptr_t)(fid);
+	register uintptr_t a7 asm ("a7") = (uintptr_t)(ext);
+	struct sbiret ret;
+
+	asm volatile (
+		"ecall"
+		: "+r" (a0), "+r" (a1)
+		: "r" (a2), "r" (a3), "r" (a4), "r" (a5), "r" (a6), "r" (a7)
+		: "memory");
+	ret.error = a0;
+	ret.value = a1;
+
+	return ret;
+}
```



### Add guest_sbi_probe_extension

添加guest_sbi_probe_extension()函数，允许客户端代码探测SBI扩展。由于guest_sbi_probe_extension()需要SBI_ERR_NOT_SUPPORTED，因此借此机会引入所有SBI错误代码。尽管我们需要一个当前的扩展ID和基础扩展函数ID，但我们不会引入所有当前的扩展ID或基础扩展函数ID，因为我们更愿意在需要时引入它们。

```c
// tools/testing/selftests/kvm/include/riscv/processor.h
+/* SBI return error codes */
+#define SBI_SUCCESS				0
+#define SBI_ERR_FAILURE				-1
+#define SBI_ERR_NOT_SUPPORTED			-2
+#define SBI_ERR_INVALID_PARAM			-3
+#define SBI_ERR_DENIED				-4
+#define SBI_ERR_INVALID_ADDRESS			-5
+#define SBI_ERR_ALREADY_AVAILABLE		-6
+#define SBI_ERR_ALREADY_STARTED			-7
+#define SBI_ERR_ALREADY_STOPPED			-8

+enum sbi_ext_id {
+	SBI_EXT_BASE = 0x10,
+};
+
+enum sbi_ext_base_fid {
+	SBI_EXT_BASE_PROBE_EXT = 3,
+};

+bool guest_sbi_probe_extension(int extid, long *out_val);

// tools/testing/selftests/kvm/lib/riscv/processor.c
+bool guest_sbi_probe_extension(int extid, long *out_val)
+{
+	struct sbiret ret;
+
+	ret = sbi_ecall(SBI_EXT_BASE, SBI_EXT_BASE_PROBE_EXT, extid,
+			0, 0, 0, 0, 0);
+
+	__GUEST_ASSERT(!ret.error || ret.error == SBI_ERR_NOT_SUPPORTED,
+		       "ret.error=%ld, ret.value=%ld\n", ret.error, ret.value);
+
+	if (ret.error == SBI_ERR_NOT_SUPPORTED)
+		return false;
+
+	if (out_val)
+		*out_val = ret.value;
+
+	return true;
+}
```



### Add steal_time test support

随着对RISC-V KVM的STA支持的引入，我们可以将RISC-V支持添加到 steal-time 测试中。

```c
// tools/testing/selftests/kvm/Makefile
+TEST_GEN_PROGS_riscv += guest_print_test
+TEST_GEN_PROGS_riscv += kvm_binary_stats_test
+TEST_GEN_PROGS_riscv += steal_time
    
// tools/testing/selftests/kvm/include/riscv/processor.h
enum sbi_ext_id {
 	SBI_EXT_BASE = 0x10,
+	SBI_EXT_STA = 0x535441,
 };

// tools/testing/selftests/kvm/steal_time.c
// ...
```

[linux/tools/testing/selftests/kvm/steal_time.c at kvm/steal-time-v3 · jones-drew/linux (github.com)](https://github.com/jones-drew/linux/blob/kvm/steal-time-v3/tools/testing/selftests/kvm/steal_time.c)



### Add get-reg-list test for STA registers

将SBI STA及其两个寄存器添加到get-reg-list测试中。

[linux/tools/testing/selftests/kvm/get-reg-list.c at kvm/steal-time-v3 · jones-drew/linux (github.com)](https://github.com/jones-drew/linux/blob/kvm/steal-time-v3/tools/testing/selftests/kvm/get-reg-list.c)



# 4 总结

一个完整特性的PATCH应该如何设计：

* 通常根据子系统或软件运行环境进行划分，比如此PATCH功能划分为：guest kernel/host KVM；
* 第一个PATCH，通常会把整体框架搭好，以及实现一些common的功能，函数定义为空，在后续PATCH中逐步填充；
* 后续的一系列PATCH都用于补全框架，以实现完整特性支持；
* 最后，提供测试代码，例如：`tools/testing/selftests/kvm/steal_time.c`。
  * linux 的特性测试代码通常不能也不需要完全自行实现，对此kvm相关测试代码来说，linux提供了一组工具函数集用于虚拟机配置，声明在 `tools/testing/selftests/kvm/lib/kvm_util.c` 中。
