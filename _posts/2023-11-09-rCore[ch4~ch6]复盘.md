---
title: rCore[ch4~ch6]复盘

date: 2023-11-09 17:00:00 +0800

categories: [读书笔记, rCore指导文档]

tags: [rCore]

description: 

---

# 四 地址空间

## 1 概述

我们目前还没有对内存管理功能进行进一步拓展，仅仅是把程序放到某处的物理内存中。在内存访问方面，所有的应用都直接通过物理地址访问物理内存，这使得应用开发者需要了解繁琐的物理地址空间布局，访问内存也很不方便。在上一章中，出于任务切换的需要，所有的应用都在初始化阶段被加载到内存中并同时驻留下去直到它们全部运行结束。而且，所有的应用都直接通过物理地址访问物理内存。这会带来以下问题：

- 首先，内核提供给应用的内存访问接口不够透明，也不好用。由于应用直接访问物理内存，这需要它在构建的时候就清楚所运行计算机的物理内存空间布局，还需规划自己需要被加载到哪个地址运行。为了避免冲突可能还需要应用的开发者们对此进行协商，这显然是一件在今天看来不够通用且极端麻烦的事情。
- 其次，内核并没有对应用的访存行为进行任何保护措施，每个应用都有计算机系统中整个物理内存的读写权力。即使应用被限制在 U 特权级下运行，它还是能够造成很多麻烦：比如它可以读写其他应用的数据来窃取信息或者破坏其它应用的正常运行；甚至它还可以修改内核的代码段来替换掉原本的 `trap_handler` 函数，来挟持内核执行恶意代码。总之，这造成系统既不安全、也不稳定。
- 再次，目前应用的内存使用空间在其运行前已经限定死了，内核不能灵活地给应用程序提供的运行时动态可用内存空间。比如一个应用结束后，这个应用所占的空间就被释放了，但这块空间无法动态地给其它还在运行的应用使用。

因此，为了简化应用开发，防止应用胡作非为，本章将更好地管理物理内存，并提供给应用一个抽象出来的更加透明易用、也更加安全的访存接口，这就是**基于分页机制的虚拟内存。**

* 站在应用程序运行的角度看，就是存在一个从 “0” 地址开始的非常大的可读/可写/可执行的地址空间；
* 站在操作系统的角度看，每个应用被局限在分配给它的物理内存空间中运行，无法读写其它应用和操作系统所在的内存空间。

---

`Address Space OS` 的总体结构如下图所示：

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311241640091.png" style="zoom:25%;" />

通过上图，大致可以看出 Address Space OS 为了提高操作系统和应用程序执行的安全性，增强了内存管理能力，提供了地址空间隔离机制，给APP的内存地址空间划界，不能越界访问OS和其他APP。在具体实现上：

* 扩展了 TaskManager 的管理范围，每个 Task 的上下文 `Task Context` 还包括该任务的地址空间，在切换任务时，也要切换任务的地址空间；
* 新增的内存管理模块主要包括与内核中动态内存分配相关的页帧分配、堆分配，以及表示应用地址空间的 `Apps MemSets` 类型和内核自身地址空间的 `Kernel MemSet` 类型。
  *  `MemSet` 类型所包含的页表 PageTable 建立了虚实地址映射关系，而另外一个 MemArea 表示任务的合法空间范围。

---

```rust
#[no_mangle]
/// the rust entry-point of os
pub fn rust_main() -> ! {
    clear_bss();
    println!("[kernel] Hello, world!");
    mm::init();
    println!("[kernel] back to world!");
    mm::remap_test();
    trap::init();
    //trap::enable_interrupt();
    trap::enable_timer_interrupt();
    timer::set_next_trigger();
    task::run_first_task();
    panic!("Unreachable in rust_main!");
}
```

## 2 重点内容梳理

### 2.1 在内核中支持动态内存分配

如果要实现**动态内存分配**的能力，需要操作系统需要有如下功能：

- 初始时能提供一块大内存空间作为初始的 “堆”；
- 提供在堆上分配和释放内存的函数接口；
- 提供空闲空间管理的连续内存分配算法；
- 提供建立在堆上的数据结构和操作；

Rust 的标准库中提供了很多开箱即用的堆数据结构，利用它们能够大大提升我们的开发效率，包括两类：

1. **智能指针：**`Box<T>`、`Rc<T>`、`RefCell<T>`、`Mutex<T>`
2. **容器：**`Vec<T>`、`BTreeMap<K, V>`、`BTreeSet<T>`、`LinkedList<T>`、`VecDeque`、`String`

> 在动态内存分配方面， Rust 和 C++ 很像，事实上 Rust 有意从 C++ 借鉴了这部分优秀特性，并强制Rust编程人员遵守 **借用规则** 。以 `Box<T>` 为例，在它被创建的时候，会在堆上分配一块空间保存它指向的数据；而在 `Box<T>` 生命周期结束被回收的时候，堆上的那块空间也会立即被一并回收。这也就是说，我们无需手动回收资源，它和绑定的变量会被自动回收。

---

Rust语言在 `alloc` crate 中设定了一套简洁规范的接口，只要实现了这套接口，内核就可以很方便地支持动态内存分配了。但由于我们的操作系统内核运行在禁用标准库（即 `no_std` ）的裸机平台上，核心库 `core` 并没有动态内存分配的功能，这个时候就要考虑利用 `alloc` 库定义的接口来实现基本的动态内存分配器。

`alloc` 库需要我们提供给它一个**全局的动态内存分配器，**它会利用该分配器来管理堆空间，从而使得与堆相关的智能指针或容器数据结构可以正常工作。具体而言，我们的动态内存分配器需要实现它提供的 `GlobalAlloc` Trait，这个 Trait 有两个必须实现的抽象接口：

```rust
// alloc::alloc::GlobalAlloc

pub unsafe fn alloc(&self, layout: Layout) -> *mut u8;
pub unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout);
```

将 `buddy_system_allocator` 中提供的 `LockedHeap` 实例化成一个全局变量，实际上 `LockedHeap` 已经实现了 `GlobalAlloc` 要求的抽象接口了，再使用 `alloc` 要求的 `#[global_allocator]` 语义项进行标记。

```rust
// os/src/mm/heap_allocator.rs

use buddy_system_allocator::LockedHeap;
use crate::config::KERNEL_HEAP_SIZE;

#[global_allocator]
static HEAP_ALLOCATOR: LockedHeap = LockedHeap::empty();

static mut HEAP_SPACE: [u8; KERNEL_HEAP_SIZE] = [0; KERNEL_HEAP_SIZE];

pub fn init_heap() {
    unsafe {
        HEAP_ALLOCATOR
            .lock()
            .init(HEAP_SPACE.as_ptr() as usize, KERNEL_HEAP_SIZE);
    }
}

// os/src/main.rs

#![feature(alloc_error_handler)]

// os/src/mm/heap_allocator.rs
#[alloc_error_handler]
pub fn handle_alloc_error(layout: core::alloc::Layout) -> ! {
    panic!("Heap allocation error, layout = {:?}", layout);
}
```

在使用任何 `alloc` 中提供的堆数据结构之前，我们需要先调用 `init_heap` 函数来给我们的全局分配器一块内存用于分配。在rCore中，这块内存是一个 `static mut` 且被零初始化的字节数组，位于内核的 `.bss` 段中。

### 2.2 物理页帧管理

物理页帧是实际的抽象内存单元，既可以用来实际存放应用/内核的数据/代码，也能够用来存储应用/内核的多级页表。当 `Bootloader` 把操作系统内核加载到物理内存中后，物理内存上已经有一部分用于放置内核的代码和数据。我们需要将剩下的空闲内存以单个物理页帧为单位管理起来，当需要存放应用数据或扩展应用的多级页表时分配空闲的物理页帧，并在应用出错或退出的时候回收应用占有的所有物理页帧。

#### 可用物理页的分配与回收

我们用一个左闭右开的物理页号区间来表示可用的物理内存，则：

- 区间的左端点应该是 `ekernel` 的物理地址以上取整方式转化成的物理页号；
- 区间的右端点应该是 `MEMORY_END` 以下取整方式转化成的物理页号。

这个区间将被传给我们后面实现的物理页帧管理器用于初始化。

```rust
// os/src/config.rs
pub const MEMORY_END: usize = 0x80800000;

// os/src/linker.ld
// .ekernel
```

我们声明一个 `FrameAllocator` Trait 来描述一个物理页帧管理器需要提供哪些功能，且实现了一种最简单的栈式物理页帧管理策略 `StackFrameAllocator`，然后创建 `StackFrameAllocator` 的全局实例 `FRAME_ALLOCATOR` ：

```rust
// os/src/mm/frame_allocator.rs
trait FrameAllocator {
    fn new() -> Self;
    fn alloc(&mut self) -> Option<PhysPageNum>;
    fn dealloc(&mut self, ppn: PhysPageNum);
}

// os/src/mm/frame_allocator.rs
pub struct StackFrameAllocator {
    current: usize,  //空闲内存的起始物理页号
    end: usize,      //空闲内存的结束物理页号
    recycled: Vec<usize>,	//以后入先出的方式保存了被回收的物理页号
}

// os/src/mm/frame_allocator.rs
use crate::sync::UPSafeCell;
type FrameAllocatorImpl = StackFrameAllocator;
lazy_static! {
    pub static ref FRAME_ALLOCATOR: UPSafeCell<FrameAllocatorImpl> = unsafe {
        UPSafeCell::new(FrameAllocatorImpl::new())
    };
}
```

在正式分配物理页帧之前，我们需要将物理页帧全局管理器 `FRAME_ALLOCATOR` 初始化：

```rust
// os/src/mm/frame_allocator.rs

pub fn init_frame_allocator() {
    extern "C" {
        fn ekernel();
    }
    FRAME_ALLOCATOR
        .exclusive_access()
        .init(PhysAddr::from(ekernel as usize).ceil(), PhysAddr::from(MEMORY_END).floor());
}
```

#### 分配/回收物理页帧的接口: FrameTracker

公开给其他内核模块调用的分配/回收物理页帧的接口如下：

```rust
// os/src/mm/frame_allocator.rs
pub fn frame_alloc() -> Option<FrameTracker> {
    FRAME_ALLOCATOR
        .exclusive_access()
        .alloc()
        .map(|ppn| FrameTracker::new(ppn))
}

fn frame_dealloc(ppn: PhysPageNum) {
    FRAME_ALLOCATOR
        .exclusive_access()
        .dealloc(ppn);
}
```

可以发现， `frame_alloc` 的返回值类型并不是 `FrameAllocator` 要求的物理页号 `PhysPageNum` ，而是将其进一步包装为一个 `FrameTracker` 。**这里借用了 RAII 的思想，**将一个物理页帧的生命周期绑定到一个 `FrameTracker` 变量上，即：

* 当一个 `FrameTracker` 被创建的时候，我们需要从 `FRAME_ALLOCATOR` 中分配一个物理页帧；
* 当一个 `FrameTracker` 生命周期结束被编译器回收的时候，我们需要将它控制的物理页帧回收到 `FRAME_ALLOCATOR` 中；

这里我们只需为 `FrameTracker` 实现 `Drop` Trait 即可。当一个 `FrameTracker` 实例被回收的时候，它的 `drop` 方法会自动被编译器调用，通过之前实现的 `frame_dealloc` 我们就将它控制的物理页帧回收以供后续使用了。

```rust
// os/src/mm/frame_allocator.rs
impl Drop for FrameTracker {
    fn drop(&mut self) {
        frame_dealloc(self.ppn);
    }
}
```

---

从其他内核模块的视角看来，物理页帧分配的接口是调用 `frame_alloc` 函数得到一个 `FrameTracker` （如果物理内存还有剩余），它就代表了一个物理页帧，当它的生命周期结束之后它所控制的物理页帧将被自动回收。

### 2.3 多级页表管理

关于地址空间的介绍：[地址空间 - rCore-Tutorial-Book-v3 3.6.0-alpha.1 文档 (rcore-os.cn)](https://rcore-os.cn/rCore-Tutorial-Book-v3/chapter4/2address-space.html)

SV39 多级页表的硬件机制：[SV39 多级页表的硬件机制 - rCore-Tutorial-Book-v3 3.6.0-alpha.1 文档 (rcore-os.cn)](https://rcore-os.cn/rCore-Tutorial-Book-v3/chapter4/3sv39-implementation-1.html)

#### 页表项

让我们先来实现页表项中的标志位 `PTEFlags` ：

```rust
// os/Cargo.toml
[dependencies]
bitflags = "1.2.1"

// os/src/main.rs
#[macro_use]
extern crate bitflags;

// os/src/mm/page_table.rs
use bitflags::*;

bitflags! {
    pub struct PTEFlags: u8 {
        const V = 1 << 0;
        const R = 1 << 1;
        const W = 1 << 2;
        const X = 1 << 3;
        const U = 1 << 4;
        const G = 1 << 5;
        const A = 1 << 6;
        const D = 1 << 7;
    }
}
```

接下来我们实现页表项 `PageTableEntry` ：

```rust
// os/src/mm/page_table.rs
#[derive(Copy, Clone)]
#[repr(C)]
pub struct PageTableEntry {
    pub bits: usize,
}

impl PageTableEntry {
    pub fn new(ppn: PhysPageNum, flags: PTEFlags) -> Self {
        PageTableEntry {
            bits: ppn.0 << 10 | flags.bits as usize,
        }
    }
    pub fn empty() -> Self {
        PageTableEntry {
            bits: 0,
        }
    }
    pub fn ppn(&self) -> PhysPageNum {
        (self.bits >> 10 & ((1usize << 44) - 1)).into()
    }
    pub fn flags(&self) -> PTEFlags {
        PTEFlags::from_bits(self.bits as u8).unwrap()
    }
}
```

我们让编译器自动为 `PageTableEntry` 实现 `Copy/Clone` Trait，来让这个类型以值语义赋值/传参的时候不会发生所有权转移，而是拷贝一份新的副本。

#### 多级页表原理

https://rcore-os.cn/rCore-Tutorial-Book-v3/chapter4/3sv39-implementation-1.html#id6

SV39 中虚拟页号被分为三级 **页索引** (Page Index) ，因此这是一种三级页表。在这种三级页表的树结构中，自上而下分为三种不同的节点：一级/二级/三级页表节点：

* 树的根节点被称为一级页表节点，一级页表节点可以通过一级页索引找到二级页表节点；
* 二级页表节点可以通过二级页索引找到三级页表节点；
* 三级页表节点是树的叶节点，通过三级页索引可以找到一个页表项；

注意，非叶节点（页目录表，非末级页表）的表项标志位含义和叶节点（页表，末级页表）相比有一些不同：

- 当 `V` 为 0 的时候，代表当前指针是一个空指针，无法走向下一级节点，即该页表项对应的虚拟地址范围是无效的；
- 只有当 `V` 为1 且 `R/W/X` 均为 0 时，表示是一个合法的页目录表项，其包含的指针会指向下一级的页表；
- 注意: 当 `V` 为1 且 `R/W/X` 不全为 0 时，表示是一个合法的页表项，其包含了虚地址对应的物理页号。

那么 SV39 多级页表相比线性表到底能节省多少内存呢？这里直接给出结论：设某个应用地址空间实际用到的区域总大小为 `S` 字节，则地址空间对应的多级页表消耗内存为 `S/512` 左右。

接下来，我们给出 SV39 地址转换的全过程图示：

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312131046223.png" alt="image-20231213104508449" style="zoom: 80%;" />

在 SV39 模式中我们采用三级页表，即将 27 位的虚拟页号分为三个等长的部分，第 26-18 位为一级页索引 VPN0 ，第 17-9 位为二级页索引 VPN1 ，第 8-0 位为三级页索引 VPN2 。

我们也将页表分为一级页表（多级页表的根节点），二级页表，三级页表（多级页表的叶节点）。每个页表都用 9 位索引，因此有 29=512 个页表项，而每个页表项都是 8 字节，因此每个页表大小都为 512×8=4KiB 。正好是一个物理页的大小。我们可以把一个页表放到一个物理页中，并用一个物理页号来描述它。

具体来说，假设我们有虚拟地址 (VPN0,VPN1,VPN2,offset) ：

- 我们首先会记录装载「当前所用的一级页表的物理页」的页号到 satp 寄存器中；
- 把 VPN0 作为偏移在一级页表的物理页中找到二级页表的物理页号；
- 把 VPN1 作为偏移在二级页表的物理页中找到三级页表的物理页号；
- 把 VPN2 作为偏移在三级页表的物理页中找到要访问位置的物理页号；
- 物理页号对应的物理页基址（即物理页号左移12位）加上 offset 就是虚拟地址对应的物理地址。

这样处理器通过这种多次转换，终于从虚拟页号找到了一级页表项，从而得出了物理页号和虚拟地址所对应的物理地址。

#### 页表数据结构与访问接口

SV39 多级页表是以节点为单位进行管理的。每个节点恰好存储在一个物理页帧中，它的位置可以用一个物理页号来表示。

```rust
// os/src/mm/page_table.rs
pub struct PageTable {
    root_ppn: PhysPageNum,
    frames: Vec<FrameTracker>,
}

impl PageTable {
    pub fn new() -> Self {
        let frame = frame_alloc().unwrap();
        PageTable {
            root_ppn: frame.ppn,
            frames: vec![frame],
        }
    }
}
```

#### 建立与拆除虚拟地址映射

多级页表并不是被创建出来之后就不再变化的，为了 MMU 能够通过地址转换正确找到应用地址空间中的数据实际被内核放在内存中位置，操作系统需要动态维护一个虚拟页号到页表项的映射，支持插入/删除键值对，其方法签名如下：

```rust
// os/src/mm/page_table.rs
impl PageTable {
    pub fn map(&mut self, vpn: VirtPageNum, ppn: PhysPageNum, flags: PTEFlags);
    pub fn unmap(&mut self, vpn: VirtPageNum);
}
```

对于内核地址空间，我们采用一种最简单的 **恒等映射** (Identical Mapping) ，即对于物理内存上的每个物理页帧，我们都在多级页表中用一个与其物理页号相等的虚拟页号来映射。

注意，应用和内核的地址空间是隔离的。而直接访问物理页帧的操作只会在内核中进行，应用无法看到物理页帧管理器和多级页表等内核数据结构。因此，上述的恒等映射只需被附加到内核地址空间即可。

---

关于建立和拆除虚实地址映射关系的 `map` 和 `unmap` 方法，它们都依赖于一个很重要的过程，即**在多级页表中找到一个虚拟地址对应的页表项。**找到之后，只要修改页表项的内容即可完成键值对的插入和删除。在寻找页表项的时候，可能出现页表的中间级节点还未被创建的情况，这个时候我们需要手动分配一个物理页帧来存放这个节点，并将这个节点接入到当前的多级页表的某级中。

```rust
// os/src/mm/address.rs
impl VirtPageNum {
    pub fn indexes(&self) -> [usize; 3] {
        let mut vpn = self.0;
        let mut idx = [0usize; 3];
        for i in (0..3).rev() {
            idx[i] = vpn & 511;
            vpn >>= 9;
        }
        idx
    }
}

// os/src/mm/page_table.rs
impl PageTable {
    fn find_pte_create(&mut self, vpn: VirtPageNum) -> Option<&mut PageTableEntry> {
        let idxs = vpn.indexes();
        let mut ppn = self.root_ppn;
        let mut result: Option<&mut PageTableEntry> = None;
        for i in 0..3 {
            let pte = &mut ppn.get_pte_array()[idxs[i]];
            if i == 2 {
                result = Some(pte);
                break;
            }
            if !pte.is_valid() {
                let frame = frame_alloc().unwrap();
                *pte = PageTableEntry::new(frame.ppn, PTEFlags::V);
                self.frames.push(frame);
            }
            ppn = pte.ppn();
        }
        result
    }
}

// os/src/mm/page_table.rs
impl PageTable {
    pub fn map(&mut self, vpn: VirtPageNum, ppn: PhysPageNum, flags: PTEFlags) {
        let pte = self.find_pte_create(vpn).unwrap();
        assert!(!pte.is_valid(), "vpn {:?} is mapped before mapping", vpn);
        *pte = PageTableEntry::new(ppn, flags | PTEFlags::V);
    }
    pub fn unmap(&mut self, vpn: VirtPageNum) {
        let pte = self.find_pte(vpn).unwrap();
        assert!(pte.is_valid(), "vpn {:?} is invalid before unmapping", vpn);
        *pte = PageTableEntry::empty();
    }
}
```

我们还需要 `PageTable` 提供一种类似 MMU 操作的手动查页表的方法：

```rust
// os/src/mm/page_table.rs

impl PageTable {
    /// Temporarily used to get arguments from user space.
    pub fn from_token(satp: usize) -> Self {
        Self {
            root_ppn: PhysPageNum::from(satp & ((1usize << 44) - 1)),
            frames: Vec::new(),
        }
    }
    pub fn translate(&self, vpn: VirtPageNum) -> Option<PageTableEntry> {
        self.find_pte(vpn)
            .map(|pte| {pte.clone()})
    }
}
```

### 2.4 内核与应用地址空间

#### 地址空间抽象

##### 逻辑段: MapArea

我们以逻辑段 `MapArea` 为单位描述一段连续地址的虚拟内存：

```rust
// os/src/mm/memory_set.rs

pub struct MapArea {
    vpn_range: VPNRange,
    data_frames: BTreeMap<VirtPageNum, FrameTracker>,
    map_type: MapType,
    map_perm: MapPermission,
}
```

* `VPNRange` 描述一段虚拟页号的连续区间，表示该逻辑段在地址区间中的位置和长度；
* `MapType` 描述该逻辑段内的所有虚拟页面映射到物理页帧的同一种方式，支持两种方式 `Identical/Framed`：
  * `Identical` 表示恒等映射方式。恒等映射方式主要是用在启用多级页表之后，内核仍能够在虚存地址空间中访问一个特定的物理地址指向的物理内存。
  * `Framed` 则表示对于每个虚拟页面都有一个新分配的物理页帧与之对应，虚地址与物理地址的映射关系是相对随机的。

* 当逻辑段采用 `MapType::Framed` 方式映射到物理内存的时候， `data_frames` 是一个保存了该逻辑段内的每个虚拟页面和它被映射到的物理页帧 `FrameTracker` 的一个键值对容器 `BTreeMap` 中，这些物理页帧被用来存放实际内存数据而不是作为多级页表中的中间节点。
* `MapPermission` 表示控制该逻辑段的访问方式，它是页表项标志位 `PTEFlags` 的一个子集，仅保留 U/R/W/X 四个标志位，因为其他的标志位仅与硬件的地址转换机制细节相关。

在地址空间中插入一个逻辑段 `MapArea` 的时候，需要同时维护地址空间的多级页表 `page_table` 记录的虚拟页号到页表项的映射关系，也需要用到这个映射关系来找到向哪些物理页帧上拷贝初始数据。这用到了 `MapArea` 提供的另外几个方法：

```rust
// os/src/mm/memory_set.rs

impl MapArea {
    pub fn new(
        start_va: VirtAddr,
        end_va: VirtAddr,
        map_type: MapType,
        map_perm: MapPermission
    ) -> Self;
    
    pub fn map(&mut self, page_table: &mut PageTable) {
        for vpn in self.vpn_range {
            self.map_one(page_table, vpn);
        }
    }
    pub fn unmap(&mut self, page_table: &mut PageTable) {
        for vpn in self.vpn_range {
            self.unmap_one(page_table, vpn);
        }
    }
    
    pub fn map_one(&mut self, page_table: &mut PageTable, vpn: VirtPageNum) {
        let ppn: PhysPageNum;
        match self.map_type {
            MapType::Identical => {
                ppn = PhysPageNum(vpn.0);
            }
            MapType::Framed => {
                let frame = frame_alloc().unwrap();
                ppn = frame.ppn;
                self.data_frames.insert(vpn, frame);
            }
        }
        let pte_flags = PTEFlags::from_bits(self.map_perm.bits).unwrap();
        page_table.map(vpn, ppn, pte_flags);
    }
    pub fn unmap_one(&mut self, page_table: &mut PageTable, vpn: VirtPageNum) {
        match self.map_type {
            MapType::Framed => {
                self.data_frames.remove(&vpn);
            }
            _ => {}
        }
        page_table.unmap(vpn);
    }
    
    /// data: start-aligned but maybe with shorter length
    /// assume that all frames were cleared before
    pub fn copy_data(&mut self, page_table: &mut PageTable, data: &[u8]);
}
```

> 注意，对于 `unmap_one` 函数，当以 `Framed` 映射的时候，不要忘记同时将虚拟页面被映射到的物理页帧 `FrameTracker` 从 `data_frames` 中移除，这样这个物理页帧才能立即被回收以备后续分配。

---

##### 地址空间: MemorySet

地址空间是一系列有关联的不一定连续的逻辑段，这种关联一般是指这些逻辑段组成的虚拟内存空间与一个运行的程序（目前把一个运行的程序称为任务，后续会称为进程）绑定，即这个运行的程序对代码和数据的直接访问范围限制在它关联的虚拟地址空间之内。这样我们就有应用的地址空间，内核的地址空间等说法了。

地址空间使用 `MemorySet` 类型来表示：

```rust
// os/src/mm/memory_set.rs

pub struct MemorySet {
    page_table: PageTable,
    areas: Vec<MapArea>,
}

// os/src/mm/memory_set.rs

impl MemorySet {
    pub fn new_bare() -> Self;
    fn push(&mut self, mut map_area: MapArea, data: Option<&[u8]>);
    /// Assume that no conflicts.
    pub fn insert_framed_area(
        &mut self,
        start_va: VirtAddr, end_va: VirtAddr, permission: MapPermission
    );
    pub fn new_kernel() -> Self;
    /// Include sections in elf and trampoline and TrapContext and user stack,
    /// also returns user_sp and entry point.
    pub fn from_elf(elf_data: &[u8]) -> (Self, usize, usize);
}
```

> 注意 `PageTable` 下挂着所有多级页表的节点所在的物理页帧，而每个 `MapArea` 下则挂着对应逻辑段中的数据所在的物理页帧，这两部分合在一起构成了一个地址空间所需的所有物理页帧。这同样是一种 RAII 风格，当一个地址空间 `MemorySet` 生命周期结束后，这些物理页帧都会被回收。

*  `new_bare` 方法可以新建一个空的地址空间；
*  `push` 方法可以在当前地址空间插入一个新的逻辑段 `map_area` ，如果它是以 `Framed` 方式映射到物理内存，还可以可选地在那些被映射到的物理页帧上写入一些初始化数据 `data` ；
*  `insert_framed_area` 方法调用 `push` ，可以在当前地址空间插入一个 `Framed` 方式映射到物理内存的逻辑段；
* `new_kernel` 可以生成内核地址空间；
* `from_elf` 分析应用的 ELF 文件格式的内容，解析出各数据段并生成对应的应用地址空间；

---

#### 内核地址空间

地址空间抽象的重要意义在于 **隔离** (Isolation) ，当内核让应用执行前，内核需要控制 MMU 使用这个应用的多级页表进行地址转换。由于每个应用地址空间在创建的时候也顺带设置好了多级页表，使得只有那些存放了它的代码和数据的物理页帧能够通过该多级页表被映射到，这样它就只能访问自己的代码和数据而无法触及其他应用或内核的内容。

启用分页模式下，内核代码的访存地址也会被视为一个虚拟地址并需要经过 MMU 的地址转换，因此我们也需要为内核对应构造一个地址空间，它除了仍然需要允许内核的各数据段能够被正常访问之后，还需要包含所有应用的内核栈以及一个 **跳板** (Trampoline) 。

下图是软件看到的 64 位地址空间在 SV39 分页模式下，实际可能通过 MMU 检查的最高 256GiB：

![img](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312131322199.png)

下面则给出了内核地址空间的低 256GiB 的布局：

![../_images/kernel-as-low.png](https://rcore-os.cn/rCore-Tutorial-Book-v3/_images/kernel-as-low.png)

内核的四个逻辑段 `.text/.rodata/.data/.bss` 被恒等映射到物理内存，这使得我们在无需调整内核内存布局 `os/src/linker.ld` 的情况下就仍能象启用页表机制之前那样访问内核的各个段。

此外， 内核地址空间中还需要存在一个恒等映射到内核数据段之外的可用物理页帧的逻辑段，这样才能在启用页表机制之后，内核仍能以纯软件的方式读写这些物理页帧。

`new_kernel` 将映射跳板和地址空间中最低 256GiB 中的内核逻辑段。我们从低地址到高地址依次创建 5 个逻辑段并通过 `push` 方法将它们插入到内核地址空间中，上面我们已经详细介绍过这 5 个逻辑段。跳板是通过 `map_trampoline` 方法来映射的。

```rust
// os/src/mm/memory_set.rs

extern "C" {
    fn stext();
    fn etext();
    fn srodata();
    fn erodata();
    fn sdata();
    fn edata();
    fn sbss_with_stack();
    fn ebss();
    fn ekernel();
    fn strampoline();
}

impl MemorySet {
    /// Without kernel stacks.
    pub fn new_kernel() -> Self {
        let mut memory_set = Self::new_bare();
        // map trampoline
        memory_set.map_trampoline();
        // map kernel sections
        println!(".text [{:#x}, {:#x})", stext as usize, etext as usize);
        println!(".rodata [{:#x}, {:#x})", srodata as usize, erodata as usize);
        println!(".data [{:#x}, {:#x})", sdata as usize, edata as usize);
        println!(".bss [{:#x}, {:#x})", sbss_with_stack as usize, ebss as usize);
        println!("mapping .text section");
        memory_set.push(MapArea::new(
            (stext as usize).into(),
            (etext as usize).into(),
            MapType::Identical,
            MapPermission::R | MapPermission::X,
        ), None);
        println!("mapping .rodata section");
        memory_set.push(MapArea::new(
            (srodata as usize).into(),
            (erodata as usize).into(),
            MapType::Identical,
            MapPermission::R,
        ), None);
        println!("mapping .data section");
        memory_set.push(MapArea::new(
            (sdata as usize).into(),
            (edata as usize).into(),
            MapType::Identical,
            MapPermission::R | MapPermission::W,
        ), None);
        println!("mapping .bss section");
        memory_set.push(MapArea::new(
            (sbss_with_stack as usize).into(),
            (ebss as usize).into(),
            MapType::Identical,
            MapPermission::R | MapPermission::W,
        ), None);
        println!("mapping physical memory");
        memory_set.push(MapArea::new(
            (ekernel as usize).into(),
            MEMORY_END.into(),
            MapType::Identical,
            MapPermission::R | MapPermission::W,
        ), None);
        memory_set
    }
}
```

---

#### 应用地址空间

之前，我们直接将丢弃了所有符号信息的应用二进制镜像链接到内核，在初始化的时候内核仅需将他们加载到正确的初始物理地址就能使它们正确执行。但本章中，我们希望效仿内核地址空间的设计，同样借助页表机制使得应用地址空间的各个逻辑段也可以有不同的访问方式限制，这样可以提早检测出应用的错误并及时将其终止以最小化它对系统带来的恶劣影响。

在第三章中，每个应用链接脚本中的起始地址被要求是不同的，这样它们的代码和数据存放的位置才不会产生冲突。但这是一种对于应用开发者很不方便的设计。现在，借助地址空间的抽象，我们终于可以让所有应用程序都使用同样的起始地址，这也意味着所有应用可以使用同一个链接脚本了 `user/src/linker.ld`。

下图展示了应用地址空间的布局：

![../_images/app-as-full.png](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312131332622.png)

* 左侧给出了应用地址空间最低 256GiB 的布局。从 0x10000 开始向高地址放置应用内存布局中的各个逻辑段，最后放置带有一个保护页面的用户栈，这些逻辑段都是以 `Framed` 方式映射到物理内存的；
* 右侧则给出了最高 256GiB 的布局。可以看出它只是和内核地址空间一样将跳板放置在最高页，还将 Trap 上下文放置在次高页中。这两个虚拟页面虽然位于应用地址空间，但是它们并不包含 U 标志位，应用程序无法访问它们，事实上它们在地址空间切换的时候才会发挥作用。

在创建应用地址空间的时候，我们需要对 `get_app_data` 得到的 ELF 格式数据进行解析，找到各个逻辑段所在位置和访问限制并插入进来，最终得到一个完整的应用地址空间：

```rust
// os/src/mm/memory_set.rs

impl MemorySet {
    /// Include sections in elf and trampoline and TrapContext and user stack,
    /// also returns user_sp and entry point.
    pub fn from_elf(elf_data: &[u8]) -> (Self, usize, usize) {
        let mut memory_set = Self::new_bare();
        // map trampoline
        memory_set.map_trampoline();
        // map program headers of elf, with U flag
        let elf = xmas_elf::ElfFile::new(elf_data).unwrap();
        let elf_header = elf.header;
        let magic = elf_header.pt1.magic;
        assert_eq!(magic, [0x7f, 0x45, 0x4c, 0x46], "invalid elf!");
        let ph_count = elf_header.pt2.ph_count();
        let mut max_end_vpn = VirtPageNum(0);
        for i in 0..ph_count {
            let ph = elf.program_header(i).unwrap();
            if ph.get_type().unwrap() == xmas_elf::program::Type::Load {
                let start_va: VirtAddr = (ph.virtual_addr() as usize).into();
                let end_va: VirtAddr = ((ph.virtual_addr() + ph.mem_size()) as usize).into();
                let mut map_perm = MapPermission::U;
                let ph_flags = ph.flags();
                if ph_flags.is_read() { map_perm |= MapPermission::R; }
                if ph_flags.is_write() { map_perm |= MapPermission::W; }
                if ph_flags.is_execute() { map_perm |= MapPermission::X; }
                let map_area = MapArea::new(
                    start_va,
                    end_va,
                    MapType::Framed,
                    map_perm,
                );
                max_end_vpn = map_area.vpn_range.get_end();
                memory_set.push(
                    map_area,
                    Some(&elf.input[ph.offset() as usize..(ph.offset() + ph.file_size()) as usize])
                );
            }
        }
        // map user stack with U flags
        let max_end_va: VirtAddr = max_end_vpn.into();
        let mut user_stack_bottom: usize = max_end_va.into();
        // guard page
        user_stack_bottom += PAGE_SIZE;
        let user_stack_top = user_stack_bottom + USER_STACK_SIZE;
        memory_set.push(MapArea::new(
            user_stack_bottom.into(),
            user_stack_top.into(),
            MapType::Framed,
            MapPermission::R | MapPermission::W | MapPermission::U,
        ), None);
        // map TrapContext
        memory_set.push(MapArea::new(
            TRAP_CONTEXT.into(),
            TRAMPOLINE.into(),
            MapType::Framed,
            MapPermission::R | MapPermission::W,
        ), None);
        (memory_set, user_stack_top, elf.header.pt2.entry_point() as usize)
    }
}
```

`from_elf` 函数不仅返回应用地址空间 `memory_set` ，也同时返回用户栈虚拟地址 `user_stack_top` 以及从解析 ELF 得到的该应用入口点地址，它们将被我们用来创建应用的任务控制块。

### 2.5 扩展内核以支持分页机制

对现有的操作系统进行如下的功能扩展：

- 创建内核页表，使能分页机制，建立内核的虚拟地址空间；
- 扩展Trap上下文，在保存与恢复Trap上下文的过程中切换页表（即切换虚拟地址空间）；
- 建立用于内核地址空间与应用地址空间相互切换所需的跳板空间；
- 扩展任务控制块包括虚拟内存相关信息，并在加载执行创建基于某应用的任务时，建立应用的虚拟地址空间；
- 改进Trap处理过程和sys_write等系统调用的实现以支持分离的应用地址空间和内核地址空间。

#### 创建内核地址空间并使能MMU

当 RustSBI 初始化完成后， CPU 将跳转到内核入口点并在 S 特权级上执行，此时还并没有开启分页模式，内核的每次访存是直接的物理内存访问。而在开启分页模式之后，内核代码在访存时只能看到内核地址空间，此时每次访存需要通过 MMU 的地址转换。这两种模式之间的过渡在内核初始化期间完成。

在 `rust_main` 函数中，我们首先调用 `mm::init` 进行内存管理子系统的初始化：

```rust
// os/src/mm/memory_set.rs
lazy_static! {
    pub static ref KERNEL_SPACE: Arc<UPSafeCell<MemorySet>> = Arc::new(unsafe {
        UPSafeCell::new(MemorySet::new_kernel()
    )});
}

// os/src/mm/page_table.rs
pub fn token(&self) -> usize {
    8usize << 60 | self.root_ppn.0
}

// os/src/mm/memory_set.rs
impl MemorySet {
    pub fn activate(&self) {
        let satp = self.page_table.token();
        unsafe {
            satp::write(satp);
            asm!("sfence.vma" :::: "volatile");
        }
    }
}

// os/src/mm/mod.rs
pub use memory_set::KERNEL_SPACE;

pub fn init() {
    heap_allocator::init_heap();
    frame_allocator::init_frame_allocator();
    KERNEL_SPACE.exclusive_access().activate();
}
```

* 首先进行了全局动态内存分配器的初始化，因为接下来马上就要用到 Rust 的堆数据结构；
* 接下来，初始化物理页帧管理器（内含堆数据结构 `Vec<T>` ）使能可用物理页帧的分配和回收能力；
* 最后创建内核地址空间并让 CPU 开启分页模式， MMU 在地址转换的时候使用内核的多级页表，这一切均在一行之内做到：
  * 首先引用 `KERNEL_SPACE` ，这是它第一次被使用，就在此时它会被初始化，调用 `MemorySet::new_kernel` 创建一个内核地址空间；
  * 最后，我们调用 `MemorySet::activate`，写当前 CPU 的 `satp` ，从这一刻开始 SV39 分页模式就被启用了，而且 MMU 会使用内核地址空间的多级页表进行地址转换；

> **<font color='red'>Notice！</font>**
>
> 我们必须注意切换 `satp` 是否是一个 *平滑* 的过渡：其含义是指，切换 `satp` 的指令及其下一条指令这两条相邻的指令的虚拟地址是相邻的（由于切换 `satp` 的指令并不是一条跳转指令， `pc` 只是简单的自增当前指令的字长），而它们所在的物理地址一般情况下也是相邻的，但是它们所经过的地址转换流程却是不同的——切换 `satp` 导致 MMU 查的多级页表是不同的。这就要求前后两个地址空间在切换 `satp` 的指令 *附近* 的映射满足某种意义上的连续性。
>
> 幸运的是，我们做到了这一点。这条写入 `satp` 的指令及其下一条指令都在内核内存布局的代码段中，在切换之后是一个恒等映射，而在切换之前是视为物理地址直接取指，也可以将其看成一个恒等映射。这完全符合我们的期待：**即使切换了地址空间，指令仍应该能够被连续的执行。**

#### Trap机制扩展: trampoline

##### 为什么需要跳板？

我们看到无论是内核还是应用的地址空间，最高的虚拟页面都是一个跳板。同时应用地址空间的次高虚拟页面还被设置为用来存放应用的 Trap 上下文。那么跳板究竟起什么作用呢？为何不直接把 Trap 上下文仍放到应用的内核栈中呢？

看一下目前的 rCore 的 Trap 处理流程：

1. 当一个应用 Trap 到内核时，`sscratch` 已指向该应用的内核栈栈顶，我们用一条指令即可从用户栈切换到内核栈，然后直接将 Trap 上下文压入内核栈栈顶；
2. 当 Trap 处理完毕返回用户态的时候，将 Trap 上下文中的内容恢复到寄存器上，最后将保存着应用用户栈顶的 `sscratch` 与 sp 进行交换，也就从内核栈切换回了用户栈。

在这个过程中，`sscratch` 起到了非常关键的作用，它使得我们可以在不破坏任何通用寄存器的情况下，完成用户栈与内核栈的切换，以及位于内核栈顶的 Trap 上下文的保存与恢复。

然而，一旦使能了分页机制，一切就并没有这么简单了，我们必须在这个过程中同时完成地址空间的切换。具体来说：

* 当 `__alltraps` 保存 Trap 上下文的时候，我们必须通过修改 satp 从应用地址空间切换到内核地址空间，因为 trap handler 只有在内核地址空间中才能访问；
* 同理，在 `__restore` 恢复 Trap 上下文的时候，我们也必须从内核地址空间切换回应用地址空间，因为应用的代码和数据只能在它自己的地址空间中才能访问，应用是看不到内核地址空间的。

这样就要求地址空间的切换不能影响指令的连续执行，即**<font color='red'>要求应用和内核地址空间在切换地址空间指令附近是平滑的。</font>**

> 我们为何将应用的 Trap 上下文放到应用地址空间的次高页面而不是内核地址空间中的内核栈中呢？原因在于，在保存 Trap 上下文到内核栈中之前，我们必须完成两项工作：
>
> * 必须先切换到内核地址空间，这就需要将内核地址空间的 token 写入 satp 寄存器；
> * 之后还需要保存应用的内核栈栈顶的位置，这样才能以它为基址保存 Trap 上下文；
>
> 这两步需要用寄存器作为临时周转，然而我们无法在不破坏任何一个通用寄存器的情况下做到这一点。因为事实上我们需要用到内核的两条信息：内核地址空间的 token ，以及应用的内核栈栈顶的位置，RISC-V却只提供一个 `sscratch` 寄存器可用来进行周转。所以，我们不得不将 Trap 上下文保存在应用地址空间的一个虚拟页面中，而不是切换到内核地址空间去保存。

接下来是跳板页面的建立，我们将 `trap.S` 中的整段汇编代码放置在 `.text.trampoline` 段，并在调整内存布局的时候将它对齐到代码段的一个页面中：

```assembly
# os/src/linker.ld

    stext = .;
    .text : {
        *(.text.entry)
+        . = ALIGN(4K);
+        strampoline = .;
+        *(.text.trampoline);
+        . = ALIGN(4K);
        *(.text .text.*)
    }
```

这样，这段汇编代码放在一个物理页帧中，且 `__alltraps` 恰好位于这个物理页帧的开头，其物理地址被外部符号 `strampoline` 标记。在开启分页模式之后，内核和应用代码都只能看到各自的虚拟地址空间，而在它们的视角中，这段汇编代码都被放在它们各自地址空间的最高虚拟页面上，**由于这段汇编代码在执行的时候涉及到地址空间切换，故而被称为跳板页面。**

无论是内核还是应用的地址空间，跳板的虚拟页均位于同样位置，且它们也将会映射到同一个实际存放这段汇编代码的物理页帧。也就是说，在执行 `__alltraps` 或 `__restore` 函数进行地址空间切换的时候，应用的用户态虚拟地址空间和操作系统内核的内核态虚拟地址空间对切换地址空间的指令所在页的映射方式均是相同的，这就说明了这段切换地址空间的指令控制流仍是可以连续执行的。

 `map_trampoline` 代码如下：

```rust
// os/src/config.rs

pub const TRAMPOLINE: usize = usize::MAX - PAGE_SIZE + 1;

// os/src/mm/memory_set.rs

impl MemorySet {
    /// Mention that trampoline is not collected by areas.
    fn map_trampoline(&mut self) {
        self.page_table.map(
            VirtAddr::from(TRAMPOLINE).into(),
            PhysAddr::from(strampoline as usize).into(),
            PTEFlags::R | PTEFlags::X,
        );
    }
}
```

---

##### Trap机制扩展

为了方便实现，我们在 Trap 上下文中包含更多内容：

```rust
// os/src/trap/context.rs

#[repr(C)]
pub struct TrapContext {
    pub x: [usize; 32],
    pub sstatus: Sstatus,
    pub sepc: usize,
    pub kernel_satp: usize,
    pub kernel_sp: usize,
    pub trap_handler: usize,
}
```

在多出的三个字段中：

- `kernel_satp` 表示内核地址空间的 token ，即内核页表的起始物理地址；
- `kernel_sp` 表示当前应用在内核地址空间中的内核栈栈顶的虚拟地址；
- `trap_handler` 表示内核中 trap handler 入口点的虚拟地址。

它们在应用初始化的时候由内核写入应用地址空间中的 TrapContext 的相应位置，此后就不再被修改。

重点看一下现在的 `__alltraps` 和 `__restore` 各是如何在保存和恢复 Trap 上下文的同时也切换地址空间的：

```assembly
# os/src/trap/trap.S

    .section .text.trampoline
    .globl __alltraps
    .globl __restore
    .align 2
__alltraps:
    csrrw sp, sscratch, sp
    # now sp->*TrapContext in user space, sscratch->user stack
    # save other general purpose registers
    sd x1, 1*8(sp)
    # skip sp(x2), we will save it later
    sd x3, 3*8(sp)
    # skip tp(x4), application does not use it
    # save x5~x31
    .set n, 5
    .rept 27
        SAVE_GP %n
        .set n, n+1
    .endr
    # we can use t0/t1/t2 freely, because they have been saved in TrapContext
    csrr t0, sstatus
    csrr t1, sepc
    sd t0, 32*8(sp)
    sd t1, 33*8(sp)
    # read user stack from sscratch and save it in TrapContext
    csrr t2, sscratch
    sd t2, 2*8(sp)
    # load kernel_satp into t0
    ld t0, 34*8(sp)
    # load trap_handler into t1
    ld t1, 36*8(sp)
    # move to kernel_sp
    ld sp, 35*8(sp)
    # switch to kernel space
    csrw satp, t0
    sfence.vma
    # jump to trap_handler
    jr t1

__restore:
    # a0: *TrapContext in user space(Constant); a1: user space token
    # switch to user space
    csrw satp, a1
    sfence.vma
    csrw sscratch, a0
    mv sp, a0
    # now sp points to TrapContext in user space, start restoring based on it
    # restore sstatus/sepc
    ld t0, 32*8(sp)
    ld t1, 33*8(sp)
    csrw sstatus, t0
    csrw sepc, t1
    # restore general purpose registers except x0/sp/tp
    ld x1, 1*8(sp)
    ld x3, 3*8(sp)
    .set n, 5
    .rept 27
        LOAD_GP %n
        .set n, n+1
    .endr
    # back to user stack
    ld sp, 2*8(sp)
    sret
```

为何我们在 `__alltraps` 中需要借助寄存器 `jr` 而不能直接 `call trap_handler` ？因为在内存布局中，这条 `.text.trampoline` 段中的跳转指令和 `trap_handler` 都在代码段之内，汇编器（Assembler）和链接器（Linker）会根据 `linker-qemu/k210.ld` 的地址布局描述，设定跳转指令的地址，并计算二者地址偏移量，让跳转指令的实际效果为当前 pc 自增这个偏移量。但实际上由于我们设计的缘故，这条跳转指令在被执行的时候，它的虚拟地址被操作系统内核设置在地址空间中的最高页面之内，所以加上这个偏移量并不能正确的得到 `trap_handler` 的入口地址。

**问题的本质可以概括为：跳转指令实际被执行时的虚拟地址和在编译器/汇编器/链接器进行后端代码生成和链接形成最终机器码时设置此指令的地址是不同的。**

---

#### Task管理扩展

首先，任务控制块 `TaskControlBlock` 扩展如下：

```rust
// os/src/task/task.rs

pub struct TaskControlBlock {
    pub task_cx: TaskContext,
    pub task_status: TaskStatus,
    pub memory_set: MemorySet,
    pub trap_cx_ppn: PhysPageNum,
    pub base_size: usize,
}
```

新增了三个字段：

* 应用的地址空间 `memory_set` ；
* 位于应用地址空间次高页的 Trap 上下文被实际存放在物理页帧的物理页号 `trap_cx_ppn`
* `base_size` 统计了应用数据的大小，也就是在应用地址空间中从 0x0 开始到用户栈结束一共包含多少字节。

> ***扩展任务控制块的创建***

```rust
// os/src/config.rs

/// Return (bottom, top) of a kernel stack in kernel space.
pub fn kernel_stack_position(app_id: usize) -> (usize, usize) {
    let top = TRAMPOLINE - app_id * (KERNEL_STACK_SIZE + PAGE_SIZE);
    let bottom = top - KERNEL_STACK_SIZE;
    (bottom, top)
}

// os/src/task/task.rs

impl TaskControlBlock {
    pub fn new(elf_data: &[u8], app_id: usize) -> Self {
        // memory_set with elf program headers/trampoline/trap context/user stack
        let (memory_set, user_sp, entry_point) = MemorySet::from_elf(elf_data);
        let trap_cx_ppn = memory_set
            .translate(VirtAddr::from(TRAP_CONTEXT).into())
            .unwrap()
            .ppn();
        let task_status = TaskStatus::Ready;
        // map a kernel-stack in kernel space
        let (kernel_stack_bottom, kernel_stack_top) = kernel_stack_position(app_id);
        KERNEL_SPACE
            .exclusive_access()
            .insert_framed_area(
                kernel_stack_bottom.into(),
                kernel_stack_top.into(),
                MapPermission::R | MapPermission::W,
            );
        let task_control_block = Self {
            task_status,
            task_cx: TaskContext::goto_trap_return(kernel_stack_top), //***
            memory_set,
            trap_cx_ppn,
            base_size: user_sp,
        };
        // prepare TrapContext in user space
        let trap_cx = task_control_block.get_trap_cx();
        *trap_cx = TrapContext::app_init_context(	///***
            entry_point,
            user_sp,
            KERNEL_SPACE.exclusive_access().token(),
            kernel_stack_top,
            trap_handler as usize,
        );
        task_control_block
    }
}

// os/src/task/context.rs
impl TaskContext {
    pub fn goto_trap_return() -> Self {
        Self {
            ra: trap_return as usize,
            s: [0; 12],
        }
    }
}

// os/src/task/task.rs
impl TaskControlBlock {
    pub fn get_trap_cx(&self) -> &'static mut TrapContext {
        self.trap_cx_ppn.get_mut()
    }
}

// os/src/trap/context.rs
impl TrapContext {
    pub fn set_sp(&mut self, sp: usize) { self.x[2] = sp; }
    pub fn app_init_context(
        entry: usize,
        sp: usize,
        kernel_satp: usize,
        kernel_sp: usize,
        trap_handler: usize,
    ) -> Self {
        let mut sstatus = sstatus::read();
        sstatus.set_spp(SPP::User);
        let mut cx = Self {
            x: [0; 32],
            sstatus,
            sepc: entry,
            kernel_satp,
            kernel_sp,
            trap_handler,
        };
        cx.set_sp(sp);
        cx
    }
}
```

在 `TaskControlBlock::new` 函数中，最关键的是`task_cx` 和 `trap_cx` 的初始化：

* `task_cx` 构造

  在应用的内核栈顶压入一个跳转到 `trap_return` 而不是 `__restore` 的任务上下文，这主要是为了能够支持对该应用的启动并顺利切换到用户地址空间执行。在构造方式上，只是将 ra 寄存器的值设置为 `trap_return` 的地址。 `trap_return` 是后面要介绍的新版的 Trap 处理的一部分；

* `trap_cx` 构造

  调用 `TrapContext::app_init_context` 函数。和之前实现相比，`TrapContext::app_init_context` 需要补充上让应用在 `__alltraps` 能够顺利进入到内核地址空间并跳转到 trap handler 入口点的相关信息。

当每个应用第一次获得 CPU 使用权即将进入用户态执行的时候，它的内核栈顶放置着我们在内核加载应用的时候构造的一个任务上下文。在 `__switch` 切换到该应用的任务上下文的时候，内核将会跳转到 `trap_return` 并返回用户态开始该应用的启动执行。

> ***内核初始化时加载所有应用***

在内核初始化的时候，需要将所有的应用加载到全局应用管理器中：

```rust
// os/src/task/mod.rs

struct TaskManagerInner {
    tasks: Vec<TaskControlBlock>,
    current_task: usize,
}

lazy_static! {
    pub static ref TASK_MANAGER: TaskManager = {
        println!("init TASK_MANAGER");
        let num_app = get_num_app();
        println!("num_app = {}", num_app);
        let mut tasks: Vec<TaskControlBlock> = Vec::new();
        for i in 0..num_app {
            tasks.push(TaskControlBlock::new(
                get_app_data(i),
                i,
            ));
        }
        TaskManager {
            num_app,
            inner: RefCell::new(TaskManagerInner {
                tasks,
                current_task: 0,
            }),
        }
    };
}
```

---

#### Trap处理扩展

让我们来看现在 `trap_handler` 的改进实现：

```rust
// os/src/trap/mod.rs
fn set_kernel_trap_entry() {
    unsafe {
        stvec::write(trap_from_kernel as usize, TrapMode::Direct);
    }
}

#[no_mangle]
pub fn trap_from_kernel() -> ! {
    panic!("a trap from kernel!"); //SBI...
}

#[no_mangle]
pub fn trap_handler() -> ! {
    set_kernel_trap_entry();
    let cx = current_trap_cx();
    let scause = scause::read();
    let stval = stval::read();
    match scause.cause() {
        //...
    }
    trap_return();
}

// os/src/trap/mod.rs
fn set_user_trap_entry() {
    unsafe {
        stvec::write(TRAMPOLINE as usize, TrapMode::Direct);
    }
}

#[no_mangle]
pub fn trap_return() -> ! {
    set_user_trap_entry();
    let trap_cx_ptr = TRAP_CONTEXT;
    let user_satp = current_user_token();
    extern "C" {
        fn __alltraps();
        fn __restore();
    }
    let restore_va = __restore as usize - __alltraps as usize + TRAMPOLINE;
    unsafe {
        asm!(
            "fence.i",
            "jr {restore_va}",
            restore_va = in(reg) restore_va,
            in("a0") trap_cx_ptr,
            in("a1") user_satp,
            options(noreturn)
        );
    }
    panic!("Unreachable in back_to_user!");
}
```

* 由于应用的 Trap 上下文不在内核地址空间，因此我们调用 `current_trap_cx` 来获取当前应用的 Trap 上下文的可变引用而不是像之前那样作为参数传入 `trap_handler` ；

* 在 `trap_handler` 完成 Trap 处理之后，我们需要调用 `trap_return` 返回用户态：

  * 首先调用 `set_user_trap_entry` ，来让应用 Trap 到 S 的时候可以跳转到 `__alltraps` ；

    > 我们把 `stvec` 设置为内核和应用地址空间共享的跳板页面的起始地址 `TRAMPOLINE` 而不是编译器在链接时看到的 `__alltraps` 的地址。这是因为启用分页模式之后，内核只能通过跳板页面上的虚拟地址来实际取得 `__alltraps` 和 `__restore` 的汇编代码。

  * 最后我们需要跳转到 `__restore` ，同时需要携带两个参数 `trap_cx_ptr` 和 `user_satp`，以执行：切换到应用地址空间、从 Trap 上下文中恢复通用寄存器、 `sret` 继续执行应用；

    >关键在于如何找到 `__restore` 在内核/应用地址空间中共同的虚拟地址？
    >
    >由于 `__alltraps` 是对齐到地址空间跳板页面的起始地址 `TRAMPOLINE` 上的， 则 `__restore` 的虚拟地址只需在 `TRAMPOLINE` 基础上加上 `__restore` 相对于 `__alltraps` 的偏移量即可。这里 `__alltraps` 和 `__restore` 都是指编译器在链接时看到的内核内存布局中的地址。

### 2.6 超越物理内存的地址空间

## 3 疑难解决

## *4 //TODO 实验





#  五 进程管理

## 1 概述

到目前为止，操作系统启动后，能运行完它管理所有的应用程序。但在整个执行过程中，应用程序是被动地被操作系统加载运行，开发者与操作系统之间没有交互，开发者与应用程序之间没有交互，应用程序不能控制其它应用的执行。这使得开发者不能灵活地选择执行某个程序。为了方便开发者灵活执行程序，本章要完成的操作系统的核心目标是： **让开发者能够控制程序的运行** 。

于是，本章开发了一个用户 **终端** (Terminal) 程序或称 **命令行** 应用（Command Line Application, 俗称 **shell** ），形成用户与操作系统进行交互的命令行界面（Command Line Interface），它就和我们今天常用的 OS 中的命令行应用（如 Linux 中的 bash，Windows 中的 CMD 等）没有什么不同：只需在其中输入命令即可启动或杀死应用，或者监控系统的运行状况。这自然是现代 OS 中不可缺少的一部分，并大大增加了系统的 **可交互性** ，使得用户可以更加灵活地控制系统。

为了在用户态就可以借助操作系统的服务动态灵活地管理和控制应用的执行，我们需要在已有的 **任务** 抽象的基础上进一步扩展，形成新的抽象： **进程** ，并实现若干基于 **进程** 的强大系统调用。

![image-20231213165008083](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312131650133.png)

通过上图，大致可以看出 ProcessOS 在内部结构上没有特别大的改动，但把任务抽象进化成了进程抽象，其主要改动集中在进程管理的功能上：  

* 提供新的系统调用服务：`sys_fork` (创建子进程)、`sys_waitpid` (等待子进程结束并回收子进程资源)、`sys_exec`（用新的应用内容覆盖当前进程，即达到执行新应用的目的）；
* 为了让用户能够输入命令或执行程序的名字，ProcessOS 还增加了一个 `read` 系统调用服务，这样用户通过操作系统的命令行接口 – 新添加的 shell 应用程序发出命令，来动态地执行各种新的应用，提高了用户与操作系统之间的交互能力。

而由于有了进程的新抽象，需要对已有任务控制块进行重构，ProcessOS 中与进程相关的核心数据结构如下图所示：

![image-20231213165607535](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312131657973.png)

* 进程控制块 `TaskControlBlock` 包含与进程运行/切换/调度/地址空间相关的各种资源和信息；
* 以前的任务管理器 `TaskManager` 分离为处理器管理结构 `Processor` 和新的 `TaskManager` ：
  *  `Processor` 负责管理 CPU 上正在执行的任务和一些相关信息；
  * 新的任务管理器 `TaskManager` 仅负责管理没在执行的所有任务，以及各种新的进程管理相关的系统调用服务。

---

本章对操作系统的更新，可总结为 **用户/内核** 两方面：

* **用户层**
  * 在 user/src/lib.rs 中新增 `sys_fork/sys_exec/sys_waitpid` 等函数，将对应的系统调用按照与内核约定的 ABI 在 `syscall` 中转化为一条用于触发系统调用的 `ecall` 的指令；
  * 用户初始程序 `initproc.rs` 和 shell 程序 `user_shell.rs` ，可以认为它们位于内核和其他应用程序之间的中间层提供一些基础功能，但是它们仍处于用户态的应用层。前者会被内核唯一自动加载、也是最早加载并执行，后者则负责从键盘接收用户输入的应用名并执行对应的应用；
* **内核层**
  * **基于应用名的链接与加载支持**
  * **重构进程管理机制的核心数据结构**
    * 分离 `TaskManager` 结构：任务管理器 `TaskManager` 负责管理所有的任务状态，`Processor`维护着 CPU 当前正在执行的任务；
    * 任务控制块 `TaskControlBlock` 新增了 PID 、内核栈、应用数据大小、父子进程、退出码等信息；
  * **进程管理机制的设计实现**
    * 初始进程的创建：内核初始化时调用 `add_initproc`；
    * 进程切换机制：调用 `schedule` 函数进行进程切换，它会首先切换到处理器的 idle 控制流（即 `Processor::run` 方法），然后在里面选取要切换到的进程并切换过去；
    * 进程调度机制：在进程切换的时候我们需要选取一个进程切换过去 `TaskManager::fetch_task`；
    * 进程生成机制：系统调用 `fork/exec`；
    * 进程资源回收机制：当一个进程主动退出或出错退出的时候，它的父进程通过 `waitpid` 系统调用回收该进程的全部资源；
    * 进程的 I/O 输入机制：为了支持用户终端 `user_shell` 读取用户键盘输入的功能，还需要实现 `read` 系统调用；

---

```rust
#[no_mangle]
/// the rust entry-point of os
pub fn rust_main() -> ! {
    clear_bss();
    println!("[kernel] Hello, world!");
    mm::init();
    mm::remap_test();
    task::add_initproc();
    println!("after initproc!");
    trap::init();
    //trap::enable_interrupt();
    trap::enable_timer_interrupt();
    timer::set_next_trigger();
    loader::list_apps();
    task::run_tasks();
    panic!("Unreachable in rust_main!");
}
```

## 2 重点内容梳理

### 2.1 关于进程模型

可执行文件本身可以看成一张编译器解析源代码之后总结出的一张记载如何利用各种硬件资源进行一轮生产流程的 **蓝图** 。而内核的一大功能便是作为一个硬件资源管理器，它可以随时启动一轮生产流程（即执行任意一个应用），这需要选中一张蓝图（此时确定执行哪个可执行文件），接下来就需要内核按照蓝图上所记载的对资源的需求来对应的将各类资源分配给它，让这轮生产流程得以顺利进行。当按照蓝图上的记载生产流程完成（应用退出）之后，内核还需要将对应的硬件资源回收以便后续的重复利用。

因此，进程就是操作系统选取某个可执行文件并对其进行一次动态执行的过程。相比可执行文件，它的动态性主要体现在：

1. 它是一个过程，从时间上来看有开始也有结束；
2. 在该过程中对于可执行文件中给出的需求要相应对 **硬件/虚拟资源** 进行 **动态绑定和解绑** 。

这里需要指出的是，两个进程可以选择同一个可执行文件执行，然而它们却是截然不同的进程：它们的启动时间、占据的硬件资源、输入数据均有可能是不同的，这些条件均会导致它们是不一样的执行过程。

---

本章实现的内核中采用的一种非常简单的进程模型：

* 这个进程模型有三个运行状态：就绪态、运行态和等待态；
* 有基于独立页表的地址空间；
* 可被操作系统调度来分时占用 CPU 执行；
* 可以动态创建和退出；
* 可通过系统调用获得操作系统的服务。

我们实现的进程模型建立在地址空间抽象之上：每个进程都需要一个地址空间，它涵盖了它选择的可执行文件的内存布局，还包含一些其他的逻辑段。且进程模型需要操作系统支持一些重要的系统调用：创建进程、执行新程序、等待进程结束等，来达到应用程序执行的动态灵活性。

### 2.2 用户层

#### fork/exec/waitpid系统调用

##### fork系统调用

在内核初始化完毕之后会创建一个进程——即 **用户初始进程** (Initial Process) ，它是目前在内核中以硬编码方式创建的唯一一个进程。其他所有的进程都是通过一个名为 `fork` 的系统调用来创建的。

```rust
/// 功能：当前进程 fork 出来一个子进程。
/// 返回值：对于子进程返回 0，对于当前进程则返回子进程的 PID 。
/// syscall ID：220
pub fn sys_fork() -> isize;
```

进程 A 调用 `fork` 创建一个新进程 B：

* 两者包含的用户态的代码段、堆栈段及其他数据段的内容完全相同，但是它们是被放在两个独立的地址空间中的，两个进程通用寄存器也几乎完全相同。

* 两者唯有用来保存 `fork` 系统调用返回值的 a0 寄存器的值是不同的。这区分了两个进程：原进程的返回值为它新创建进程的 PID ，而新创建进程的返回值为 0 。
* 由于新的进程是原进程主动调用 `fork` 衍生出来的，我们称新进程为原进程的 **子进程** (Child Process) ，相对的原进程则被称为新进程的 **父进程** (Parent Process) 。相比创建一个进程， `fork` 的另一个重要功能是建立一对新的父子关系，两者更容易进行合作或通信。

---

##### exec系统调用

如果仅有 `fork` 的话，那么所有的进程都只能和用户初始进程一样执行同样的代码段，我们还需要引入 `exec` 系统调用来执行不同的可执行文件：

```rust
/// 功能：将当前进程的地址空间清空并加载一个特定的可执行文件，返回用户态后开始它的执行。
/// 参数：path 给出了要加载的可执行文件的名字；
/// 返回值：如果出错的话（如找不到名字相符的可执行文件）则返回 -1，否则不应该返回。
/// syscall ID：221
pub fn sys_exec(path: &str) -> isize;

// user/src/exec.rs
pub fn sys_exec(path: &str) -> isize {
    syscall(SYSCALL_EXEC, [path.as_ptr() as usize, 0, 0])
}
```

利用 `fork` 和 `exec` 的组合，我们很容易在一个进程内 `fork` 出一个子进程并执行一个特定的可执行文件。

---

##### waitpid系统调用

当进程退出的时候内核立即回收一部分资源并将该进程标记为 **僵尸进程** (Zombie Process) 。之后，由该进程的父进程通过一个名为 `waitpid` 的系统调用来收集该进程的返回状态并回收掉它所占据的全部资源，这样这个进程才被彻底销毁。系统调用 `waitpid` 的原型如下：

```rust
/// 功能：当前进程等待一个子进程变为僵尸进程，回收其全部资源并收集其返回值。
/// 参数：pid 表示要等待的子进程的进程 ID，如果为 -1 的话表示等待任意一个子进程；
/// exit_code 表示保存子进程返回值的地址，如果这个地址为 0 的话表示不必保存。
/// 返回值：如果要等待的子进程不存在则返回 -1；否则如果要等待的子进程均未结束则返回 -2；
/// 否则返回结束的子进程的进程 ID。
/// syscall ID：260
pub fn sys_waitpid(pid: isize, exit_code: *mut i32) -> isize;
```

如果一个进程先于它的子进程结束，在它退出的时候，它的所有子进程将成为进程树的根节点——用户初始进程的子进程，同时这些子进程的父进程也会转成用户初始进程。这之后，这些子进程的资源就由用户初始进程负责回收了，这也是用户初始进程很重要的一个用途。

---

#### 用户初始程序: initproc

```rust
// user/src/bin/initproc.rs
#![no_std]
#![no_main]

#[macro_use]
extern crate user_lib;

use user_lib::{
    fork,
    wait,
    exec,
    yield_,
};

#[no_mangle]
fn main() -> i32 {
    if fork() == 0 {
        exec("user_shell\0");
    } else {
        loop {
            let mut exit_code: i32 = 0;
            let pid = wait(&mut exit_code);
            if pid == -1 {
                yield_();
                continue;
            }
            println!(
                "[initproc] Released a zombie process, pid={}, exit_code={}",
                pid,
                exit_code,
            );
        }
    }
    0
}
```

#### shell程序: user_shell

```rust
/// 功能：从文件中读取一段内容到缓冲区。
/// 参数：fd 是待读取文件的文件描述符，切片 buffer 则给出缓冲区。
/// 返回值：如果出现了错误则返回 -1，否则返回实际读到的字节数。
/// syscall ID：63
pub fn sys_read(fd: usize, buffer: &mut [u8]) -> isize;

// user/src/syscall.rs
pub fn sys_read(fd: usize, buffer: &mut [u8]) -> isize {
    syscall(SYSCALL_READ, [fd, buffer.as_mut_ptr() as usize, buffer.len()])
}

// user/src/lib.rs
pub fn read(fd: usize, buf: &mut [u8]) -> isize { sys_read(fd, buf) }

// user/src/console.rs
const STDIN: usize = 0;

pub fn getchar() -> u8 {
    let mut c = [0u8; 1];
    read(STDIN, &mut c);
    c[0]
}

// user/src/bin/user_shell.rs
#![no_std]
#![no_main]

extern crate alloc;

#[macro_use]
extern crate user_lib;

const LF: u8 = 0x0au8;
const CR: u8 = 0x0du8;
const DL: u8 = 0x7fu8;
const BS: u8 = 0x08u8;

use alloc::string::String;
use user_lib::{fork, exec, waitpid, yield_};
use user_lib::console::getchar;

#[no_mangle]
#[no_mangle]
pub fn main() -> i32 {
    println!("Rust user shell");
    let mut line: String = String::new();
    print!(">> ");
    loop {
        let c = getchar();
        match c {
            LF | CR => {
                println!("");
                if !line.is_empty() {
                    line.push('\0');
                    let pid = fork();
                    if pid == 0 {
                        // child process
                        if exec(line.as_str()) == -1 {
                            println!("Error when executing!");
                            return -4;
                        }
                        unreachable!();
                    } else {
                        let mut exit_code: i32 = 0;
                        let exit_pid = waitpid(pid as usize, &mut exit_code);
                        assert_eq!(pid, exit_pid);
                        println!(
                            "Shell: Process {} exited with code {}",
                            pid, exit_code
                        );
                    }
                    line.clear();
                }
                print!(">> ");
            }
			BS | DL => {
                if !line.is_empty() {
                    print!("{}", BS as char);
                    print!(" ");
                    print!("{}", BS as char);
                    line.pop();
                }
            }
            _ => {
                print!("{}", c as char);
                line.push(c as char);
            }
        }
    }
}
```

### 2.3 内核层

#### 基于应用名的链接与加载

在实现 `exec` 系统调用的时候，我们需要根据应用的名字而不仅仅是一个编号来获取应用的 ELF 格式数据。修改 Rust 辅助脚本，以支持生成包含应用名的链接文件 `link_app.S`。

```assembly
// os/build.rs
for i in 0..apps.len() {
    writeln!(f, r#"    .quad app_{}_start"#, i)?;
}
writeln!(f, r#"    .quad app_{}_end"#, apps.len() - 1)?;

writeln!(f, r#"
.global _app_names
_app_names:"#)?;
for app in apps.iter() {
    writeln!(f, r#"    .string "{}""#, app)?;
}

for (idx, app) in apps.iter().enumerate() {
    ...
}

// os/link_app.S
 .section .data
    .global _num_app
_num_app:
    .quad 15
    .quad app_0_start
    .quad app_1_start
......
    .global _app_names
_app_names:
    .string "exit"
    .string "fantastic_text"
......
    .section .data
    .global app_0_start
    .global app_0_end
    .align 3
app_0_start:
    .incbin "../user/target/riscv64gc-unknown-none-elf/release/exit"
app_0_end:
......
```

---

而在加载器 `loader.rs` 中，我们会分析 `link_app.S` 中的内容，并用一个全局可见的 *只读* 向量 `APP_NAMES` 来按照顺序将所有应用的名字保存在内存中（注意链接器会自动在每个字符串的结尾加入分隔符 `\0` ）：

```rust
// os/src/loader.rs
lazy_static! {
    static ref APP_NAMES: Vec<&'static str> = {
        let num_app = get_num_app();
        extern "C" { fn _app_names(); }
        let mut start = _app_names as usize as *const u8;
        let mut v = Vec::new();
        unsafe {
            for _ in 0..num_app {
                let mut end = start;
                while end.read_volatile() != '\0' as u8 {
                    end = end.add(1);
                }
                let slice = core::slice::from_raw_parts(start, end as usize - start as usize);
                let str = core::str::from_utf8(slice).unwrap();
                v.push(str);
                start = end.add(1);
            }
        }
        v
    };
}
```

使用 `get_app_data_by_name` 可以按照应用的名字来查找获得应用的 ELF 数据，而 `list_apps` 在内核初始化时被调用，它可以打印出所有可用的应用的名字。

```rust
// os/src/loader.rs
pub fn get_app_data_by_name(name: &str) -> Option<&'static [u8]> {
    let num_app = get_num_app();
    (0..num_app)
        .find(|&i| APP_NAMES[i] == name)
        .map(|i| get_app_data(i))
}

pub fn list_apps() {
    println!("/**** APPS ****");
    for app in APP_NAMES.iter() {
        println!("{}", app);
    }
    println!("**************/")
}
```

#### 进程管理的核心数据结构

##### 进程标识符与内核栈

进程标识符 `PidHandle` 内核栈 `KernelStack` ：

* 同一时间存在的所有进程都有一个唯一的进程标识符，它们是互不相同的整数，这样才能表示表示进程的唯一性。这里我们使用 RAII 的思想，将其抽象为一个 `PidHandle` 类型，当它的生命周期结束后对应的整数会被编译器自动回收；
* 内核栈 `KernelStack` 也用到了 RAII 的思想，具体来说，实际保存它的物理页帧的生命周期与它绑定在一起，当 `KernelStack` 生命周期结束后，这些物理页帧也将会被编译器自动回收。本章将之前的应用编号替换为进程标识符，我们可以在内核栈 `KernelStack` 中保存着它所属进程的 PID 。

可以看到，两者都利用到了 RAII 思想，因此都需要实现 `Drop` trait。

> `PidHandle`

```rust
// os/src/task/pid.rs
pub struct PidHandle(pub usize);

// os/src/task/pid.rs
struct PidAllocator {
    current: usize,
    recycled: Vec<usize>,
}

impl PidAllocator {
    pub fn new() -> Self {
        PidAllocator {
            current: 0,
            recycled: Vec::new(),
        }
    }
    pub fn alloc(&mut self) -> PidHandle {
        if let Some(pid) = self.recycled.pop() {
            PidHandle(pid)
        } else {
            self.current += 1;
            PidHandle(self.current - 1)
        }
    }
    pub fn dealloc(&mut self, pid: usize) {
        assert!(pid < self.current);
        assert!(
            self.recycled.iter().find(|ppid| **ppid == pid).is_none(),
            "pid {} has been deallocated!", pid
        );
        self.recycled.push(pid);
    }
}

lazy_static! {
    static ref PID_ALLOCATOR : UPSafeCell<PidAllocator> = unsafe {
        UPSafeCell::new(PidAllocator::new())
    };
}

// os/src/task/pid.rs
pub fn pid_alloc() -> PidHandle {
    PID_ALLOCATOR.exclusive_access().alloc()
}

// os/src/task/pid.rs
impl Drop for PidHandle {
    fn drop(&mut self) {
        PID_ALLOCATOR.exclusive_access().dealloc(self.0);
    }
}
```

---

> `KernelStack`

```rust
// os/src/task/pid.rs
pub struct KernelStack {
    pid: usize,
}

// os/src/task/pid.rs
/// Return (bottom, top) of a kernel stack in kernel space.
pub fn kernel_stack_position(app_id: usize) -> (usize, usize) {
    let top = TRAMPOLINE - app_id * (KERNEL_STACK_SIZE + PAGE_SIZE);
    let bottom = top - KERNEL_STACK_SIZE;
    (bottom, top)
}

impl KernelStack {
    pub fn new(pid_handle: &PidHandle) -> Self {
        let pid = pid_handle.0;
        let (kernel_stack_bottom, kernel_stack_top) = kernel_stack_position(pid);
        KERNEL_SPACE
            .exclusive_access()
            .insert_framed_area(
                kernel_stack_bottom.into(),
                kernel_stack_top.into(),
                MapPermission::R | MapPermission::W,
            );
        KernelStack {
            pid: pid_handle.0,
        }
    }
    pub fn push_on_top<T>(&self, value: T) -> *mut T where
        T: Sized, {
        let kernel_stack_top = self.get_top();
        let ptr_mut = (kernel_stack_top - core::mem::size_of::<T>()) as *mut T;
        unsafe { *ptr_mut = value; }
        ptr_mut
    }
    pub fn get_top(&self) -> usize {
        let (_, kernel_stack_top) = kernel_stack_position(self.pid);
        kernel_stack_top
    }
}

// os/src/task/pid.rs
impl Drop for KernelStack {
    fn drop(&mut self) {
        let (kernel_stack_bottom, _) = kernel_stack_position(self.pid);
        let kernel_stack_bottom_va: VirtAddr = kernel_stack_bottom.into();
        KERNEL_SPACE
            .exclusive_access()
            .remove_area_with_start_vpn(kernel_stack_bottom_va.into());
    }
}
```

---

##### 进程控制块

承接前面的章节，我们仅需对任务控制块 `TaskControlBlock` 进行若干改动并让它直接承担进程控制块的功能：

```rust
// os/src/task/task.rs
pub struct TaskControlBlock {
    // immutable
    pub pid: PidHandle,
    pub kernel_stack: KernelStack,
    // mutable
    inner: UPSafeCell<TaskControlBlockInner>,
}

pub struct TaskControlBlockInner {
    pub trap_cx_ppn: PhysPageNum,
    pub base_size: usize,
    pub task_cx: TaskContext,
    pub task_status: TaskStatus,
    pub memory_set: MemorySet,
    pub parent: Option<Weak<TaskControlBlock>>,
    pub children: Vec<Arc<TaskControlBlock>>,
    pub exit_code: i32,
}
```

`TaskControlBlock` 包含两部分：

- 在初始化之后就不再变化的元数据：直接放在任务控制块中。这里将进程标识符 `PidHandle` 和内核栈 `KernelStack` 放在其中；
- 在运行过程中可能发生变化的元数据：则放在 `TaskControlBlockInner` 中，将它再包裹上一层 `UPSafeCell<T>` 放在任务控制块中。

任务控制块 `TaskControlBlock` 目前提供以下方法：

```rust
// os/src/task/task.rs

impl TaskControlBlock {
    pub fn inner_exclusive_access(&self) -> RefMut<'_, TaskControlBlockInner> {
        self.inner.exclusive_access()
    }
    pub fn getpid(&self) -> usize {
        self.pid.0
    }
    pub fn new(elf_data: &[u8]) -> Self {...}
    pub fn exec(&self, elf_data: &[u8]) {...}
    pub fn fork(self: &Arc<TaskControlBlock>) -> Arc<TaskControlBlock> {...}
}
```

- `new` 用来创建一个新的进程，目前仅用于内核中手动创建唯一一个初始进程 `initproc` 。
- `exec` 用来实现 `exec` 系统调用，即当前进程加载并执行另一个 ELF 格式可执行文件。
- `fork` 用来实现 `fork` 系统调用，即当前进程 fork 出来一个与之几乎相同的子进程。

---

##### 进程管理器

本章将任务管理器 `TaskManager` 对于 CPU 的监控职能拆分到下面即将介绍的处理器管理结构 `Processor` 中去，`TaskManager` 自身仅负责管理所有任务。

```rust
// os/src/task/manager.rs
pub struct TaskManager {
    ready_queue: VecDeque<Arc<TaskControlBlock>>,
}

/// A simple FIFO scheduler.
impl TaskManager {
    pub fn new() -> Self {
        Self { ready_queue: VecDeque::new(), }
    }
    pub fn add(&mut self, task: Arc<TaskControlBlock>) {
        self.ready_queue.push_back(task);
    }
    pub fn fetch(&mut self) -> Option<Arc<TaskControlBlock>> {
        self.ready_queue.pop_front()
    }
}

lazy_static! {
    pub static ref TASK_MANAGER: UPSafeCell<TaskManager> = unsafe {
        UPSafeCell::new(TaskManager::new())
    };
}

pub fn add_task(task: Arc<TaskControlBlock>) {
    TASK_MANAGER.exclusive_access().add(task);
}

pub fn fetch_task() -> Option<Arc<TaskControlBlock>> {
    TASK_MANAGER.exclusive_access().fetch()
}
```

---

#### 处理器的执行状态

##### 正在执行的任务

处理器管理结构 `Processor` 负责从任务管理器 `TaskManager` 中分出去的维护 CPU 状态的职责：

```rust
// os/src/task/processor.rs
pub struct Processor {
    current: Option<Arc<TaskControlBlock>>, // 当前处理器正在执行的任务
    idle_task_cx: TaskContext,				// 当前处理器的idel控制流上下文
}

impl Processor {
    pub fn new() -> Self {
        Self {
            current: None,
            idle_task_cx: TaskContext::zero_init(),
        }
    }
}

// 在单核CPU环境下，我们仅创建单个 Processor 的全局实例 
// os/src/task/processor.rs
lazy_static! {
    pub static ref PROCESSOR: UPSafeCell<Processor> = unsafe {
        UPSafeCell::new(Processor::new())
    };
}
```

在抢占式调度模型中，在一个处理器上执行的任务常常被换入或换出，因此我们需要维护在一个处理器上正在执行的任务，可以查看它的信息或对它进行替换。

---

##### 任务调度的idle控制流

`Processor` 有一个不同的 idle 控制流，它运行在这个 CPU 核的启动栈上，功能是尝试从任务管理器中选出一个任务来在当前 CPU 核上执行。在内核初始化完毕之后，会通过调用 `run_tasks` 函数来进入 idle 控制流：

```rust
// os/src/task/processor.rs
pub fn run_tasks() {
    loop {
        let mut processor = PROCESSOR.exclusive_access();
        if let Some(task) = fetch_task() {
            let idle_task_cx_ptr = processor.get_idle_task_cx_ptr();
            // access coming task TCB exclusively
            let mut task_inner = task.inner_exclusive_access();
            let next_task_cx_ptr = &task_inner.task_cx as *const TaskContext;
            task_inner.task_status = TaskStatus::Running;
            // stop exclusively accessing coming task TCB manually
            drop(task_inner);
            processor.current = Some(task);
            // stop exclusively accessing processor manually
            drop(processor);
            unsafe {
                __switch(
                    idle_task_cx_ptr,
                    next_task_cx_ptr,
                );
            }
        }
    }
}

impl Processor {
    fn get_idle_task_cx_ptr(&mut self) -> *mut TaskContext {
        &mut self.idle_task_cx as *mut _
    }
}
```

可以看到，调度功能的主体是 `run_tasks()` 。它循环调用 `fetch_task` 直到顺利从任务管理器中取出一个任务，随后调用 `__switch` 来从当前的 idle 控制流切换到接下来要执行的任务。

---

上面介绍了从 idle 控制流通过任务调度切换到某个任务开始执行的过程。而反过来，当一个应用用尽了内核本轮分配给它的时间片或者它主动调用 `yield` 系统调用交出 CPU 使用权之后，内核会调用 `schedule` 函数来切换到 idle 控制流并开启新一轮的任务调度。

```rust
// os/src/task/processor.rs
pub fn schedule(switched_task_cx_ptr: *mut TaskContext) {
    let mut processor = PROCESSOR.exclusive_access();
    let idle_task_cx_ptr = processor.get_idle_task_cx_ptr();
    drop(processor);
    unsafe {
        __switch(
            switched_task_cx_ptr,
            idle_task_cx_ptr,
        );
    }
}
```

这里，我们需要传入即将被切换出去的任务的 task_cx_ptr 来在合适的位置保存任务上下文，之后就可以通过 `__switch` 来切换到 idle 控制流。从源代码来看，切换回去之后，内核将跳转到 `run_tasks` 中 `__switch` 返回之后的位置，也即开启了下一轮的调度循环。

---

##### 为什么要设计idle控制流？

读到这里不免会产生一些疑问，为什么需要一个额外的 `idle_task_cx` ？相当于在之前的 `current_task_cx` 和 `next_task_cx` 之前嵌入一个控制流，看起来多了一次上下文切换的开销，这么设计有什么好处？

>这样做的主要目的是使得换入/换出进程和调度执行流在内核层各自执行在不同的内核栈上，分别是进程自身的内核栈和内核初始化时使用的启动栈。这样的话，调度相关的数据不会出现在进程内核栈上，也使得调度机制对于换出进程的Trap执行流是不可见的，它在决定换出的时候只需调用schedule而无需操心调度的事情。从而各执行流的分工更加明确了，虽然带来了更大的开销。
>
>idle 控制流的设计和 xv6 的很像，rCore 让 `run_tasks` 始终保持运行， 可以把这个函数想象成一个游戏机，它的作用就是不断挑选可运行的游戏卡（进程），然后放到 CPU 这个卡槽中。一定时间后这个 CPU 卡槽要给别的游戏卡（进程）用了就把这个当前的卡弹出来（这里就用了 `schedule` 函数弹出），然后游戏机又开始选下一张能用的游戏卡（进程）了。
>
>游戏卡（进程）只知道自己被插上要运行，以及到某个点自己要弹出来，它不知道游戏机的存在，这样就做到了任务调度的透明。

---

#### 进程管理机制的设计实现

##### 创建初始进程 - INITPROC

内核初始化完毕之后即会调用 `task` 子模块提供的 `add_initproc` 函数来将初始进程 `initproc` 加入任务管理器，但在这之前我们需要初始化初始进程的进程控制块 `INITPROC` ，这个过程基于 `lazy_static` 在运行时完成。

```rust
// os/src/task/mod.rs
use crate::loader::get_app_data_by_name;
use manager::add_task;

lazy_static! {
    pub static ref INITPROC: Arc<TaskControlBlock> = Arc::new(
        TaskControlBlock::new(get_app_data_by_name("initproc").unwrap())
    );
}

pub fn add_initproc() {
    add_task(INITPROC.clone());
}

// os/src/task/task.rs
use super::{PidHandle, pid_alloc, KernelStack};
use super::TaskContext;
use crate::config::TRAP_CONTEXT;
use crate::trap::TrapContext;

// impl TaskControlBlock
pub fn new(elf_data: &[u8]) -> Self {
    // memory_set with elf program headers/trampoline/trap context/user stack
    let (memory_set, user_sp, entry_point) = MemorySet::from_elf(elf_data);
    let trap_cx_ppn = memory_set
        .translate(VirtAddr::from(TRAP_CONTEXT).into())
        .unwrap()
        .ppn();
    // alloc a pid and a kernel stack in kernel space
    let pid_handle = pid_alloc();
    let kernel_stack = KernelStack::new(&pid_handle);
    let kernel_stack_top = kernel_stack.get_top();
    // push a task context which goes to trap_return to the top of kernel stack
    let task_control_block = Self {
        pid: pid_handle,
        kernel_stack,
        inner: unsafe { UPSafeCell::new(TaskControlBlockInner {
            trap_cx_ppn,
            base_size: user_sp,
            task_cx: TaskContext::goto_trap_return(kernel_stack_top),
            task_status: TaskStatus::Ready,
            memory_set,
            parent: None,
            children: Vec::new(),
            exit_code: 0,
        })},
    };
    // prepare TrapContext in user space
    let trap_cx = task_control_block.inner_exclusive_access().get_trap_cx();
    *trap_cx = TrapContext::app_init_context(
        entry_point,
        user_sp,
        KERNEL_SPACE.exclusive_access().token(),
        kernel_stack_top,
        trap_handler as usize,
    );
    task_control_block
}
```

##### 进程调度机制 - yield/exit/timer/panic

当应用调用 `sys_yield` 主动交出使用权、本轮时间片用尽或者由于某些原因内核中的处理无法继续的时候，就会在内核中调用 `suspend_current_and_run_next` 函数触发调度机制并进行任务切换，代码如下：

```rust
// os/src/task/mod.rs
use processor::{task_current_task, schedule};
use manager::add_task;

pub fn suspend_current_and_run_next() {
    // There must be an application running.
    let task = take_current_task().unwrap();

    // ---- access current TCB exclusively
    let mut task_inner = task.inner_exclusive_access();
    let task_cx_ptr = &mut task_inner.task_cx as *mut TaskContext;
    // Change status to Ready
    task_inner.task_status = TaskStatus::Ready;
    drop(task_inner);
    // ---- stop exclusively accessing current PCB

    // push back to ready queue.
    add_task(task);
    // jump to scheduling cycle
    schedule(task_cx_ptr);
}
```

首先通过 `take_current_task` 来取出当前正在执行的任务，修改其进程控制块内的状态，随后将这个任务放入任务管理器的队尾。接着调用 `schedule` 函数来触发调度并切换任务。注意，当仅有一个任务的时候， `suspend_current_and_run_next` 的效果是会继续执行这个任务。

---

##### 进程生成机制 - fork/exec

在内核中手动生成的进程只有初始进程 `initproc` ，余下所有的进程都是它直接或间接 fork 出来的。当一个子进程被 fork 出来之后，它可以调用 `exec` 系统调用来加载并执行另一个可执行文件。因此， `fork/exec` 两个系统调用提供了进程的生成机制。下面我们分别来介绍二者的实现。

> ***fork 内核实现***

* `MemorySet::from_existed_user` 拷贝生成一份和原进程相同的地址空间；
* `TaskControlBlock::fork` 构建进程控制块，其工作包括：维护父子进程关系、分配PID等；
* `sys_fork` 区分了父子进程，父进程返回 new_pid，子进程返回 0；

`fork`为子进程创建一个和父进程几乎完全相同的应用地址空间：

```rust
// os/src/mm/memory_set.rs
impl MapArea {
    pub fn from_another(another: &MapArea) -> Self {
        Self {
            vpn_range: VPNRange::new(
                another.vpn_range.get_start(),
                another.vpn_range.get_end()
            ),
            data_frames: BTreeMap::new(),
            map_type: another.map_type,
            map_perm: another.map_perm,
        }
    }
}

impl MemorySet {
    pub fn from_existed_user(user_space: &MemorySet) -> MemorySet {
        let mut memory_set = Self::new_bare();
        // map trampoline
        memory_set.map_trampoline();
        // copy data sections/trap_context/user_stack
        for area in user_space.areas.iter() {
            let new_area = MapArea::from_another(area);
            memory_set.push(new_area, None);
            // copy data from another space
            for vpn in area.vpn_range {
                let src_ppn = user_space.translate(vpn).unwrap().ppn();
                let dst_ppn = memory_set.translate(vpn).unwrap().ppn();
                dst_ppn.get_bytes_array().copy_from_slice(src_ppn.get_bytes_array());
            }
        }
        memory_set
    }
}
```

除了 trampoline 跳板页面之外的逻辑段都包含在 `areas` 中。我们遍历原地址空间中的所有逻辑段，将复制之后的逻辑段插入新的地址空间，在插入的时候就已经实际分配了物理页帧了。

接着，我们实现 `TaskControlBlock::fork` 来从父进程的进程控制块创建一份子进程的控制块：

```rust
// os/src/task/task.rs

impl TaskControlBlock {
    pub fn fork(self: &Arc<TaskControlBlock>) -> Arc<TaskControlBlock> {
		//...
}
```

在具体实现 `sys_fork` 的时候，我们需要特别注意如何体现父子进程的差异：

```rust
// os/src/syscall/process.rs

pub fn sys_fork() -> isize {
    let current_task = current_task().unwrap();
    let new_task = current_task.fork();
    let new_pid = new_task.pid.0;
    // modify trap context of new_task, because it returns immediately after switching
    let trap_cx = new_task.inner_exclusive_access().get_trap_cx();
    // we do not have to move to next instruction since we have done it before
    // for child process, fork returns 0
    trap_cx.x[10] = 0;  //x[10] is a0 reg
    // add new task to scheduler
    add_task(new_task);
    new_pid as isize
}

// os/src/trap/mod.rs
#[no_mangle]
pub fn trap_handler() -> ! {
    set_kernel_trap_entry();
    let scause = scause::read();
    let stval = stval::read();
    match scause.cause() {
        Trap::Exception(Exception::UserEnvCall) => {
            // jump to next instruction anyway
            let mut cx = current_trap_cx();
            cx.sepc += 4;
            // get system call return value
            let result = syscall(cx.x[17], [cx.x[10], cx.x[11], cx.x[12]]);
            // cx is changed during sys_exec, so we have to call it again
            cx = current_trap_cx();
            cx.x[10] = result as usize;
        }
    ...
}
```

---

> ***exec 内核实现***

`exec` 系统调用使得一个进程能够加载一个新应用的 ELF 可执行文件中的代码和数据替换原有的应用地址空间中的内容，并开始执行。从进程控制块的层面出发，它在解析传入的 ELF 格式数据之后只做了两件事情：

- 首先是从 ELF 文件生成一个全新的地址空间并直接替换进来，这将导致原有的地址空间生命周期结束，里面包含的全部物理页帧都会被回收；
- 然后是修改新的地址空间中的 Trap 上下文，将解析得到的应用入口点、用户栈位置以及一些内核的信息进行初始化，这样才能正常实现 Trap 机制；

```rust
impl TaskControlBlock {
    pub fn exec(&self, elf_data: &[u8]) {
    	//...
    }
}
```

有了 `exec` 函数后， `sys_exec` 就很容易实现了：

```rust
// os/src/mm/page_table.rs
pub fn translated_str(token: usize, ptr: *const u8) -> String {
    let page_table = PageTable::from_token(token);
    let mut string = String::new();
    let mut va = ptr as usize;
    loop {
        let ch: u8 = *(page_table.translate_va(VirtAddr::from(va)).unwrap().get_mut());
        if ch == 0 {
            break;
        } else {
            string.push(ch as char);
            va += 1;
        }
    }
    string
}

// os/src/syscall/process.rs
pub fn sys_exec(path: *const u8) -> isize {
    let token = current_user_token();
    let path = translated_str(token, path);
    if let Some(data) = get_app_data_by_name(path.as_str()) {
        let task = current_task().unwrap();
        task.exec(data);
        0
    } else {
        -1
    }
}
```

注意，应用在 `sys_exec` 系统调用中传递给内核的只有一个要执行的应用名字符串在当前应用地址空间中的起始地址，如果想在内核中具体获得字符串的话就需要手动查页表。

> ***系统调用后重新获取 Trap 上下文***

注意，对于系统调用 `sys_exec` 来说，一旦调用它之后，我们会发现 `trap_handler` 原来上下文中的 `cx` 失效了——因为它是用来访问之前地址空间中 Trap 上下文被保存在的那个物理页帧的，而现在它已经被回收掉了。因此，为了能够处理类似的这种情况，我们在 `syscall` 分发函数返回之后需要重新获取 `cx` ，目前的实现如下：

```rust
// os/src/trap/mod.rs

#[no_mangle]
pub fn trap_handler() -> ! {
    set_kernel_trap_entry();
    let scause = scause::read();
    let stval = stval::read();
    match scause.cause() {
        Trap::Exception(Exception::UserEnvCall) => {
            // jump to next instruction anyway
            let mut cx = current_trap_cx();
            cx.sepc += 4;
            // get system call return value
            let result = syscall(cx.x[17], [cx.x[10], cx.x[11], cx.x[12]]);
            // cx is changed during sys_exec, so we have to call it again
            cx = current_trap_cx();
            cx.x[10] = result as usize;
        }
        ...
    }
    trap_return();
}
```

---

##### user_shell的输入机制 - read

为了实现shell程序 `user_shell` 的输入机制，我们需要实现 `sys_read` 系统调用使得应用能够取得用户的键盘输入。

```rust
// os/src/syscall/fs.rs
use crate::sbi::console_getchar;

const FD_STDIN: usize = 0;

pub fn sys_read(fd: usize, buf: *const u8, len: usize) -> isize {
    match fd {
        FD_STDIN => {
            assert_eq!(len, 1, "Only support len = 1 in sys_read!");
            let mut c: usize;
            loop {
                c = console_getchar();
                if c == 0 {
                    suspend_current_and_run_next();
                    continue;
                } else {
                    break;
                }
            }
            let ch = c as u8;
            let mut buffers = translated_byte_buffer(current_user_token(), buf, len);
            unsafe { buffers[0].as_mut_ptr().write_volatile(ch); }
            1
        }
        _ => {
            panic!("Unsupported fd in sys_read!");
        }
    }
}
```

---

##### 进程的资源回收机制

> ***进程退出 `sys_exit`***

当应用调用 `sys_exit` 系统调用主动退出或者出错由内核终止之后，会在内核中调用 `exit_current_and_run_next` 函数退出当前进程并切换到下一个进程。

```rust
// os/src/mm/memory_set.rs
impl MemorySet {
    pub fn recycle_data_pages(&mut self) {
        self.areas.clear();
    }
}

// os/src/task/mod.rs
pub fn exit_current_and_run_next(exit_code: i32) {
    // take from Processor
    let task = take_current_task().unwrap();
    // **** access current TCB exclusively
    let mut inner = task.inner_exclusive_access();
    // Change status to Zombie
    inner.task_status = TaskStatus::Zombie;
    // Record exit code
    inner.exit_code = exit_code;
    // do not move to its parent but under initproc

    // ++++++ access initproc TCB exclusively
    {
        let mut initproc_inner = INITPROC.inner_exclusive_access();
        for child in inner.children.iter() {
            child.inner_exclusive_access().parent = Some(Arc::downgrade(&INITPROC));
            initproc_inner.children.push(child.clone());
        }
    }
    // ++++++ stop exclusively accessing parent PCB

    inner.children.clear();
    // deallocate user space
    inner.memory_set.recycle_data_pages();
    drop(inner);
    // **** stop exclusively accessing current PCB
    // drop task manually to maintain rc correctly
    drop(task);
    // we do not have to save task context
    let mut _unused = TaskContext::zero_init();
    schedule(&mut _unused as *mut _);
}
```

---

> ***父进程回收子进程资源 `sys_waitpid`***

父进程通过 `sys_waitpid` 系统调用来回收子进程的资源并收集它的一些信息：

```rust
// os/src/syscall/process.rs

/// If there is not a child process whose pid is same as given, return -1.
/// Else if there is a child process but it is still running, return -2.
pub fn sys_waitpid(pid: isize, exit_code_ptr: *mut i32) -> isize {
    let task = current_task().unwrap();
    // find a child process

    // ---- access current TCB exclusively
    let mut inner = task.inner_exclusive_access();
    if inner.children
        .iter()
        .find(|p| {pid == -1 || pid as usize == p.getpid()})
        .is_none() {
        return -1;
        // ---- stop exclusively accessing current PCB
    }
    let pair = inner.children
        .iter()
        .enumerate()
        .find(|(_, p)| {
            // ++++ temporarily access child PCB exclusively
            p.inner_exclusive_access().is_zombie() && (pid == -1 || pid as usize == p.getpid())
            // ++++ stop exclusively accessing child PCB
        });
    if let Some((idx, _)) = pair {
        let child = inner.children.remove(idx);
        // confirm that child will be deallocated after removing from children list
        assert_eq!(Arc::strong_count(&child), 1);
        let found_pid = child.getpid();
        // ++++ temporarily access child TCB exclusively
        let exit_code = child.inner_exclusive_access().exit_code;
        // ++++ stop exclusively accessing child PCB
        *translated_refmut(inner.memory_set.token(), exit_code_ptr) = exit_code;
        found_pid as isize
    } else {
        -2
    }
    // ---- stop exclusively accessing current PCB automatically
}

// user/src/lib.rs
pub fn wait(exit_code: &mut i32) -> isize {
    loop {
        match sys_waitpid(-1, exit_code as *mut _) {
            -2 => { yield_(); }
            // -1 or a real pid
            exit_pid => return exit_pid,
        }
    }
}
```

## 3 //TODO: ch1~ch5总结

从项目 make run 静态编译链接并启动，到最终在 `user_shell` 中运行 matrix 应用程序直至退出为目标，自顶向下地梳理一下 `ch1 ~ ch5` 的流程，整体上划分为如下阶段：

> * 项目构建的静态编译链接过程
> * 项目的启动过程
>   * qemu、RustSBI引导内核启动；
>   * 内核初始化，调度初始进程 `initproc` 的运行；
>   * 初始进程 `fork/exec` 生成子进程 `user_shell`；
> * `user_shell` 等待并循环读取输入并解析（此处运行 hello_world）
>   * `usershell` 调用 fork/exec 生成 hello_world 对应的子进程，并调度运行；
> * 应用程序运行阶段：`hello_world` 程序
>   * 用户层执行 `println!`，执行用户态系统调用接口 `sys_write`，陷入内核；
>   * 内核层进行 trap_handler 分发，最终会调用SBI服务 `console_putchar` 循环打印字符串的每个字符；
> * 应用程序退出：`exit(main)`
>   * 用户库 `lib.rs ` 约束了应用程序的生命周期，从 `_start` 开始执行第一条用户态指令，最终调用 `exit/sys_exit` ；
>   * `sys_exit` 陷入内核后，调用 `exit_current_and_run_next`，此时系统中仅存在 `initproc/user_shell` 两个进程，从而再次进入 `user_shell` 流程中。

---







## *4 //TODO 实验



# 六 文件系统

## 1 概述

随着操作系统功能的增强，在操作系统的管理下，应用程序不用理解持久存储设备的硬件细节，而只需对 **文件** 这种持久存储数据的抽象进行读写就可以了，由操作系统中的文件系统和存储设备驱动程序一起来完成繁琐的持久存储设备的管理与读写。

本章我们将实现一个简单的文件系统 – ***easyfs***，能够对 **持久存储设备** (Persistent Storage) 这种 I/O 资源进行管理。对于应用程序访问持久存储设备的需求，内核需要新增两种文件：常规文件和目录文件，它们均以文件系统所维护的 **磁盘文件** 形式被组织并保存在持久存储设备上。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312141816248.png" alt="文件系统示意图" style="zoom: 50%;" />

---

FileSystemOS 的总体架构如下：

![img](https://rcore-os.cn/rCore-Tutorial-Book-v3/_images/fsos-fsdisk.png)

通过上图，可以看出 FilesystemOS 增加了对文件系统的支持：

* 对应用程序提供了文件访问相关的系统调用服务，包括 sys_open、sys_close、sys_read、sys_write；
* 在进程管理上，进一步扩展资源管理的范围，把打开的文件相关信息放到 `fd table` 数据结构中，纳入进程的管辖中；
* 在设备管理层面，增加了块设备驱动 – BlockDrv ，通过访问块设备数据来读写文件系统与文件的各种数据；
* 文件系统 – EasyFS，完成文件与存储块之间的数据/地址映射关系，通过块设备驱动 BlockDrv 进行基于存储块的读写；

---

本章参考经典的 UNIX 基于索引结构的文件系统，设计了一个简化的有一级目录并支持 `open，read， write， close` 一系列操作的文件系统。这里简要介绍一下在内核中添加文件系统的大致开发过程：

* **应用层：支持用户能够编写与文件访问相关的应用程序；**
* **文件系统层：在用户态实现 easyfs 文件系统，测试后再嵌入内核，自下而上可分为五层；**
  * 磁盘块设备接口层：读写磁盘块设备的trait接口
  * 块缓存层：位于内存的磁盘块数据缓存
  * 磁盘数据结构层：表示磁盘文件系统的数据结构
  * 磁盘块管理器层：实现对磁盘文件系统的管理
  * 索引节点层：实现文件创建/文件打开/文件读写等操作

* **将 easyfs 文件系统接入内核**
  * 在Qemu模拟的 `virtio` 块设备上实现块设备驱动程序；
  * 将文件访问相关的系统调用与 easyfs 文件系统连接起来；

## 2 重点内容梳理

本章的代码涉及了一些较复杂的 Rust 编程技巧，尤其是对模板参数的使用值得学习。

### 2.1 用户层文件系统接口

#### 文件打开open

在读写一个常规文件之前，应用首先需要通过内核提供的 `sys_open` 系统调用让该文件在进程的文件描述符表中占一项，并得到操作系统的返回值–文件描述符，即文件关联的表项在文件描述表中的索引值：

```rust
/// 功能：打开一个常规文件，并返回可以访问它的文件描述符。
/// 参数：path 描述要打开的文件的文件名（简单起见，文件系统不需要支持目录，所有的文件都放在根目录 / 下），
/// flags 描述打开文件的标志，具体含义下面给出。
/// 返回值：如果出现了错误则返回 -1，否则返回打开常规文件的文件描述符。可能的错误原因是：文件不存在。
/// syscall ID：56
fn sys_open(path: &str, flags: u32) -> isize

// user/src/lib.rs
bitflags! {
    pub struct OpenFlags: u32 {
        const RDONLY = 0;
        const WRONLY = 1 << 0;
        const RDWR = 1 << 1;
        const CREATE = 1 << 9;
        const TRUNC = 1 << 10;
    }
}

pub fn open(path: &str, flags: OpenFlags) -> isize {
    sys_open(path, flags.bits)
}
```

---

#### 文件关闭close

在打开文件，对文件完成了读写操作后，还需要关闭文件，这样才让进程释放被这个文件占用的内核资源。 `close` 的调用参数是文件描述符，当文件被关闭后，该文件在内核中的资源会被释放，文件描述符会被回收。。

```rust
/// 功能：当前进程关闭一个文件。
/// 参数：fd 表示要关闭的文件的文件描述符。
/// 返回值：如果成功关闭则返回 0 ，否则返回 -1 。可能的出错原因：传入的文件描述符并不对应一个打开的文件。

// usr/src/lib.rs
pub fn close(fd: usize) -> isize { sys_close(fd) }

// user/src/syscall.rs
const SYSCALL_CLOSE: usize = 57;

pub fn sys_close(fd: usize) -> isize {
    syscall(SYSCALL_CLOSE, [fd, 0, 0])
}
```

---

#### 文件顺序读写read/write

在打开一个文件之后，我们就可以用之前的 `sys_read/sys_write` 两个系统调用来对它进行读写了。需要注意的是，常规文件的读写模式和之前介绍过的几种文件有所不同。标准输入输出和匿名管道都属于一种流式读写，而常规文件则是顺序读写和随机读写的结合。由于常规文件可以看成一段字节序列，我们应该能够随意读写它的任一段区间的数据，即随机读写。然而用户仅仅通过 `sys_read/sys_write` 两个系统调用不能做到这一点。

事实上，进程为每个它打开的常规文件维护了一个偏移量，在刚打开时初始值一般为 0 字节。当 `sys_read/sys_write` 的时候，将会从文件字节序列偏移量的位置开始 **顺序** 把数据读到应用缓冲区/从应用缓冲区写入数据。操作完成之后，偏移量向后移动读取/写入的实际字节数。这意味着，下次 `sys_read/sys_write` 将会从刚刚读取/写入之后的位置继续。如果仅使用 `sys_read/sys_write` 的话，则只能从头到尾顺序对文件进行读写。当我们需要从头开始重新写入或读取的话，只能通过 `sys_close` 关闭并重新打开文件来将偏移量重置为 0。为了解决这种问题，有另一个系统调用 `sys_lseek` 可以调整进程打开的一个常规文件的偏移量，这样便能对文件进行随机读写。但在本教程中并未实现这个系统调用。

### 2.2 简易文件系统easyfs

`easy-fs` crate 自下而上大致可以分成五个不同的层次：

1. **磁盘块设备接口层：**定义了以块大小为单位对磁盘块设备进行读写的trait接口；
2. **块缓存层：**在内存中缓存磁盘块的数据，避免频繁读写磁盘；
3. **磁盘数据结构层：**磁盘上的超级块、位图、索引节点、数据块、目录项等核心数据结构和相关处理；
4. **磁盘块管理器层：**合并了上述核心数据结构和磁盘布局所形成的磁盘文件系统数据结构，以及基于这些结构的创建/打开文件系统的相关处理和磁盘块的分配和回收处理；
5. **索引节点层：**管理索引节点（即文件控制块）数据结构，并实现 “文件创建/文件打开/文件读写” 等成员函数来向上支持文件操作相关的系统调用；

---

#### L5 - 块设备接口层

在 `easy-fs` 库的最底层声明了一个块设备的抽象接口 `BlockDevice` ：

```rust
// easy-fs/src/block_dev.rs

pub trait BlockDevice : Send + Sync + Any {
    fn read_block(&self, block_id: usize, buf: &mut [u8]);
    fn write_block(&self, block_id: usize, buf: &[u8]);
}
```

- `read_block` 将编号为 `block_id` 的块从磁盘读入内存中的缓冲区 `buf` ；
- `write_block` 将内存中的缓冲区 `buf` 中的数据写入磁盘编号为 `block_id` 的块；

在 `easy-fs` 中并没有一个实现了 `BlockDevice` Trait 的具体类型。因为块设备仅支持以块为单位进行随机读写，所以需要由具体的块设备驱动来实现这两个方法，实际上这是需要由文件系统的使用者（比如操作系统内核或直接测试 `easy-fs` 文件系统的 `easy-fs-fuse` 应用程序）提供并接入到 `easy-fs` 库的。

---

 `easy-fs` 库的块缓存层会调用这两个方法，进行块缓存的管理。这也体现了 `easy-fs` 的泛用性：它可以访问实现了 `BlockDevice` Trait 的块设备驱动程序。

#### L4 - 块缓存层

由于操作系统频繁读写速度缓慢的磁盘块会极大降低系统性能，因此常见的手段是先通过 `read_block` 将一个块上的数据从磁盘读到内存中的一个缓冲区中，这个缓冲区中的内容是可以直接读写的，那么后续对这个数据块的大部分访问就可以在内存中完成了。如果缓冲区中的内容被修改了，那么后续还需要通过 `write_block` 将缓冲区中的内容写回到磁盘块中。

我们还需要将缓冲区统一管理起来。当我们要读写一个块的时候，首先就是去全局管理器中查看这个块是否已被缓存到内存缓冲区中。如果是这样，则在一段连续时间内对于一个块进行的所有操作均是在同一个固定的缓冲区中进行的，这解决了同步性问题。此外，通过 `read/write_block` 进行块实际读写的时机完全交给块缓存层的全局管理器处理，上层子系统无需操心。全局管理器会尽可能将更多的块操作合并起来，并在必要的时机发起真正的块实际读写。

---

##### 块缓存

块缓存 `BlockCache` 的定义如下：

```rust
// easy-fs/src/lib.rs
pub const BLOCK_SZ: usize = 512;

// easy-fs/src/block_cache.rs
pub struct BlockCache {
    cache: [u8; BLOCK_SZ],
    block_id: usize,
    block_device: Arc<dyn BlockDevice>,
    modified: bool, // 记录这个块从磁盘载入内存缓存之后，它有没有被修改过
}
```

当我们创建一个 `BlockCache` 的时候，这将触发一次 `read_block` 将一个块上的数据从磁盘读到缓冲区 `cache` ：

```rust
// easy-fs/src/block_cache.rs
impl BlockCache {
    /// Load a new BlockCache from disk.
    pub fn new(
        block_id: usize,
        block_device: Arc<dyn BlockDevice>
    ) -> Self {
        let mut cache = [0u8; BLOCK_SZ];
        block_device.read_block(block_id, &mut cache);
        Self {
            cache,
            block_id,
            block_device,
            modified: false,
        }
    }
}
```

同时，`BlockCache` 的设计也体现了 RAII 思想， 它管理着一个缓冲区的生命周期。当 `BlockCache` 的生命周期结束之后缓冲区也会被从内存中回收，这个时候 `modified` 标记将会决定数据是否需要写回磁盘：

```rust
// easy-fs/src/block_cache.rs
impl BlockCache {
    pub fn sync(&mut self) {
        if self.modified {
            self.modified = false;
            self.block_device.write_block(self.block_id, &self.cache);
        }
    }
}

impl Drop for BlockCache {
    fn drop(&mut self) {
        self.sync()
    }
}
```

> 事实上， `sync` 并不是只有在 `drop` 的时候才会被调用。在 Linux 中，通常有一个后台进程负责定期将内存中缓冲区的内容写回磁盘。另外有一个 `sys_fsync` 系统调用可以让应用主动通知内核将一个文件的修改同步回磁盘。由于我们的实现比较简单， `sync` 仅会在 `BlockCache` 被 `drop` 时才会被调用。

---

##### 块缓存管理器

块缓存全局管理器的功能是：

* 当我们要对一个磁盘块进行读写时，首先看它是否已经被载入到内存缓存中了，如果已经被载入的话则直接返回，否则需要先读取磁盘块的数据到内存缓存中；
* 此时，如果内存中驻留的磁盘块缓冲区的数量已满，则需要遵循某种缓存替换算法将某个块的缓存从内存中移除，再将刚刚读到的块数据加入到内存缓存中。我们这里使用一种类 FIFO 的简单缓存替换算法，因此在管理器中只需维护一个队列。

```rust
// easy-fs/src/block_cache.rs
use alloc::collections::VecDeque;

pub struct BlockCacheManager {
    queue: VecDeque<(usize, Arc<Mutex<BlockCache>>)>, //队列 queue 中管理的是块编号和块缓存的二元组
}

impl BlockCacheManager {
    pub fn new() -> Self {
        Self { queue: VecDeque::new() }
    }
}
```

`get_block_cache` 方法尝试从块缓存管理器中获取一个编号为 `block_id` 的块的块缓存，如果找不到，会从磁盘读取到内存中，还有可能会发生缓存替换：

```rust
// easy-fs/src/block_cache.rs

impl BlockCacheManager {
    pub fn get_block_cache(
        &mut self,
        block_id: usize,
        block_device: Arc<dyn BlockDevice>,
    ) -> Arc<Mutex<BlockCache>> {
        if let Some(pair) = self.queue
            .iter()
            .find(|pair| pair.0 == block_id) {
                Arc::clone(&pair.1)
        } else {
            // substitute
            if self.queue.len() == BLOCK_CACHE_SIZE {
                // from front to tail
                if let Some((idx, _)) = self.queue
                    .iter()
                    .enumerate()
                    .find(|(_, pair)| Arc::strong_count(&pair.1) == 1) {
                    self.queue.drain(idx..=idx);
                } else {
                    panic!("Run out of BlockCache!");
                }
            }
            // load block into mem and push back
            let block_cache = Arc::new(Mutex::new(
                BlockCache::new(block_id, Arc::clone(&block_device))
            ));
            self.queue.push_back((block_id, Arc::clone(&block_cache)));
            block_cache
        }
    }
}
```

> ***简单的缓存替换算法：***
>
> 这里使用一种类 FIFO 算法：每加入一个块缓存时要从队尾加入；要替换时则从队头弹出。但此时队头对应的块缓存可能仍在使用：判断的标志是其强引用计数 ≥ 2 ，即除了块缓存管理器保留的一份副本之外，在外面还有若干份副本正在使用。因此，我们的做法是从队头遍历到队尾找到第一个强引用计数恰好为 1 的块缓存并将其替换出去。
>
> 那么是否有可能出现队列已满且其中所有的块缓存都正在使用的情形呢？事实上，只要我们的上限 `BLOCK_CACHE_SIZE` 设置的足够大，超过所有应用同时访问的块总数上限，那么这种情况永远不会发生。但是，如果我们的上限设置不足，内核将 panic （基于简单内核设计的思路）。

最后，创建 `BlockCacheManager` 的全局实例：

```rust
// easy-fs/src/block_cache.rs
lazy_static! {
    pub static ref BLOCK_CACHE_MANAGER: Mutex<BlockCacheManager> = Mutex::new(
        BlockCacheManager::new()
    );
}

pub fn get_block_cache(
    block_id: usize,
    block_device: Arc<dyn BlockDevice>
) -> Arc<Mutex<BlockCache>> {
    BLOCK_CACHE_MANAGER.lock().get_block_cache(block_id, block_device)
}
```

这样对于其他模块而言，就可以直接通过 `get_block_cache` 方法来请求块缓存了。这里需要指出的是，它返回的是一个 `Arc<Mutex<BlockCache>>` ，调用者需要通过 `.lock()` 获取里层互斥锁 `Mutex` 才能对最里面的 `BlockCache` 进行操作，比如通过 `read/modify` 访问缓冲区里面的磁盘数据结构。

#### L3 - 磁盘布局层

对于一个文件系统而言，最重要的功能是如何将一个逻辑上的文件目录树结构映射到磁盘上，决定磁盘上的每个块应该存储文件相关的哪些数据。

##### easyfs磁盘布局设计

easy-fs 的磁盘布局如下图所示：

![../_images/文件系统布局.png](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312151034915.png)

**索引节点** (Inode, Index Node) 是文件系统中的一种重要数据结构。逻辑目录树结构中的每个文件和目录都对应一个 inode ，我们前面提到的文件系统实现中，文件/目录的底层编号实际上就是指 inode 编号。

* inode 中包含了我们通过 `stat` 工具能够看到的文件/目录的元数据（大小/访问权限/类型等信息）；
* inode 还包含了实际保存对应文件/目录数据的数据块（位于最后的数据块区域中）的索引信息，从而能够找到文件/目录的数据被保存在磁盘的哪些块中。从索引方式上看，同时支持直接索引和间接索引。

> 每个区域中均存储着不同的磁盘数据结构， `easy-fs` 文件系统能够对磁盘中的数据进行解释并将其结构化。

---

##### 超级块

超级块 `SuperBlock` 的结构与方法如下：

```rust
// easy-fs/src/layout.rs

#[repr(C)]
pub struct SuperBlock {
    magic: u32,
    pub total_blocks: u32,
    pub inode_bitmap_blocks: u32,
    pub inode_area_blocks: u32,
    pub data_bitmap_blocks: u32,
    pub data_area_blocks: u32,
}

// easy-fs/src/layout.rs
impl SuperBlock {
    pub fn initialize(
        &mut self,
        total_blocks: u32,
        inode_bitmap_blocks: u32,
        inode_area_blocks: u32,
        data_bitmap_blocks: u32,
        data_area_blocks: u32,
    ) {
        *self = Self {
            magic: EFS_MAGIC,
            total_blocks,
            inode_bitmap_blocks,
            inode_area_blocks,
            data_bitmap_blocks,
            data_area_blocks,
        }
    }
    pub fn is_valid(&self) -> bool {
        self.magic == EFS_MAGIC
    }
}
```

- `initialize` 可以在创建一个 easy-fs 的时候对超级块进行初始化，注意各个区域的块数是以参数的形式传入进来的，它们的划分是更上层的磁盘块管理器需要完成的工作；
- `SuperBlock` 是一个磁盘上数据结构，它就存放在磁盘上编号为 0 的块的起始处；

---

##### 索引节点/数据块位图

在 easy-fs 布局中存在两类不同的位图，分别对索引节点和数据块进行管理。

* 每个位图都由若干个块组成，每个块大小为 512 bytes，即 4096 bits；
* 每个 bit 都代表一个 “索引节点/数据块” 的分配状态， 0 意味着未分配，而 1 则意味着已经分配出去；
* 位图所要做的事情是通过基于 bit 为单位的分配（寻找一个为 0 的bit位并设置为 1）和回收（将bit位清零）来进行索引节点/数据块的分配和回收。

```rust
// easy-fs/src/bitmap.rs
pub struct Bitmap {
    start_block_id: usize,
    blocks: usize,
}

//磁盘块上位图区域的数据则是要以磁盘数据结构 BitmapBlock 的格式进行操作
type BitmapBlock = [u64; 64];

impl Bitmap {
    pub fn new(start_block_id: usize, blocks: usize) -> Self {
        Self {
            start_block_id,
            blocks,
        }
    }
}
```

`BitmapBlock` 是一个磁盘数据结构，它将位图区域中的一个磁盘块解释为长度为 64 的一个 `u64` 数组， 每个 `u64` 打包了一组 64 bits，于是整个数组包含 `64×64=4096 bits`，且可以以组为单位进行操作。

---

`Bitmap` 如何分配与回收一个bit：

```rust
// easy-fs/src/bitmap.rs
const BLOCK_BITS: usize = BLOCK_SZ * 8;

impl Bitmap {
    pub fn alloc(&self, block_device: &Arc<dyn BlockDevice>) -> Option<usize> {
        for block_id in 0..self.blocks {
            let pos = get_block_cache(
                block_id + self.start_block_id as usize,
                Arc::clone(block_device),
            )
            .lock()
            .modify(0, |bitmap_block: &mut BitmapBlock| {
                if let Some((bits64_pos, inner_pos)) = bitmap_block
                    .iter()
                    .enumerate()
                    .find(|(_, bits64)| **bits64 != u64::MAX)
                    .map(|(bits64_pos, bits64)| {
                        (bits64_pos, bits64.trailing_ones() as usize)
                    }) {
                    // modify cache
                    bitmap_block[bits64_pos] |= 1u64 << inner_pos;
                    Some(block_id * BLOCK_BITS + bits64_pos * 64 + inner_pos as usize)
                } else {
                    None
                }
            });
            if pos.is_some() {
                return pos;
            }
        }
        None
    }
}

/// Return (block_pos, bits64_pos, inner_pos)
fn decomposition(mut bit: usize) -> (usize, usize, usize) {
    let block_pos = bit / BLOCK_BITS;
    bit = bit % BLOCK_BITS;
    (block_pos, bit / 64, bit % 64)
}

impl Bitmap {
    pub fn dealloc(&self, block_device: &Arc<dyn BlockDevice>, bit: usize) {
        let (block_pos, bits64_pos, inner_pos) = decomposition(bit);
        get_block_cache(
            block_pos + self.start_block_id,
            Arc::clone(block_device)
        ).lock().modify(0, |bitmap_block: &mut BitmapBlock| {
            assert!(bitmap_block[bits64_pos] & (1u64 << inner_pos) > 0);
            bitmap_block[bits64_pos] -= 1u64 << inner_pos;
        });
    }
}
```

---

##### 索引节点

在磁盘上的索引节点区域，每个块上都保存着若干个索引节点 `DiskInode` ：

```rust
// easy-fs/src/layout.rs
const INODE_DIRECT_COUNT: usize = 28;

#[repr(C)]
pub struct DiskInode {
    pub size: u32,
    pub direct: [u32; INODE_DIRECT_COUNT],
    pub indirect1: u32,
    pub indirect2: u32,
    type_: DiskInodeType,
}

#[derive(PartialEq)]
pub enum DiskInodeType {
    File,
    Directory,
}
```

每个文件/目录在磁盘上均以一个 `DiskInode` 的形式存储。其中包含文件/目录的元数据： 

* `size` 表示文件/目录内容的字节数；
*  `type_` 表示索引节点的类型 `DiskInodeType` ，目前仅支持文件 `File` 和目录 `Directory` 两种类型；
* 其余的 `direct/indirect1/indirect2` 都是存储文件内容/目录内容的数据块的索引，这也是索引节点名字的由来。

索引方式分成直接索引和间接索引两种，细节见：https://rcore-os.cn/rCore-Tutorial-Book-v3/chapter6/2fs-implementation.html#id11

---

###### get_block_id

`get_block_id` 方法体现了 `DiskInode` 最重要的数据块索引功能，它可以从索引中查到它自身用于保存文件内容的第 `block_id` 个数据块的块编号，这样后续才能对这个数据块进行访问：

```rust
// easy-fs/src/layout.rs
const INODE_INDIRECT1_COUNT: usize = BLOCK_SZ / 4;
const INDIRECT1_BOUND: usize = DIRECT_BOUND + INODE_INDIRECT1_COUNT;
type IndirectBlock = [u32; BLOCK_SZ / 4];

impl DiskInode {
    pub fn get_block_id(&self, inner_id: u32, block_device: &Arc<dyn BlockDevice>) -> u32 {
    	//...
    }
 }
```

> 解释一下这个 `inner_id`：
>
> 首先看直接索引，它对应一个一个数组 `[u32; INODE_DIRECT_COUNT]`，其中每个元素为存储实际数据内容的全局 `block_id`。同时，又扩充了 `indirect1/indirect2` 两级间接索引，一共三层索引，每层索引的数据块数量分别为：
>
> * `direct`：28
> * `indirect1`：512 / 4 = 128
>
> * `indirect2`：512 / 4 * 512 / 4 = 128 * 128
>
> 每个索引节点能够索引数据块的最大数量为： 28 + 128 + 128 *128，将其记作 `INODE_BLOCKS_MAX_NUM`。
>
> 综上所述，这个 `inner_id` 是一个单个索引节点内部所容纳数据块的 “内部索引”，但通过 `inner_id` 并不能直接获取到 `block_id`，因为这并不是一个大范围的数组结构，而是按需动态创建起来的分层数组结构，因此想找到 `inner_id` 所对应的 `block_id` 会复杂一些。 

---

###### increase/clear_size

注意，在对文件/目录初始化之后，它的 `size` 均为 0 ，此时并不会索引到任何数据块。它需要通过 `increase_size` 方法逐步扩充容量。在扩充的时候，自然需要一些新的数据块来作为索引块或是保存内容的数据块。我们需要先编写一些辅助方法来确定在容量扩充的时候额外需要多少块：

```rust
// easy-fs/src/layout.rs

impl DiskInode {
    /// Return block number correspond to size.
    pub fn data_blocks(&self) -> u32;
    fn _data_blocks(size: u32) -> u32;
    /// Return number of blocks needed include indirect1/2.
    pub fn total_blocks(size: u32) -> u32;
    pub fn blocks_num_needed(&self, new_size: u32) -> u32;
}
```

下面给出 `increase_size` 和 `clear_size` 方法的接口：

```rust
// easy-fs/src/layout.rs
impl DiskInode {
    pub fn increase_size(
        &mut self,
        new_size: u32,
        new_blocks: Vec<u32>,
        block_device: &Arc<dyn BlockDevice>,
    );
    
    /// Clear size to zero and return blocks that should be deallocated.
    ///
    /// We will clear the block contents to zero later.
    pub fn clear_size(&mut self, block_device: &Arc<dyn BlockDevice>) -> Vec<u32>;
}
```

* `increase_size`：其中 new_size 表示容量扩充之后的文件大小； new_blocks 是一个保存了本次容量扩充所需块编号的向量，这些块都是由上层的磁盘块管理器负责分配的。
* `clear_size`：将回收的所有块的编号保存在一个向量中返回给磁盘块管理器。

---

###### read/write_at

上层还需要通过 `DiskInode` 来读写它索引的那些数据块中的数据。这些数据可以被视为一个字节序列，而每次都是选取其中的一段连续区间进行操作，以 `read_at` 为例：

```rust
// easy-fs/src/layout.rs
type DataBlock = [u8; BLOCK_SZ];

impl DiskInode {
    pub fn read_at(
        &self,
        offset: usize,
        buf: &mut [u8],
        block_device: &Arc<dyn BlockDevice>,
    ) -> usize {
        let mut start = offset;
        let mut start_block = start / BLOCK_SZ;
        //...
        loop {
            get_block_cache(self.get_block_id(start_block, block_device), x);
        }
    }
    
    //...
}
```

**它的含义是：**将文件内容从 `offset` 字节开始的部分读到内存中的缓冲区 `buf` 中，并返回实际读到的字节数。如果文件剩下的内容还足够多，那么缓冲区会被填满；否则文件剩下的全部内容都会被读到缓冲区中。具体实现上有很多细节，但大致的思路是遍历位于字节区间 `start,end` 中间的那些块，将它们视为一个 `DataBlock` （也就是一个字节数组），并将其中的部分内容复制到缓冲区 `buf` 中适当的区域。 `start_block` 维护着目前是文件内部第多少个数据块，需要首先调用 `get_block_id` 从索引中查到这个数据块在块设备中的块编号，随后才能传入 `get_block_cache` 中将正确的数据块缓存到内存中进行访问。

>`write_at` 的实现思路基本上和 `read_at` 完全相同。但不同的是 `write_at` 不会出现失败的情况；只要 Inode 管理的数据块的大小足够，传入的整个缓冲区的数据都必定会被写入到文件中。当从 `offset` 开始的区间超出了文件范围的时候，就需要调用者在调用 `write_at` 之前提前调用 `increase_size` ，将文件大小扩充到区间的右端，保证写入的完整性。

---

##### 数据块与目录项

作为一个文件而言，它的内容在文件系统看来没有任何既定的格式，都只是一个字节序列。因此每个保存内容的数据块都只是一个字节数组：

```rust
// easy-fs/src/layout.rs

type DataBlock = [u8; BLOCK_SZ];
```

然而，目录的内容却需要遵从一种特殊的格式。目录项 `DirEntry` 的定义如下：

```rust
// easy-fs/src/layout.rs
const NAME_LENGTH_LIMIT: usize = 27;

#[repr(C)]
pub struct DirEntry {
    name: [u8; NAME_LENGTH_LIMIT + 1],
    inode_number: u32,
}

pub const DIRENT_SZ: usize = 32;
```

---

#### L2 - 磁盘块管理器层

实现 easy-fs 的整体磁盘布局，将各段区域及上面的磁盘数据结构结构整合起来就是简易文件系统 `EasyFileSystem` 的职责。它知道每个布局区域所在的位置，磁盘块的分配和回收也需要经过它才能完成，因此某种意义上讲它还可以看成一个磁盘块管理器。

>  注意从这一层开始，所有的数据结构就都放在内存上了。

```rust
// easy-fs/src/efs.rs

pub struct EasyFileSystem {
    pub block_device: Arc<dyn BlockDevice>,
    pub inode_bitmap: Bitmap,
    pub data_bitmap: Bitmap,
    inode_area_start_block: u32,
    data_area_start_block: u32,
}

impl EasyFileSystem {
    pub fn create(
        block_device: Arc<dyn BlockDevice>,
        total_blocks: u32,
        inode_bitmap_blocks: u32,
    ) -> Arc<Mutex<Self>>;
    
    pub fn open(block_device: Arc<dyn BlockDevice>) -> Arc<Mutex<Self>>;
    
    pub fn get_disk_inode_pos(&self, inode_id: u32) -> (u32, usize) {
        let inode_size = core::mem::size_of::<DiskInode>();
        let inodes_per_block = (BLOCK_SZ / inode_size) as u32;
        let block_id = self.inode_area_start_block + inode_id / inodes_per_block;
        (block_id, (inode_id % inodes_per_block) as usize * inode_size)
    }
    pub fn get_data_block_id(&self, data_block_id: u32) -> u32 {
        self.data_area_start_block + data_block_id
    }
    
    pub fn alloc_inode(&mut self) -> u32 {
        self.inode_bitmap.alloc(&self.block_device).unwrap() as u32
    }
    /// Return a block ID not ID in the data area.
    pub fn alloc_data(&mut self) -> u32 {
        self.data_bitmap.alloc(&self.block_device).unwrap() as u32 + self.data_area_start_block
    }
    pub fn dealloc_data(&mut self, block_id: u32);
}
```

* 通过 `create` 方法可以在块设备上创建并初始化一个 easy-fs 文件系统；
  * 根据 inode 位图的大小计算 inode 区域至少需要多少个块才能够使得 inode 位图中的每个bit都能够有一个实际的 inode 可以对应，这样就确定了 inode 位图区域和 inode 区域的大小。剩下的块都分配给数据块位图区域和数据块区域。
* 通过 `open` 方法可以从一个已写入了 easy-fs 镜像的块设备上打开我们的 easy-fs；
  * 它只需将块设备编号为 0 的块作为超级块读取进来，就可以从中知道 easy-fs 的磁盘布局，由此可以构造 `efs` 实例。
* `EasyFileSystem` 知道整个磁盘布局，即可以从 `inode_bitmap` 或 `datablock_bitmap` 上分配的 bit 编号，来算出各个存储inode和数据块的磁盘块在磁盘上的实际位置。
* inode 和数据块的分配/回收也由 `EasyFileSystem` 负责：
  * `alloc_data` 和 `dealloc_data` 传入/返回的参数都表示数据块在块设备上的编号，而不是在数据块位图中分配的bit编号；
  * `dealloc_inode` 未实现，因为现在还不支持文件删除；

---

重点看一下 `EasyFileSystem::create` 方法的实现，该函数在块设备上创建并初始化一个 easyfs 文件系统：

<img src="https://rcore-os.cn/rCore-Tutorial-Book-v3/_images/文件系统布局.png" alt="../_images/文件系统布局.png" style="zoom:67%;" />

```rust
// easy-fs/src/efs.rs

impl EasyFileSystem {
    pub fn create(
        block_device: Arc<dyn BlockDevice>,
        total_blocks: u32,
        inode_bitmap_blocks: u32,
    ) -> Arc<Mutex<Self>> {
        // calculate block size of areas & create bitmaps
        let inode_bitmap = Bitmap::new(1, inode_bitmap_blocks as usize);
        let inode_num = inode_bitmap.maximum();
        let inode_area_blocks =
            ((inode_num * core::mem::size_of::<DiskInode>() + BLOCK_SZ - 1) / BLOCK_SZ) as u32;
        let inode_total_blocks = inode_bitmap_blocks + inode_area_blocks;
        let data_total_blocks = total_blocks - 1 - inode_total_blocks;
        let data_bitmap_blocks = (data_total_blocks + 4096) / 4097;
        let data_area_blocks = data_total_blocks - data_bitmap_blocks;
        let data_bitmap = Bitmap::new(
            (1 + inode_bitmap_blocks + inode_area_blocks) as usize,
            data_bitmap_blocks as usize,
        );
        let mut efs = Self {
            block_device: Arc::clone(&block_device),
            inode_bitmap,
            data_bitmap,
            inode_area_start_block: 1 + inode_bitmap_blocks, // 1 is SuperBlock
            data_area_start_block: 1 + inode_total_blocks + data_bitmap_blocks,
        };
        // clear all blocks
        for i in 0..total_blocks {
            get_block_cache(
                i as usize,
                Arc::clone(&block_device)
            )
            .lock()
            .modify(0, |data_block: &mut DataBlock| {
                for byte in data_block.iter_mut() { *byte = 0; }
            });
        }
        // initialize SuperBlock
        get_block_cache(0, Arc::clone(&block_device))
        .lock()
        .modify(0, |super_block: &mut SuperBlock| {
            super_block.initialize(
                total_blocks,
                inode_bitmap_blocks,
                inode_area_blocks,
                data_bitmap_blocks,
                data_area_blocks,
            );
        });
        // write back immediately
        // create a inode for root node "/"
        assert_eq!(efs.alloc_inode(), 0);
        let (root_inode_block_id, root_inode_offset) = efs.get_disk_inode_pos(0);
        get_block_cache(
            root_inode_block_id as usize,
            Arc::clone(&block_device)
        )
        .lock()
        .modify(root_inode_offset, |disk_inode: &mut DiskInode| {
            disk_inode.initialize(DiskInodeType::Directory);
        });
        Arc::new(Mutex::new(efs))
    }
}
```

创建根目录 `/` 。首先需要调用 `alloc_inode` 在 inode 位图中分配一个 inode ，由于这是第一次分配，它的编号固定是 0 。接下来需要将分配到的 inode 初始化为 easy-fs 中的唯一的目录，故需要调用 `get_disk_inode_pos` 来根据 inode 编号获取该 inode 所在的块的编号以及块内偏移，之后就可以将它们传给 `get_block_cache` 和 `modify` 了。

---

#### L1 - 索引节点层

索引节点层直接服务于文件相关系统调用，代码在 `vfs.rs` 中。`EasyFileSystem` 实现了磁盘布局并能够将磁盘块有效的管理起来。但是对于文件系统的使用者而言，他们往往不关心磁盘布局是如何实现的，而是更希望能够直接看到目录树结构中逻辑上的文件和目录。为此需要设计索引节点 `Inode` 暴露给文件系统的使用者，让他们能够直接对文件和目录进行操作。

> `Inode` 和 `DiskInode` 的区别从它们的名字中就可以看出： `DiskInode` 放在磁盘块中比较固定的位置，而 `Inode` 是放在内存中的记录文件索引节点信息的数据结构。

```rust
// easy-fs/src/vfs.rs

pub struct Inode {
    block_id: usize,
    block_offset: usize,
    fs: Arc<Mutex<EasyFileSystem>>,
    block_device: Arc<dyn BlockDevice>,
}
```

* `block_id` 和 `block_offset` 记录该 `Inode` 对应的 `DiskInode` 保存在磁盘上的具体位置方便我们后续对它进行访问；
*  `fs` 是指向 `EasyFileSystem` 的一个指针，因为对 `Inode` 的种种操作实际上都是要通过底层的文件系统来完成；

下面是文件系统的使用者对于文件系统的一些常用操作：

---

##### 直接访问DiskInode的方法

设计两个方法来简化对于 `Inode` 对应的磁盘上的 `DiskInode` 的访问流程，而不是每次都需要 `get_block_cache.lock.read/modify` ：

```rust
// easy-fs/src/vfs.rs

impl Inode {
    fn read_disk_inode<V>(&self, f: impl FnOnce(&DiskInode) -> V) -> V {
        get_block_cache(
            self.block_id,
            Arc::clone(&self.block_device)
        ).lock().read(self.block_offset, f)
    }

    fn modify_disk_inode<V>(&self, f: impl FnOnce(&mut DiskInode) -> V) -> V {
        get_block_cache(
            self.block_id,
            Arc::clone(&self.block_device)
        ).lock().modify(self.block_offset, f)
    }
}
```

##### 获取根目录的inode

 `EasyFileSystem` 提供了一个名为 `root_inode` 的方法来获取根目录的 `Inode` :

```rust
// easy-fs/src/efs.rs

impl EasyFileSystem {
    pub fn root_inode(efs: &Arc<Mutex<Self>>) -> Inode {
        let block_device = Arc::clone(&efs.lock().block_device);
        // acquire efs lock temporarily
        let (block_id, block_offset) = efs.lock().get_disk_inode_pos(0);
        // release efs lock
        Inode::new(
            block_id,
            block_offset,
            Arc::clone(efs),
            block_device,
        )
    }
}
```

##### 文件索引

文件索引的查找比较简单，仅需在根目录的目录项中**根据文件名找到文件的 inode 编号**即可。由于没有子目录的存在，这个过程只会进行一次。

```rust
// easy-fs/src/vfs.rs

impl Inode {
    pub fn find(&self, name: &str) -> Option<Arc<Inode>>;
    
    fn find_inode_id(
        &self,
        name: &str,
        disk_inode: &DiskInode,
    ) -> Option<u32>;
}
```

`find` 方法只会被根目录 `Inode` 调用，文件系统中其他文件的 `Inode` 不会调用这个方法：

它首先调用 `find_inode_id` 方法，尝试从根目录的 `DiskInode` 上找到要索引的文件名对应的 inode 编号。这就需要将根目录内容中的所有目录项都读到内存进行逐个比对。如果能够找到，则 `find` 方法会根据查到的 inode 编号，对应生成一个 `Inode` 用于后续对文件的访问。

##### 文件列举

`ls` 方法可以收集根目录下的所有文件的文件名并以向量的形式返回，这个方法只有根目录的 `Inode` 才会调用：

```rust
// easy-fs/src/vfs.rs

impl Inode {
    pub fn ls(&self) -> Vec<String> {
        let _fs = self.fs.lock();
        self.read_disk_inode(|disk_inode| {
            let file_count = (disk_inode.size as usize) / DIRENT_SZ;
            let mut v: Vec<String> = Vec::new();
            for i in 0..file_count {
                let mut dirent = DirEntry::empty();
                assert_eq!(
                    disk_inode.read_at(
                        i * DIRENT_SZ,
                        dirent.as_bytes_mut(),
                        &self.block_device,
                    ),
                    DIRENT_SZ,
                );
                v.push(String::from(dirent.name()));
            }
            v
        })
    }
}
```

##### 文件创建

`create` 方法可以在根目录下创建一个文件，该方法只有根目录的 `Inode` 会调用：

```rust
// easy-fs/src/vfs.rs

impl Inode {
    pub fn create(&self, name: &str) -> Option<Arc<Inode>> {
        let mut fs = self.fs.lock();
        if self.modify_disk_inode(|root_inode| {
            // assert it is a directory
            assert!(root_inode.is_dir());
            // has the file been created?
            self.find_inode_id(name, root_inode)
        }).is_some() {
            return None;
        }
        // create a new file
        // alloc a inode with an indirect block
        let new_inode_id = fs.alloc_inode();
        // initialize inode
        let (new_inode_block_id, new_inode_block_offset)
            = fs.get_disk_inode_pos(new_inode_id);
        get_block_cache(
            new_inode_block_id as usize,
            Arc::clone(&self.block_device)
        ).lock().modify(new_inode_block_offset, |new_inode: &mut DiskInode| {
            new_inode.initialize(DiskInodeType::File);
        });
        self.modify_disk_inode(|root_inode| {
            // append file in the dirent
            let file_count = (root_inode.size as usize) / DIRENT_SZ;
            let new_size = (file_count + 1) * DIRENT_SZ;
            // increase size
            self.increase_size(new_size as u32, root_inode, &mut fs);
            // write dirent
            let dirent = DirEntry::new(name, new_inode_id);
            root_inode.write_at(
                file_count * DIRENT_SZ,
                dirent.as_bytes(),
                &self.block_device,
            );
        });

        let (block_id, block_offset) = fs.get_disk_inode_pos(new_inode_id);
        // return inode
        Some(Arc::new(Self::new(
            block_id,
            block_offset,
            self.fs.clone(),
            self.block_device.clone(),
        )))
        // release efs lock automatically by compiler
    }
}
```

##### 文件清空

在以某些标志位打开文件（例如带有 *CREATE* 标志打开一个已经存在的文件）的时候，需要首先将文件清空。在通过文件名索引到文件的 `Inode` 之后，可以调用 `clear` 方法将该文件占据的索引块和数据块回收：

```rust
// easy-fs/src/vfs.rs

impl Inode {
    pub fn clear(&self) {
        let mut fs = self.fs.lock();
        self.modify_disk_inode(|disk_inode| {
            let size = disk_inode.size;
            let data_blocks_dealloc = disk_inode.clear_size(&self.block_device);
            assert!(data_blocks_dealloc.len() == DiskInode::total_blocks(size) as usize);
            for data_block in data_blocks_dealloc.into_iter() {
                fs.dealloc_data(data_block);
            }
        });
    }
}
```

##### 文件读写

从根目录索引到一个文件之后，可以对它进行读写。注意：和 `DiskInode` 一样，这里的读写作用在字节序列的一段区间上：

```rust
// easy-fs/src/vfs.rs
impl Inode {
    pub fn read_at(&self, offset: usize, buf: &mut [u8]) -> usize {
        let _fs = self.fs.lock();
        self.read_disk_inode(|disk_inode| {
            disk_inode.read_at(offset, buf, &self.block_device)
        })
    }
    
    pub fn write_at(&self, offset: usize, buf: &[u8]) -> usize {
        let mut fs = self.fs.lock();
        self.modify_disk_inode(|disk_inode| {
            self.increase_size((offset + buf.len()) as u32, disk_inode, &mut fs);
            disk_inode.write_at(offset, buf, &self.block_device)
        })
    }
}
```

具体实现比较简单，需要注意在执行 `DiskInode::write_at` 之前先调用 `increase_size` 对自身进行扩容：

```rust
// easy-fs/src/vfs.rs

impl Inode {
    fn increase_size(
        &self,
        new_size: u32,
        disk_inode: &mut DiskInode,
        fs: &mut MutexGuard<EasyFileSystem>,
    ) {
        if new_size < disk_inode.size {
            return;
        }
        let blocks_needed = disk_inode.blocks_num_needed(new_size);
        let mut v: Vec<u32> = Vec::new();
        for _ in 0..blocks_needed {
            v.push(fs.alloc_data());
        }
        disk_inode.increase_size(new_size, v, &self.block_device);
    }
}
```

这里会从 `EasyFileSystem` 中分配一些用于扩容的数据块并传给 `DiskInode::increase_size` 。

### 2.3 easyfsy用户态测试与打包

`easy-fs` 的测试放在另一个名为 `easy-fs-fuse` 的应用程序中，不同于 `easy-fs` ，它是一个可以调用标准库 `std` 的应用程序 ，能够在Rust应用开发环境上运行并很容易调试。

#### 模拟块设备

用 Linux 上的一个文件模拟一个块设备：

```rust
// easy-fs-fuse/src/main.rs
use std::fs::File;
use easy-fs::BlockDevice;

const BLOCK_SZ: usize = 512;

struct BlockFile(Mutex<File>);

impl BlockDevice for BlockFile {
    fn read_block(&self, block_id: usize, buf: &mut [u8]) {
        let mut file = self.0.lock().unwrap();
        file.seek(SeekFrom::Start((block_id * BLOCK_SZ) as u64))
            .expect("Error when seeking!");
        assert_eq!(file.read(buf).unwrap(), BLOCK_SZ, "Not a complete block!");
    }

    fn write_block(&self, block_id: usize, buf: &[u8]) {
        let mut file = self.0.lock().unwrap();
        file.seek(SeekFrom::Start((block_id * BLOCK_SZ) as u64))
            .expect("Error when seeking!");
        assert_eq!(file.write(buf).unwrap(), BLOCK_SZ, "Not a complete block!");
    }
}
```

---

测试主函数为 `easy-fs-fuse/src/main.rs` 中的 `efs_test` 函数中，`efs_test` 展示了 `easy-fs` 库的使用方法，大致分成以下几个步骤：

#### 创建块设备并初始化/打开文件系统

```rust
let block_file = Arc::new(BlockFile(Mutex::new({
    let f = OpenOptions::new()
        .read(true)
        .write(true)
        .create(true)
        .open("target/fs.img")?;
    f.set_len(8192 * 512).unwrap();
    f
})));

EasyFileSystem::create(	 // 块设备上不存在文件系统镜像，调用create在内存中生成efs结构，实际很少这样用
    block_file.clone(),
    4096,
    1,
);

let efs = EasyFileSystem::open(block_file.clone()); // 块设备上已放置一个合法的easyfs镜像
```

由于我们在进行测试，需要初始化测试环境，因此在虚拟块设备 `block_file` 上初始化 easy-fs 文件系统，这会将 `block_file` 用于放置 easy-fs 镜像的前 4096 个块上的数据覆盖，然后变成仅有一个根目录的初始文件系统。如果块设备上已经放置了一个合法的 easy-fs 镜像，则我们不必这样做。

#### 获取根目录Inode并进行文件操作

```rust
let root_inode = EasyFileSystem::root_inode(&efs);
```

拿到根目录 `root_inode` 之后，可以通过它进行各种文件操作，目前支持以下几种：

- 通过 `create` 创建文件；

- 通过 `ls` 列举根目录下的文件；

- 通过 `find` 根据文件名索引文件，获取根目录下的一个文件的 inode 之后，可以进行如下操作：

  - 通过 `clear` 将文件内容清空；

  - 通过 `read/write_at` 读写文件，注意我们需要将读写在文件中开始的位置 `offset` 作为一个参数传递进去；

测试方法在这里不详细介绍，大概是每次清空文件 `filea` 的内容，向其中写入一个不同长度的随机数字字符串，然后再全部读取出来，验证和写入的内容一致。其中有一个细节是：用来生成随机字符串的 `rand` crate 并不支持 `no_std` ，因此只有在用户态我们才能更容易进行测试。

### 2.4 将应用打包为easyfs镜像

在第六章中我们需要将所有的应用都链接到内核中，随后在应用管理器中通过应用名进行索引来找到应用的 ELF 数据。这样做有一个缺点，就是会造成内核体积过度膨胀。在 k210 平台上可以很明显的感觉到从第五章开始随着应用数量的增加，向开发板上烧写内核镜像的耗时显著增长。同时这也会浪费内存资源，因为未被执行的应用也占据了内存空间。

在实现了 easy-fs 文件系统之后，终于可以将这些应用打包到 easy-fs 镜像中放到磁盘中，当我们要执行应用的时候只需从文件系统中取出 ELF 执行文件格式的应用并加载到内存中执行即可。

`easy-fs-fuse` 的主体 `easy-fs-pack` 函数就实现了这个功能：

```rust
// easy-fs-fuse/src/main.rs
use clap::{Arg, App};

fn easy_fs_pack() -> std::io::Result<()> {
    let matches = App::new("EasyFileSystem packer")
        .arg(Arg::with_name("source")
            .short("s")
            .long("source")
            .takes_value(true)
            .help("Executable source dir(with backslash)")
        )
        .arg(Arg::with_name("target")
            .short("t")
            .long("target")
            .takes_value(true)
            .help("Executable target dir(with backslash)")
        )
        .get_matches();
    let src_path = matches.value_of("source").unwrap();
    let target_path = matches.value_of("target").unwrap();
    println!("src_path = {}\ntarget_path = {}", src_path, target_path);
    let block_file = Arc::new(BlockFile(Mutex::new({
        let f = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .open(format!("{}{}", target_path, "fs.img"))?;
        f.set_len(8192 * 512).unwrap();
        f
    })));
    // 4MiB, at most 4095 files
    let efs = EasyFileSystem::create(
        block_file.clone(),
        8192,
        1,
    );
    let root_inode = Arc::new(EasyFileSystem::root_inode(&efs));
    let apps: Vec<_> = read_dir(src_path)
        .unwrap()
        .into_iter()
        .map(|dir_entry| {
            let mut name_with_ext = dir_entry.unwrap().file_name().into_string().unwrap();
            name_with_ext.drain(name_with_ext.find('.').unwrap()..name_with_ext.len());
            name_with_ext
        })
        .collect();
    for app in apps {
        // load app data from host file system
        let mut host_file = File::open(format!("{}{}", target_path, app)).unwrap();
        let mut all_data: Vec<u8> = Vec::new();
        host_file.read_to_end(&mut all_data).unwrap();
        // create a file in easy-fs
        let inode = root_inode.create(app.as_str()).unwrap();
        // write data to easy-fs
        inode.write_at(0, all_data.as_slice());
    }
    // list apps
    for app in root_inode.ls() {
        println!("{}", app);
    }
    Ok(())
}
```

尽管没有进行任何同步写回磁盘的操作，我们也不用担心块缓存中的修改没有写回磁盘。因为在 `easy-fs-fuse` 这个应用正常退出的过程中，块缓存因生命周期结束会被回收，届时如果块缓存的 `modified` 标志为 true ，就会将其修改写回磁盘。

### 2.5 内核接入easyfs

为将 `easy-fs` 文件系统接入内核中以支持常规文件和目录，内核中需要有对接 `easy-fs` 文件系统的各种结构，它们**自下而上**可以分成这样几个层次：

- **块设备驱动层**

  针对内核所要运行在的 qemu 平台，我们需要在内核中接入块设备驱动并实现 `easy-fs` 所需的 `BlockDevice` Trait。

- **`easy-fs` 层**

  站在内核的角度，只需知道它接受一个块设备 `BlockDevice` ，并可以在上面打开文件系统 `EasyFileSystem` ，进而获取 `Inode` 核心数据结构，进行各种文件系统操作即可。

- **内核索引节点层**

  在内核中需要将 `easy-fs` 提供的 `Inode` 进一步封装成 `OSInode` ，以表示进程中一个打开的常规文件。由于有很多种不同的打开方式，因此在 `OSInode` 中要维护一些额外的信息。

- 文**件描述符层**

  常规文件对应的 `OSInode` 是文件的内核内部表示，因此需要为它实现 `File` Trait 从而能够可以将它放入到进程文件描述符表中并通过 `sys_read/write` 系统调用进行读写。

- **系统调用层**

  由于引入了常规文件这种文件类型，导致一些系统调用以及相关的内核机制需要进行一定的修改。

---

#### 文件抽象接口 - File

应用程序看到并被操作系统管理的 **文件** (File)  就是一系列的字节组合。操作系统不关心文件内容，只关心如何对文件按字节流进行读写的机制，这就意味着任何程序可以读写任何文件（即字节流），对文件具体内容的解析是应用程序的任务，操作系统对此不做任何干涉。

> 以上就是对 Unix 系统中最著名的一句话的本质解释：“一切皆文件”。
>
> 文件就是操作系统视角下对数据流的抽象。

有了文件这样的抽象后，操作系统内核就可以将能读写并持久存储的数据按文件来进行管理，并把文件分配给进程，让进程以很简洁的统一抽象接口 `File` 来读写数据：

```rust
// os/src/fs/mod.rs

pub trait File : Send + Sync {
    fn read(&self, buf: UserBuffer) -> usize;
    fn write(&self, buf: UserBuffer) -> usize;
}
```

* `read` 和 `write` 的实现则与文件具体是哪种类型有关，它决定了数据如何被读取和写入；
* `UserBuffer` 本质上其实只是一个 `&[u8]` ，位于应用地址空间中，内核无法直接通过用户地址空间的虚拟地址来访问，因此需要进行封装；

#### 块设备驱动层 - virtio-drivers

在启动 Qemu 模拟器的时候，我们可以配置参数来添加一块 VirtIO 块设备：

```shell
# os/Makefile

FS_IMG := ../user/target/$(TARGET)/$(MODE)/fs.img

run-inner: build
ifeq ($(BOARD),qemu)
    @qemu-system-riscv64 \
        -machine virt \
        -nographic \
        -bios $(BOOTLOADER) \
        -device loader,file=$(KERNEL_BIN),addr=$(KERNEL_ENTRY_PA) \
        -drive file=$(FS_IMG),if=none,format=raw,id=x0 \
        -device virtio-blk-device,drive=x0,bus=virtio-mmio-bus.0
```

* `-drive`：为虚拟机添加一块虚拟硬盘，内容为之前通过 `easy-fs-fuse` 工具打包的包含应用 ELF 的 easy-fs 镜像，并命名为 `x0`；
* `-device`：将硬盘 `x0` 作为一个 VirtIO 总线中的一个块设备接入到虚拟机系统中。 `virtio-mmio-bus.0` 表示 VirtIO 总线通过 MMIO 进行控制，且该块设备在总线中的编号为 0 ；

---

VirtIO 外设总线的 MMIO 物理地址区间为从 0x10001000 开头的 4KiB 。为了能够在内核中访问 VirtIO 外设总线，我们就必须在内核地址空间中对特定内存区域提前进行映射：

```rust
// os/src/config.rs

#[cfg(feature = "board_qemu")]
pub const MMIO: &[(usize, usize)] = &[
    (0x10001000, 0x1000), // 起始地址，长度
];
```

在创建内核地址空间的时候需要建立页表映射：

```rust
// os/src/mm/memory_set.rs

use crate::config::MMIO;

impl MemorySet {
    /// Without kernel stacks.
    pub fn new_kernel() -> Self {
        ...
        println!("mapping memory-mapped registers");
        for pair in MMIO {
            memory_set.push(MapArea::new(
                (*pair).0.into(),
                ((*pair).0 + (*pair).1).into(),
                MapType::Identical,
                MapPermission::R | MapPermission::W,
            ), None);
        }
        memory_set
    }
}
```

---

内核访问的块设备实例为 `BLOCK_DEVICE` ：

```rust
// os/drivers/block/mod.rs

#[cfg(feature = "board_qemu")]
type BlockDeviceImpl = virtio_blk::VirtIOBlock;

#[cfg(feature = "board_k210")]
type BlockDeviceImpl = sdcard::SDCardWrapper;

lazy_static! {
    pub static ref BLOCK_DEVICE: Arc<dyn BlockDevice> = Arc::new(BlockDeviceImpl::new());
}
```

在 qemu 上，我们使用 `VirtIOBlock` 访问 VirtIO 块设备，这里直接使用 `virtio-drivers` crate，它需要实现 `easy-fs` 要求的 `BlockDevice` Trait ：

```rust
// os/src/drivers/block/virtio_blk.rs

use virtio_drivers::{VirtIOBlk, VirtIOHeader};
const VIRTIO0: usize = 0x10001000;

pub struct VirtIOBlock(Mutex<VirtIOBlk<'static>>);

impl VirtIOBlock {
    pub fn new() -> Self {
        Self(Mutex::new(VirtIOBlk::new(
            unsafe { &mut *(VIRTIO0 as *mut VirtIOHeader) }
        ).unwrap()))
    }
}

impl BlockDevice for VirtIOBlock {
    fn read_block(&self, block_id: usize, buf: &mut [u8]) {
        self.0.lock().read_block(block_id, buf).expect("Error when reading VirtIOBlk");
    }
    fn write_block(&self, block_id: usize, buf: &[u8]) {
        self.0.lock().write_block(block_id, buf).expect("Error when writing VirtIOBlk");
    }
}
```

* 我们从 `qemu-system-riscv64` 平台上的 Virtio MMIO 区间左端 `VIRTIO0` 开始转化为一个 `&mut VirtIOHeader` 就可以在该平台上访问这些设备寄存器了；
* 很容易为 `VirtIOBlock` 实现 `BlockDevice` Trait ，因为它内部来自 `virtio-drivers` crate 的 `VirtIOBlk` 类型已经实现了 `read/write_block` 方法，我们进行转发即可；

---

在 VirtIO 架构下，需要在公共区域中放置一种叫做 VirtQueue 的环形队列，CPU 可以向此环形队列中向  VirtIO 设备提交请求，也可以从队列中取得请求的结果。对于 VirtQueue 的使用涉及到物理内存的分配和回收，但这并不在 VirtIO 驱动 `virtio-drivers` 的职责范围之内，因此它声明了数个相关的接口，需要库的使用者自己来实现：

```rust
// https://github.com/rcore-os/virtio-drivers/blob/master/src/hal.rs#L57
extern "C" {
    fn virtio_dma_alloc(pages: usize) -> PhysAddr;
    fn virtio_dma_dealloc(paddr: PhysAddr, pages: usize) -> i32;
    fn virtio_phys_to_virt(paddr: PhysAddr) -> VirtAddr;
    fn virtio_virt_to_phys(vaddr: VirtAddr) -> PhysAddr;
}

// os/src/drivers/block/virtio_blk.rs
lazy_static! {
    static ref QUEUE_FRAMES: Mutex<Vec<FrameTracker>> = Mutex::new(Vec::new());
}

#[no_mangle]
pub extern "C" fn virtio_dma_alloc(pages: usize) -> PhysAddr {
    let mut ppn_base = PhysPageNum(0);
    for i in 0..pages {
        let frame = frame_alloc().unwrap();
        if i == 0 { ppn_base = frame.ppn; }
        assert_eq!(frame.ppn.0, ppn_base.0 + i);
        QUEUE_FRAMES.lock().push(frame);
    }
    ppn_base.into()
}

#[no_mangle]
pub extern "C" fn virtio_dma_dealloc(pa: PhysAddr, pages: usize) -> i32 {
    let mut ppn_base: PhysPageNum = pa.into();
    for _ in 0..pages {
        frame_dealloc(ppn_base);
        ppn_base.step();
    }
    0
}

#[no_mangle]
pub extern "C" fn virtio_phys_to_virt(paddr: PhysAddr) -> VirtAddr {
    VirtAddr(paddr.0)
}

#[no_mangle]
pub extern "C" fn virtio_virt_to_phys(vaddr: VirtAddr) -> PhysAddr {
    PageTable::from_token(kernel_token()).translate_va(vaddr).unwrap()
}
```

- `virtio_dma_alloc/dealloc` 需要分配/回收数个 *连续* 的物理页帧，而我们的 `frame_alloc` 是逐个分配，严格来说并不保证分配的连续性。幸运的是，这个过程只会发生在内核初始化阶段，因此能够保证连续性。
- 在 `virtio_dma_alloc` 中通过 `frame_alloc` 得到的那些物理页帧 `FrameTracker` 都会被保存在全局的向量 `QUEUE_FRAMES` 以延长它们的生命周期，避免提前被回收。

#### 内核索引节点层 - OSInode

站在用户的角度看来：

* 在一个进程中可以使用多种不同的标志来打开一个文件，这会影响到打开的这个文件可以用何种方式被访问；
* 此外，在连续调用 `sys_read/write` 读写一个文件的时候，我们知道进程中也存在着一个文件读写的当前偏移量，它也随着文件读写的进行而被不断更新。

这些用户视角中的文件系统抽象特征需要内核来实现，与进程有很大的关系，而 `easy-fs` 文件系统并不涉及这些与进程结合紧密的属性。因此，我们需要将 `easy-fs` 提供的 `Inode` 加上上述信息，进一步封装为 OS 中的索引节点 `OSInode` ：

```rust
// os/src/fs/inode.rs

pub struct OSInode {
    readable: bool,
    writable: bool,
    inner: Mutex<OSInodeInner>,
}

pub struct OSInodeInner {
    offset: usize,
    inode: Arc<Inode>,
}

impl OSInode {
    pub fn new(
        readable: bool,
        writable: bool,
        inode: Arc<Inode>,
    ) -> Self {
        Self {
            readable,
            writable,
            inner: Mutex::new(OSInodeInner {
                offset: 0,
                inode,
            }),
        }
    }
}
```

`OSInode` 就表示进程中一个被打开的常规文件或目录：

* `readable/writable` 分别表明该文件是否允许通过 `sys_read/write` 进行读写；
* 在 `sys_read/write` 期间被维护偏移量 `offset` 和它在 `easy-fs` 中的 `Inode` 则加上一把互斥锁丢到 `OSInodeInner` 中。这在提供内部可变性的同时，也可以简单应对多个进程同时读写一个文件的情况。

#### 文件描述符层 - FDT/FD

一个进程可以访问多个文件，所以内核中需要有一个管理进程访问的多个文件的结构，这就是 **文件描述符表** (File Descriptor Table) ，其中的每个 **文件描述符** (File Descriptor) 代表了一个特定读写属性的I/O资源。

* 每个进程都带有一个线性的 **文件描述符表** ，记录该进程请求内核打开并读写的那些文件集合；
* 而 **文件描述符** (File Descriptor) 则是一个非负整数，表示文件描述符表中一个打开的 **文件描述符** 所处的位置（可理解为数组下标）。

进程通过文件描述符，可以在自身的文件描述符表中找到对应的文件记录信息，从而也就找到了对应的文件，并对文件进行读写。

`OSInode` 被放到进程文件描述符表中作为内核文件的具体表现形式，并可通过 `sys_read/write` 系统调用进行读写操作，因此我们需要为它实现 `File` Trait ：

```rust
// os/src/fs/inode.rs

impl File for OSInode {
    fn readable(&self) -> bool { self.readable }
    fn writable(&self) -> bool { self.writable }
    fn read(&self, mut buf: UserBuffer) -> usize {
        let mut inner = self.inner.lock();
        let mut total_read_size = 0usize;
        for slice in buf.buffers.iter_mut() {
            let read_size = inner.inode.read_at(inner.offset, *slice);
            if read_size == 0 {
                break;
            }
            inner.offset += read_size;
            total_read_size += read_size;
        }
        total_read_size
    }
    fn write(&self, buf: UserBuffer) -> usize {
        let mut inner = self.inner.lock();
        let mut total_write_size = 0usize;
        for slice in buf.buffers.iter() {
            let write_size = inner.inode.write_at(inner.offset, *slice);
            assert_eq!(write_size, slice.len());
            inner.offset += write_size;
            total_write_size += write_size;
        }
        total_write_size
    }
}
```

* 为 `File` Trait 新增了 `readable/writable` 两个抽象接口从而在 `sys_read/sys_write` 的时候进行简单的访问权限检查；
* 在 `read/write` 的全程需要获取 `OSInode` 的互斥锁，保证两个进程无法同时访问同个文件。

---

为了支持进程对文件的管理，我们需要在进程控制块中加入文件描述符表的相应字段：

```rust
// os/src/task/task.rs

pub struct TaskControlBlockInner {
   	//...
    pub fd_table: Vec<Option<Arc<dyn File + Send + Sync>>>,
}
```

可以看到 `fd_table` 的类型包含多层嵌套，我们**从外到里**分别说明：

- `Vec` 的动态长度特性使得我们无需设置一个固定的文件描述符数量上限，我们可以更加灵活的使用内存，而不必操心内存管理问题；
- `Option` 使得我们可以区分一个文件描述符当前是否空闲，当它是 `None` 的时候是空闲的，而 `Some` 则代表它已被占用；
- `Arc` 首先提供了共享引用能力。后面我们会提到，可能会有多个进程共享同一个文件对它进行读写。此外被它包裹的内容会被放到内核堆而不是栈上，于是它便不需要在编译期有着确定的大小；
- `dyn` 关键字表明 `Arc` 里面的类型实现了 `File/Send/Sync` 三个 Trait ，但是编译期无法知道它具体是哪个类型（可能是任何实现了 `File` Trait 的类型如 `Stdin/Stdout` ，故而它所占的空间大小自然也无法确定），需要等到运行时才能知道它的具体类型，对于一些抽象方法的调用也是在那个时候才能找到该类型实现的方法并跳转过去。

> 在 Rust 中主要通过泛型和 Trait 来实现多态。
>
> * 泛型是一种 **编译期多态** (Static  Polymorphism)，在编译一个泛型函数的时候，编译器会对于所有可能用到的类型进行实例化并对应生成一个版本的汇编代码，在编译期就能知道选取哪个版本并确定函数地址，这可能会导致生成的二进制文件体积较大；
> * Trait 对象（也即上面提到的 `dyn` 语法）是一种 **运行时多态** (Dynamic Polymorphism)，需要在运行时查一种类似于 C++ 中的 **虚表** (Virtual Table) 才能找到实际类型对于抽象接口实现的函数地址并进行调用，这样会带来一定的运行时开销，但是更省空间且灵活。

---

#### 应用访问文件的内核机制实现

##### 文件系统初始化

为了使用 `easy-fs` 提供的抽象和服务，内核需要一系列初始化操作：

1. 打开块设备
2. 从块设备 `BLOCK_DEVICE` 上打开文件系统
3. 从文件系统中获取根目录的 inode 

```rust
// os/drivers/block/mod.rs
#[cfg(feature = "board_qemu")]
type BlockDeviceImpl = virtio_blk::VirtIOBlock;

#[cfg(feature = "board_k210")]
type BlockDeviceImpl = sdcard::SDCardWrapper;

lazy_static! {
    pub static ref BLOCK_DEVICE: Arc<dyn BlockDevice> = Arc::new(BlockDeviceImpl::new());
}

// os/src/fs/inode.rs
lazy_static! {
    pub static ref ROOT_INODE: Arc<Inode> = {
        let efs = EasyFileSystem::open(BLOCK_DEVICE.clone());
        Arc::new(EasyFileSystem::root_inode(&efs))
    };
}

// os/src/fs/inode.rs
pub fn list_apps() {
    println!("/**** APPS ****");
    for app in ROOT_INODE.ls() {
        println!("{}", app);
    }
    println!("**************/")
}
```

---

##### 打开与关闭文件

接着，我们实现 `open_file` 内核函数，可根据文件名打开一个根目录下的文件：

```rust
// os/src/fs/inode.rs

pub fn open_file(name: &str, flags: OpenFlags) -> Option<Arc<OSInode>> {
    let (readable, writable) = flags.read_write();
    if flags.contains(OpenFlags::CREATE) {
        if let Some(inode) = ROOT_INODE.find(name) {
            // clear size
            inode.clear();
            Some(Arc::new(OSInode::new(
                readable,
                writable,
                inode,
            )))
        } else {
            // create file
            ROOT_INODE.create(name)
                .map(|inode| {
                    Arc::new(OSInode::new(
                        readable,
                        writable,
                        inode,
                    ))
                })
        }
    } else {
        ROOT_INODE.find(name)
            .map(|inode| {
                if flags.contains(OpenFlags::TRUNC) {
                    inode.clear();
                }
                Arc::new(OSInode::new(
                    readable,
                    writable,
                    inode
                ))
            })
    }
}
```

在其基础上，实现 `sys_open/sys_close` ：

```rust
// os/src/syscall/fs.rs

pub fn sys_open(path: *const u8, flags: u32) -> isize {
    let task = current_task().unwrap();
    let token = current_user_token();
    let path = translated_str(token, path);
    if let Some(inode) = open_file(
        path.as_str(),
        OpenFlags::from_bits(flags).unwrap()
    ) {
        let mut inner = task.acquire_inner_lock();
        let fd = inner.alloc_fd();
        inner.fd_table[fd] = Some(inode);
        fd as isize
    } else {
        -1
    }
}

pub fn sys_close(fd: usize) -> isize {
    let task = current_task().unwrap();
    let mut inner = task.inner_exclusive_access();
    if fd >= inner.fd_table.len() {
        return -1;
    }
    if inner.fd_table[fd].is_none() {
        return -1;
    }
    inner.fd_table[fd].take();
    0
}
```

* `open_file`：通过文件名从根目录开始寻找对应的 Inode ，然后再进一步封装为 OSInode 并返回；
* `sys_open`：调用了 open_file 获取文件对应的 OSInode，然后分配 fd 并插入 fd_table；
* `sys_close`：将进程控制块中的文件描述符表对应的一项改为 `None` 代表它已经空闲即可，这将导致内层的引用计数类型 `Arc` 被销毁，会减少一个文件的引用计数，当引用计数减少到 0 之后文件所占用的资源就会被自动回收。

---

##### 基于文件加载并执行应用

在有了文件系统支持之后，`sys_exec` 所需的应用的 ELF 文件格式的数据就不再需要通过应用加载器从内核的数据段获取，而是从文件系统中获取，这样内核与应用的代码/数据就解耦了：

```rust
// os/src/syscall/process.rs

pub fn sys_exec(path: *const u8) -> isize {
    let token = current_user_token();
    let path = translated_str(token, path);
    if let Some(app_inode) = open_file(path.as_str(), OpenFlags::RDONLY) {
        let all_data = app_inode.read_all();
        let task = current_task().unwrap();
        task.exec(all_data.as_slice());
        0
    } else {
        -1
    }
}
```

同样的，在内核中创建初始进程 `initproc` 也需要替换为基于文件系统的实现：

```rust
// os/src/task/mod.rs

lazy_static! {
    pub static ref INITPROC: Arc<TaskControlBlock> = Arc::new({
        let inode = open_file("initproc", OpenFlags::RDONLY).unwrap();
        let v = inode.read_all();
        TaskControlBlock::new(v.as_slice())
    });
}
```

---

##### 读写文件

基于文件抽象接口和文件描述符表，我们可以按照无结构的字节流来处理基本的文件读写，这样可以让文件读写系统调用 `sys_read/write` 变得更加具有普适性：

```rust
// os/src/syscall/fs.rs

pub fn sys_read(fd: usize, buf: *const u8, len: usize) -> isize {
    let token = current_user_token();
    let task = current_task().unwrap();
    let inner = task.inner_exclusive_access();
    if fd >= inner.fd_table.len() {
        return -1;
    }
    if let Some(file) = &inner.fd_table[fd] {
        let file = file.clone();
        // release current task TCB manually to avoid multi-borrow
        drop(inner);
        file.read(
            UserBuffer::new(translated_byte_buffer(token, buf, len))
        ) as isize
    } else {
        -1
    }
}
```

* 操作系统都是通过文件描述符在当前进程的文件描述符表中找到某个文件，无需关心文件具体的类型，只要知道它一定实现了 `File` Trait 的 `read/write` 方法即可；

* Trait 对象提供的运行时多态能力会在运行的时候帮助我们定位到符合实际类型的 `read/write` 方法。



## 3 疑难解决

easyfs 文件系统的一些 Rust 编程技巧，包括对模板、闭包等特性的使用，主要体现在从 Inode 层发起内存访问的流程中：

```rust
// easy-fs/src/block_cache.rs
impl BlockCache {
    fn addr_of_offset(&self, offset: usize) -> usize {
        &self.cache[offset] as *const _ as usize
    }
   
    pub fn get_ref<T>(&self, offset: usize) -> &T where T: Sized {
        let type_size = core::mem::size_of::<T>();
        assert!(offset + type_size <= BLOCK_SZ);
        let addr = self.addr_of_offset(offset);
        unsafe { &*(addr as *const T) }
    }

    pub fn get_mut<T>(&mut self, offset: usize) -> &mut T where T: Sized {
        let type_size = core::mem::size_of::<T>();
        assert!(offset + type_size <= BLOCK_SZ);
        self.modified = true;
        let addr = self.addr_of_offset(offset);
        unsafe { &mut *(addr as *mut T) }
    }
    
    pub fn read<T, V>(&self, offset: usize, f: impl FnOnce(&T) -> V) -> V {
        f(self.get_ref(offset))
    }

    pub fn modify<T, V>(&mut self, offset:usize, f: impl FnOnce(&mut T) -> V) -> V {
        f(self.get_mut(offset))
    }
}

// easy-fs/src/vfs.rs
impl Inode {
    fn read_disk_inode<V>(&self, f: impl FnOnce(&DiskInode) -> V) -> V {
        get_block_cache(
            self.block_id,
            Arc::clone(&self.block_device)
        ).lock().read(self.block_offset, f)
    }

    fn modify_disk_inode<V>(&self, f: impl FnOnce(&mut DiskInode) -> V) -> V {
        get_block_cache(
            self.block_id,
            Arc::clone(&self.block_device)
        ).lock().modify(self.block_offset, f)
    }
}
```

分析其中最长的一条调用链，以 `read` 为例：

```rust
inode.read_disk_inode(f) 
	+-> block_cache.read(offset, f) 
		+-> f(self.get_ref(offset))
```

* 根据传入 block_id 创建了一个 block_cache，将一个闭包参数 f 传入，其实现了 `FnOnce(&mut DiskInode) -> V` 

  >那么一个闭包该如何实现 `FnOnce(&mut DiskInode) -> V` ？：
  >
  >**一个闭包实现了哪种 Fn 特征取决于该闭包如何使用被捕获的变量，而不是取决于闭包如何捕获它们**，所有的闭包都自动实现了 `FnOnce` 特征，因此任何一个闭包都至少可以被调用一次。
  >
  >`read_disk_inode` 的实际调用场景为：
  >
  >```rust
  >impl Inode {
  >    pub fn find(&self, name: &str) -> Option<Arc<Inode>> {
  >        let fs = self.fs.lock();
  >        self.read_disk_inode(|disk_inode| {
  >            self.find_inode_id(name, disk_inode)
  >            .map(|inode_id| {
  >                let (block_id, block_offset) = fs.get_disk_inode_pos(inode_id);
  >                Arc::new(Self::new(
  >                    block_id,
  >                    block_offset,
  >                    self.fs.clone(),
  >                    self.block_device.clone(),
  >                ))
  >            })
  >        })
  >    }
  >}
  >```

* 将 BlockCache 结构中的 read<T, V> 方法中的参数类型 T 实例化为 `DiskInode`；
* 最后通过 `f(self.get_ref(offset))` 将 get_ref 方法的返回值类型 T 实例化 `DiskInode`，这样 get_ref 方法就能返回一个 `DiskInode` 大小的不可变引用，然后直接通过该引用读取各字段。`modify_disk_inode -> modify -> get_mut` 调用链同理。



## 4 //TODO: 实验









