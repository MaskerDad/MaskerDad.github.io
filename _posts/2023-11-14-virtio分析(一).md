---
title: virtio分析(一)

date: 2023-11-14 17:00:00 +0800

categories: [kernel, virt, I/O虚拟化]

tags: [virt, qemu, kvm]

description:这块内容是一个大部头，涉及的前置软硬件知识和技术都很多：PCI/PCIe、DMA/MMIO/IOMMU、Interrupt等。我接触这部分内容的时候，有很多疑惑，且通过搜索有时也找不到一个合理解释，这是有很多知识都比较模糊所导致的。这篇文档就作为系统学习这部分内容的开端吧！
---

# 1 IO虚拟化

IO虚拟化 (设备虚拟化) 可以说是虚拟化技术中开销最大的环节，针对于CPU/内存方面的虚拟化在软硬件上的优化基本上已达到瓶颈，有些云厂商甚至针对IO虚拟化的开销重新定义了一套虚拟化架构，比如X-Dragon/AWS-Nitro。

在虚拟化系统中，I/O外设只有一套，需要被多个guest VMs共享。hypervisor提供了两种机制来实现对I/O设备的访问，**一种是直通/透传（passthrough），一种是模拟（emulation）。**所谓透传，就是指guest VM可以透过hypervisor，直接访问I/O硬件，这样guest VM的I/O操作路径几乎和无虚拟化环境下的I/O路径相同，性能自然是非常高的，但设备透传存在的一个问题，就是同一个I/O设备通常无法在不同的guest VM之间实现共享和动态迁移。

> **设备透传还涉及了中断透传虚拟机的原理，这部分内容在其他文档整理吧。**

设备模拟可以解决设备直通存在的**真实设备在多虚拟机间共享**的问题，以 `qemu-kvm` 为例，我们来看一下传统的IO全虚拟化的工作流程：

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161502978.png)

1. Guest产生I/O请求，被KVM 截获；
2. Kvm 经过处理后将I/O请求存放在I/O共享页；
3. 通知Qemu，I/O已经存入I/O共享页；
4. Qemu从I/O共享页拿到I/O请求；
5. Qemu模拟代码来模拟本次的I/O，并发送给相应的设备驱动；
6.  硬件去完成I/O操作并返回结果Qemu；
7. Qemu将结果放回I/O共享页；
8. Qemu通知Kvm去I/O共享页拿结果；
9. Kvm去I/O共享页拿到结果；
10. Kvm将结果返回给Guest；

从以上过程我们可以看到，全虚拟化实现中每次IO请求都需要产生一次 `VM_Exit/VM_Entry`。

---

从总体上看，virtio 可以分为四层，包括前端 guest 中各种驱动程序模块，后端 Hypervisor （实现在Qemu上）上的处理程序模块，中间用于前后端通信的 virtio 层和 virtio-ring 层，virtio 这一层实现的是虚拟队列接口，算是前后端通信的桥梁，而 virtio-ring 则是该桥梁的具体实现，它实现了两个环形缓冲区，分别用于保存前端驱动程序和后端处理程序执行的信息。

其中前端驱动(frondend，如virtio-blk、virtio-net等)是在客户机中存在的驱动程序模块，而后端处理程序（backend）是在QEMU中实现的。在这前后端驱动之间，还定义了两层来支持客户机与QEMU之间的通信。其中，“virtio”这一层是虚拟队列接口，它在概念上将前端驱动程序附加到后端处理程序。一个前端驱动程序可以使用0个或多个队列，具体数量取决于需求。例如，virtio-net网络驱动程序使用两个虚拟队列（一个用于接收，另一个用于发送），而virtio-blk块驱动程序仅使用一个虚拟队列。

虚拟队列实际上被实现为跨越客户机操作系统和hypervisor的衔接点，但它可以通过任意方式实现，前提是客户机操作系统和virtio后端程序都遵循一定的标准，以相互匹配的方式实现它。**而virtio-ring实现了环形缓冲区(ring buffer)，用于保存前端驱动和后端处理程序执行的信息，并且它可以一次性保存前端驱动的多次I/O请求，并且交由后端去批量处理，最后实际调用宿主机中设备驱动实现物理上的I/O操作，这样做就可以根据约定实现批量处理而不是客户机中每次I/O请求都需要处理一次，从而提高客户机与hypervisor信息交换的效率。**

严格来说，virtio 和 virtio-ring 可以看做是一层，virtio-ring 实现了 virtio 的具体通信机制和数据流程。或者这么理解可能更好，virtio 层属于控制层，负责前后端之间的通知机制（kick，notify）和控制流程，而 virtio-vring 则负责具体数据流转发。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161512145.png)



# 2 virtio框架分析

## 2.1 整体架构

 在传统的设备模拟中，虚拟机内部设备驱动完全不知道自己处在虚拟化环境中，所以I/O操作会完整的走**虚拟机内核栈->QEMU->宿主机内核栈，**产生很多 `VM Exit/VM Entry`，导致性能很差。Virtio方案旨在提高I/O性能。在改方案中虚拟机能够感知到自己处于虚拟化环境中，并且会加载相应的virtio总线驱动和virtio设备驱动，执行自己定义的协议进行数据传输，减少 `VM Exit/VM Entry` 操作。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311160923492.png)

Virtio是一种前后端架构，包括**<font color='red'>前端驱动（Guest内部）、后端设备（QEMU设备）、传输协议（vring）：</font>**

*  **前端驱动**：虚拟机内部的 virtio 模拟设备对应的驱动。作用为接收用户态的请求，然后按照传输协议对请求进行封装，再写I/O操作，发送通知到QEMU后端设备。
* **后端设备：**在QEMU中创建，用来接收前端驱动发送的I/O请求，然后按照传输协议进行解析，再对物理设备进行操作，之后通过终端机制通知前端设备。
* **传输协议：**使用virtio队列（virtqueue）完成。设备有若干个队列，每个队列处理不同的数据传输（如virtio-balloon包含ivq、dvq、svq三个）。virtqueue通过vring实现。Vring是虚拟机和QEMU之间共享的一段环形缓冲区，QEMU和前端设备都可以从vring中读取数据和放入数据。

## 2.2 流程分析

### virtio流程

从代码上看，virtio的代码主要分两个部分：**QEMU和内核驱动程序。**Virtio设备的模拟就是通过QEMU完成的，QEMU代码在虚拟机启动之前，创建虚拟设备。虚拟机启动后检测到设备，调用内核的virtio设备驱动程序来加载这个virtio设备。VRING是由虚拟机virtio设备驱动创建的用于数据传输的共享内存，QEMU进程通过这块共享内存获取前端设备递交的IO请求。

> 对于KVM虚拟机，都是通过QEMU这个用户空间程序创建的，每个KVM虚拟机都是一个QEMU进程，虚拟机的virtio设备是QEMU进程模拟的，虚拟机的内存也是从QEMU进程的地址空间内分配的。

 如下图所示，虚拟机IO请求的整个流程：

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311160928172.png)

1. **虚拟机产生的IO请求会被前端的virtio设备接收，并存放在virtio设备散列表 `scatterlist` 里；**
2. **Virtio设备的virtqueue提供 `add_buf` 将散列表中的数据映射至前后端数据共享区域Vring中；**
3. **Virtqueue通过 `kick` 函数来通知后端qemu进程，`kick` 通过写pci配置空间的寄存器产生vm exit；**
4. **Qemu端注册 `ioport_write/read` 函数监听PCI配置空间的改变，获取前端的通知消息；**
5. **Qemu端维护的virtqueue队列从数据共享区vring中获取数据；**
6. **Qemu将数据封装成virtioreq;**
7. **Qemu进程将请求发送至硬件层。**

> 前后端主要通过PCI配置空间的寄存器完成前后端的通信，而IO请求的数据地址则存在vring中，并通过共享vring这个区域来实现IO请求数据的共享。

从上图中可以看到，Virtio设备的驱动分为前端与后端：前端是虚拟机的设备驱动程序，后端是host上的QEMU用户态程序。**为了实现虚拟机中的IO请求从前端设备驱动传递到后端QEMU进程中，Virtio框架提供了两个核心机制：前后端消息通知机制和数据共享机制。**

* **消息通知机制：**前端驱动设备产生IO请求后，可以通知后端QEMU进程去获取这些IO请求，递交给硬件。**<font color='red'>*IO请求如何通知？*</font>**

* **数据共享机制：**前端驱动设备在虚拟机内申请一块内存区域，将这个内存区域共享给后端QEMU进程，前端的IO请求数据就放入这块共享内存区域，QEMU接收到通知消息后，直接从共享内存取数据。由于KVM虚拟机就是一个QEMU进程，虚拟机的内存都是QEMU申请和分配的，属于QEMU进程的线性地址的一部分，因此虚拟机只需将这块内存共享区域的地址传递给QEMU进程，QEMU就能直接从共享区域存取数据。

  ***<font color='red'>数据如何共享？</font>***

### 核心机制分析

本节我们来分析这两个关键机制：**消息通知机制和数据共享机制。**

#### 消息通知机制 - PCI配置空间

由整体流程图可知，实现IO请求通知的重要结构为PCI config，即PCI配置空间，它充当了前后端通信的中介。整个消息通知的流程为：

* 首先虚拟机需要获取到PCI配置空间 => ***获取***
* 虚拟机访问PCI配置空间，相当于发送消息 => ***访问/操作***
* qemu感知到PCI配置空间的变化，然后读取消息 => ***感知***

接下来，逐个分析这三步。

##### 虚拟机如何获取PCI配置空间的？

首先，我们为虚拟机创建的virtio设备都是PCI设备，它们挂在PCI总线上，遵循通用PCI设备的发现、挂载等机制。

当虚拟机启动发现virtio PCI设备时，只有配置空间可以被访问，配置空间内保存着该设备工作所需的信息，如厂家、功能、资源要求等，通过对这个空间信息的读取，完成对PCI设备的配置。同时配置空间上有一块存储器空间，里面包含了一些寄存器和IO空间。

前后端的通知消息就是写在这些存储空间的寄存器，virtio会为它的PCI设备注册一个PCI BAR来访问这块寄存器空间。配置空间如下图所示：

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311160958223.png)

虚拟机系统在启动过程中在PCI总线上发现virtio-pci设备，就会调用virtio-pci的 `probe` 函数。该函数会将PCI配置空间上的寄存器映射到内存空间，并将这个地址赋值给virtio_pci_device的ioaddr变量。之后要对PCI配置空间上的寄存器操作时，只需要 `ioaddr + offset`。

```c++
vp_dev->ioaddr = pci_iomap(pci_dev, 0, 0);
```

`pci_iomap` 函数完成PCI BAR的映射，第一个参数是pci设备的指针，第二个参数指定我们要映射的是0号BAR，第三个参数确定要映射的BAR空间多大，当第三个参数为0时，就将整个BAR空间都映射到内存空间上。Virtio PCI设备的0号BAR指向的就是配置空间的寄存器空间，也就是配置空间上用于消息通知的寄存器。

通过 `pci_iomap` 之后，我们就可以像操作普通内存一样（调用ioread和iowrite）来读写pci硬件设备上的寄存器。

##### 虚拟机如何操作PCI配置空间的？

当前端设备的驱动程序需要通知后端QEMU程序执行某些操作的时候，就会调用 `kick` 函数，来触发读写PCI配置空间寄存器的动作。`ioread/iowrite` 实现了对配置空间寄存器的读写，例如：

```c
// 写notify寄存器
iowrite16(info->queue_index, vp_dev->ioaddr + VIRTIO_PCI_QUEUE_NOTIFY);

// 读取QEMU端在配置空间寄存器上写下的值
num = ioread16(vp_dev->ioaddr + VIRTIO_PCI_QUEUE_NUM);
```

在读写PCI设备配置空间的操作中，可以看到都是通过 `iodaar + offset` ，来指向某个寄存器，ioaddr这个变量是在Virtio-pci设备初始化的时候对它赋值，并指向配置空间寄存器的首地址位置。

##### qemu如何感知虚拟机的操作？

虚拟机内调用 `kick` 函数实现通知之后，会产生VM_EXIT。Host端的KVM模块捕获到这个EXIT之后，根据它退出的原因来做处理。如果是一个IO_EXIT，KVM会将这个退出交给用户态的QEMU程序来完成IO操作。

QEMU为KVM虚拟机模拟了virtio设备，因此后端的virtio-pci设备也是在QEMU进程中模拟生成的。QEMU对模拟的PCI设备的配置空间注册了回调函数，当虚拟机产生IO_EXIT，就调用这些函数来处理事件。

> **这里只分析legacy模式，其实在初始化阶段guest会判断设备是否支持modern模式，如果支持，回调函数会发生一些变化。**

qemu端的处理，“感知”是通过**监听PCI寄存器**实现的，下面分析监听函数的实现以及注册：

* **监听函数 - 监听PCI寄存器：**`virtio_ioport_write/read` 就是QEMU进程监听PCI配置空间上寄存器消息的函数，针对前端iowrite/ioread读写了哪个PCI寄存器，来决定下一步操作：

  ![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161029359.png)

* **监听函数的注册：**PCI寄存器的这些监听函数，都是在QEMU为虚拟机创建虚拟设备的时候注册的。QEMU先为虚拟机的virtio-pci设备创建PCI配置空间，配置空间内包含了设备的一些基本信息；然后在配置空间的存储空间位置注册了一个PCI BAR，并为这个BAR注册了回调函数监听寄存器的改变。以下代码是初始化配置空间的基本信息：

  ![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161033156.png)

  给PCI设备注册了PCI BAR，指定起始地址为PCI_BASE_ADDRESS_SPACE_IO（即PCI配置空间中存储空间到配置空间首地址的偏移值）；指定这个BAR的大小为size，回调函数为 `virtio_pci_config_ops` 中的读写函数。

  ![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161035574.png)

  这里的read/write最终都会调用virtio_ioport_write（virtio_ioport_write处理前端写寄存器时触发的事件，virtio_ioport_read处理前端要读寄存器时触发的事件）来统一的管理。

#### 数据共享机制 - Vring

消息通知之后数据如何传送呢？答案是通过 `vring`。

##### vring数据结构

VRING共享区域总共有三个表：

* **vring_desc**表，存放虚拟机产生的IO请求的地址；
* **vring_avail**表，指明vring_desc中哪些项是可用的；
* **vring_used**表，指明vring_desc中哪些项已经被递交到硬件。

***前后端对vring的操作过程是这样的：***我们往virng_desc表中存放IO请求，用vring_avail告诉QEMU进程vring_desc表中哪些项是可用的，QEMU将IO请求递交给硬件执行后，用vring_used表来告诉前端vring_desc表中哪些项已经被递交，可以释放这些项了。

```c
struct vring {
    unsigned int num;
    struct vring_desc *desc;
    struct vring_avail *avail;
    struct vring_used *used;
};
```

下面来看看 `vring_desc/vring_avail/vring_used` 的具体实现：

> **vring_desc**

```c
/* Virtio ring descriptors: 16 bytes.  These can chain together via "next". */
struct vring_desc {
    /* Address (guest-physical). */
    __virtio64 addr;
    /* Length. */
    __virtio32 len;
    /* The flags as indicated above. */
    __virtio16 flags;
    /* We chain unused descriptors via this, too */
    __virtio16 next;
};
```

该结构用于存储虚拟机产生的IO请求在内存中的地址 (GPA)，在这个表中每一行都包含四个域，如下所示：

* **Addr，**存储IO请求在虚拟机内的内存地址，是一个GPA值；
* **len，**表示这个IO请求在内存中的长度；
* **flags，**指示这一行的数据是可读、可写（VRING_DESC_F_WRITE），是否是一个请求的最后一项（VRING_DESC_F_NEXT）；
* **next，**每个IO请求都有可能包含了vring_desc表中的多行，next域就指明了这个请求的下一项在哪一行。

实际上，next将一个IO请求在vring_desc中存储的多行连接成了一个链表，当flag=~ VRING_DESC_F_NEXT，就表示这个链表到了末尾。如下图所示，表示desc表中有两个IO请求，分别通过next域组成了链表：

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161056135.png)

> **vring_avail**

存储的是每个IO请求在vring_desc中连接成的链表的表头位置，数据结构如下所示：

```c
struct vring_avail {
    __virtio16 flags;
    __virtio16 idx;
    __virtio16 ring[];
};
```

在vring_desc表中：

* `ring[]`：通过next域连接起来的链表的表头在vring_desc表中的位置；
* `idx`：指向的是ring数组中下一个可用的空闲位置；
* `flags`：一个标志域。

如下图所示， vring_avail表指明了vring_desc表中有两个IO请求组成的链表是最近更新可用的，它们分别从0号位置和3号位置开始。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161101688.png)

> **vring_used**

```c
struct vring_used_elem {
    /* Index of start of used descriptor chain. */
    __virtio32 id;
    /* Total length of the descriptor chain which was used (written to) */
    __virtio32 len;
};

struct vring_used {
    __virtio16 flags;
    __virtio16 idx;
    struct vring_used_elem ring[];
};
```

分析下 `vring_used`：

* `ring[]`：其中数组元素的结构有两个成员

  * `id`：表示处理完成的IO request在vring_desc表中的组成的链表的头结点位置；

  * `len`：表示链表的长度；

* `idx`：指向了ring数组中下一个可用的位置；
* `flags`：标记位。

如下图所示，vring_used表示vring_desc表中的从0号位置开始的IO请求已经被递交给硬件，前端可以释放vring_desc表中的相应项。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161107843.png)

##### 对vring进行操作

Vring的操作分为两部分：

* **在前端虚拟机内，**通过 `virtqueue_add_buf` 将IO请求的内存地址，该地址再放入vring_desc表中，同时更新vring_avail表；
* **在后端QEMU进程内，**根据vring_avail表的内容，通过 `virtqueue_get_buf` 从vring_desc表中取得数据，同时更新vring_used表；

> **前端：virtqueue_add_buf**

虚拟机内通过以下步骤将IO请求地址存至vring_desc表中，并通过 `kick` 函数通知后端来读取数据。

1. 将IO请求的地址存入当前空闲的vring_desc表中的addr（如果没有空闲表项，则通知后端完成读写请求，释放空间）；
2. 设置flags域，若本次IO请求还未完，则为VRING_DESC_F_NEXT，并转至 `3`；若本次IO请求的地址都已保存至vring_desc中，则为~VRING_DESC_F_NEXT，转至 `4`；
3. 根据next，找到下一个空闲的vring_desc表项，跳转至 `1`；
4. 本次IO请求已全部存在vring_desc表中，并通过next域连接成了一个链表，将链表头结点在vring_desc表中位置写入vring_avail->ring[idx]，并使idx++。

举个实际例子，假设在add_buf之前vring_desc表中已经保存了一个IO请求链表，可以从vring_avail中知道，vring_desc表中的IO请求链表头结点位置为0，然后根据next遍历整个IO请求链表的flags域，最后会终止在~_NEXT处。

1. 调用add_buf将本次IO请求放入vring_desc表中，在vring_desc表中的第3行添加一个数据项，flags域设置为NEXT，表示本次IO请求的内容还没有结束，同时next++；
2. 从next域找到下一个空闲的vring_desc表项，即第4行，添加一行数据，flags域设置为~NEXT，表示本次IO请求的内容已经结束next域置为空；
3. 更新vring_avail表，从idx找到viring_avali表中的第一个空闲位置（第2行），把添加到vring_desc表中的IO请求链表的头结点位置(也就是图中vring_desc表的第3行)，添加到vring_avail表中；更新vring_avail的idx加1。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161125701.png)

> **后端：virtqueue_get_buf**

后端获取数据的步骤如下：

1. 从vring_avail中取出数据，直到取到idx位置为止；
2. 根据vring_avail中取到的值，从vring_desc中取出链表的头结点，并根据next域依次找到其余结点；
3. 当IO请求被取出后，将链表头结点的位置值放入vring_used->ring[idx].id。

同样举个实际例子。假设在QEMU进行操作之前，vring_avial表中显示vring_desc表中有两个新的IO请求。

1. 从vring_avail表中取出第一个IO请求的位置(vring_desc第0行)，从vring_desc表的第0行开始获取IO请求，若flags为NEXT则根据next继续往下寻找；若flags为~NEXT，则表示这个IO请求已经结束；
2. QEMU将这个IO请求封装，发送硬件执行；
3. 更新vring_used表，将从vring_desc取出的IO请求的链表的头结点位置存到vring_used->idx所指向的位置，并将idx加1。这样当IO处理返回到虚拟机时，virtio驱动程序可以根据vring_uesd表中的信息释放vring_desc表的相应表项。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161238043.png)

##### 前端对vring的管理

vring属于vring_virtqueue，同时vring_vritqueue包含virtqueue。两部分分工明确：**<font color='red'>vring负责数据面，vritqueue负责控制面。</font>**

这部分后续以 virtio-balloon 为例，分析前后端数据共享的源头：GUEST内部如何管理，既vring的诞生过程。本节仅介绍virtio数据共享相关。

> **结构体**

```c
/* virtio_balloon 驱动结构 */
struct virtio_balloon {
    struct virtio_device *vdev;
        /* balloon包含三个virtqueue */
    struct virtqueue *inflate_vq, *deflate_vq, *stats_vq;
        ...
}

struct virtqueue {
    struct list_head list;
    void (*callback)(struct virtqueue *vq);
    const char *name;
    struct virtio_device *vdev;
    unsigned int index;
    unsigned int num_free;
    void *priv;
};

struct vring_virtqueue {
    struct virtqueue vq;

    /* Actual memory layout for this queue */
    struct vring vring;
        ...
}
```

> **数据共享区创建**

由linux驱动模型可知，驱动入口函数为 `virtballoon_probe`，我们由此来分析数据共享区创建过程，整体调用逻辑如下：

* **设备驱动层**

  ```c
  virtballoon_probe ->
      init_vqs ->
          virtio_find_vqs ->
              vdev->config->find_vqs
  ```

* **PCI设备层**

  ```c
  (vdev->config->find_vqs)vp_modern_find_vqs ->
      vp_modern_find_vqs ->
          vp_find_vqs ->
              vp_find_vqs_intx/msix ->
                  vp_setup_vq -> //实现pci设备中virtqueue的赋值
                      vp_dev->setup_vq  //真正创建virtqueue
  ```

* **virtqueue创建**

  ```c
  setup_vq ->
      //1. 获取设备注册的virtqueue大小等信息
      vp_ioread16
      //2. 创建vring
      vring_create_virtqueue ->
          __vring_new_virtqueue
      //3. 共享内存地址通知qemu侧模拟的设备
      vp_iowrite16
      //4. 更新notify消息发送的地址
      vq->priv_update
  ```



# 3 后端设备创建 - virtio-balloon

## 3.1 概述

根据前一章信息，virtio设备分为前端设备/通信层/后端设备，本章从后端设备设备（qemu的balloon设备为例）的初始化开始分析**。从启动到balloon设备开始初始化**基本调用流程如下：

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161537190.png)

 **balloon代码执行流程如下：**

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311161542253.png)

## 3.2 关键结构

### balloon设备结构

```c
typedef struct VirtIOBalloon {
    VirtIODevice parent_obj;
    VirtQueue *ivq, *dvq, *svq;  // 3个 virt queue
    // pages we want guest to give up 
    uint32_t num_pages; 
    // pages in balloon
    uint32_t actual;
    uint64_t stats[VIRTIO_BALLOON_S_NR];  // status 
     
    // status virtqueue 会用到
    VirtQueueElement *stats_vq_elem;
    size_t stats_vq_offset;
     
    // 定时器, 定时查询功能
    QEMUTimer *stats_timer;
    int64_t stats_last_update;
    int64_t stats_poll_interval;
     
    // features
    uint32_t host_features;
    // for adjustmem, reserved guest free memory
    uint64_t res_size;
} VirtIOBalloon;
```

* num_pages字段，是balloon中表示我们希望guest归还给host的内存大小；

- actual字段，表示balloon实际捕获的pages数目。guest处理configuration change中断，完成之后调用virtio_cwrite函数。因为写balloon设备的配置空间，所以陷出，qemu收到后会找到balloon设备，修改config时，更新balloon->actual字段；

- stats_last_update在每次从status virtioqueue中取出数据时更新；

### 消息通讯控制virtqueue

```c
struct VirtQueue
{
    VRing vring;
 
    /* Next head to pop */
    uint16_t last_avail_idx;
 
    /* Last avail_idx read from VQ. */
    uint16_t shadow_avail_idx;
 
    uint16_t used_idx;
 
    /* Last used index value we have signalled on */
    uint16_t signalled_used;
 
    /* Last used index value we have signalled on */
    bool signalled_used_valid;
 
    /* Notification enabled? */
    bool notification;
 
    uint16_t queue_index;
    //队列中正在处理的请求的数目
    unsigned int inuse;
 
    uint16_t vector;
    //回调函数
    VirtIOHandleOutput handle_output;
    VirtIOHandleAIOOutput handle_aio_output;
    VirtIODevice *vdev;
    EventNotifier guest_notifier;
    EventNotifier host_notifier;
    QLIST_ENTRY(VirtQueue) node;
};
```

## 3.3 初始化流程

### 设备类型注册

```c
type_init(virtio_register_types)
    type_register_static(&virtio_balloon_info);
        ->instance_init = virtio_balloon_instance_init,
        ->class_init = virtio_balloon_class_init,
```

### 类及实例初始化

```c
qemu_opts_foreach(qemu_find_opts("device"), device_init_func, NULL, NULL)  //vl.c
  qdev_device_add  //qdev-monitor.c
    object_new()                
       ->class_init
       ->instance_init
    object_property_set_bool(realized)  --> virtio_balloon_device_realize  //virtio-balloon.c
       ->virtio_init
       ->virtio_add_queue
```

### balloon设备实例化

`virtio_balloon_device_realize` 实例化函数主要执行两个函数完成实例化操作，首先**调用 `virtio_init` 初始化virtio设备的公共部分。**`virtio_init` 的主要工作是初始化所有virtio设备的基类 TYPE_VIRTIO_DEVICE("virtio-device") 的实例 VirtIODevice 结构体。实例化代码简化实现如下：

```c
static void virtio_balloon_device_realize(DeviceState *dev, Error **errp)
{
    virtio_init(vdev, "virtio-balloon", VIRTIO_ID_BALLOON,
                sizeof(struct virtio_balloon_config));
 
    ret = qemu_add_balloon_handler(virtio_balloon_to_target,
                                   virtio_balloon_stat,
                                   virtio_balloon_adjustmem,
                                   virtio_balloon_get_stats, s);
 
	//...
 
    s->ivq = virtio_add_queue(vdev, 128, virtio_balloon_handle_output);
    s->dvq = virtio_add_queue(vdev, 128, virtio_balloon_handle_output);
    s->svq = virtio_add_queue(vdev, 128, virtio_balloon_receive_stats);
 
    reset_stats(s);
}
```

`virio_init` 的代码流程和基本成员注释如下：

```c
void virtio_init(VirtIODevice *vdev, const char *name,
                 uint16_t device_id, size_t config_size)
{
    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
    int i;
    int nvectors = k->query_nvectors ? k->query_nvectors(qbus->parent) : 0;
 
    if (nvectors) {
        //vector_queues与 MSI中断相关
        vdev->vector_queues =
            g_malloc0(sizeof(*vdev->vector_queues) * nvectors);
    }
 
    vdev->device_id = device_id;
    vdev->status = 0;
    atomic_set(&vdev->isr, 0);  //中断请求
    vdev->queue_sel = 0;    //配置队列的时候选择队列
    //config_vector与MSI中断相关
    vdev->config_vector = VIRTIO_NO_VECTOR;
    //vq分配了1024个virtQueue并进行初始化
    vdev->vq = g_malloc0(sizeof(VirtQueue) * VIRTIO_QUEUE_MAX);
    vdev->vm_running = runstate_is_running();
    vdev->broken = false;
    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
        vdev->vq[i].vector = VIRTIO_NO_VECTOR;
        vdev->vq[i].vdev = vdev;
        vdev->vq[i].queue_index = i;
    }
 
    vdev->name = name;
    //config_len表示配置空间的长度
    vdev->config_len = config_size;
    if (vdev->config_len) {
        //config表示配置数据的存放区域
        vdev->config = g_malloc0(config_size);
    } else {
        vdev->config = NULL;
    }
    vdev->vmstate = qemu_add_vm_change_state_handler(virtio_vmstate_change,
                                                     vdev);
    vdev->device_endian = virtio_default_endian();
    //use_guest_notifier_mask与irqfd有关
    vdev->use_guest_notifier_mask = true;
}
```

`virtio_init` 主要操作为：

* 设置中断
* 申请virtqueue空间
* 申请配置数据空间

初始化操作完成后，realize函数继续调用 `virtio_add_queue` 创建了3个virtqueue ( `ivq、dvq、svq` ) 并将回调函数`virtio_balloon_handle_output` 挂接到virtqueue的handle_output，用于处理virtqueue中的数据，handle_output函数处理在消息通信一节再分析。 

## 3.4 balloon处理 

### 回调函数处理流程

前面提到过realize函数注册了3个virtqueue的回调函数，先分析 inflate 和 deflate (ivq/dvq) 涉及的函数，查询状态信息 (svq) 的函数稍后分析。ivq/dvq 注册的handle_output为 `virtio_balloon_handle_output`，当qemu侧通过virtqueue进行通知的时候会调用handle_out对数据进行处理。

```c
static void virtio_balloon_handle_output(VirtIODevice *vdev, VirtQueue *vq)
{
    VirtIOBalloon *s = VIRTIO_BALLOON(vdev);
    VirtQueueElement *elem;
    MemoryRegionSection section;
 
    for (;;) {
        size_t offset = 0;
        uint32_t pfn;
        //获取virtqueue中的数据到qemu侧virt-ring通用的数据结构
        //handle_out函数通用操作
        elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
        if (!elem) {
            if (hax_enabled() && vq == s->dvq) {
                hax_issue_invept();
            }
            return;
        }
 
        while (iov_to_buf(elem->out_sg, elem->out_num, offset, &pfn, 4) == 4) {
            ram_addr_t pa;
            ram_addr_t addr;
            int p = virtio_ldl_p(vdev, &pfn);
            //将页框转换成GPA
            pa = (ram_addr_t) p << VIRTIO_BALLOON_PFN_SHIFT;
            offset += 4;
 
            //根据pa找到对应的MemoryRegionSection
            section = memory_region_find(get_system_memory(), pa, 1);
            if (!int128_nz(section.size) ||
                !memory_region_is_ram(section.mr) ||
                memory_region_is_rom(section.mr) ||
                memory_region_is_romd(section.mr)) {
                trace_virtio_balloon_bad_addr(pa);
                memory_region_unref(section.mr);
                continue;
            }
 
            trace_virtio_balloon_handle_output(memory_region_name(section.mr),
                                               pa);
            /* Using memory_region_get_ram_ptr is bending the rules a bit, but
               should be OK because we only want a single page.  */
            addr = section.offset_within_region;
            //根据section获取对应的HVA，然后调用balloon函数处理对应页面
            balloon_page(memory_region_get_ram_ptr(section.mr) + addr, pa,
                         !!(vq == s->dvq));
            memory_region_unref(section.mr);
        }
 
        //处理完后通知geust，此处为handle_out通用操作
        virtqueue_push(vq, elem, offset);
        virtio_notify(vdev, vq);
        g_free(elem);
    }
}
```

handle_output函数使用virtqueue_pop取出virtqueue中对应的数据到VirtQueueElement结构体中，在经过地址转换后得到了HVA地址，然后将HVA和队列信息（dvq/ivq）传入balloon_page进行qemu侧的balloon处理。

### qemu处理队列分类

`balloon_page` 函数根据deflate参数判断此次操作时inflate还是deflate，分如下操作：

1. 如果使deflate操作，直接返回。因为deflate操作表示gust会再次使用对应的页面地址，主要是gust内部取消掉这部分页面不可用的标志，QEMU侧因为提供给gust的虚拟地址空间一直是保留状态所以无需特殊处理；

     2. 如果使inflate操作，表示对应的页面将不会再提供给gust使用，所以此时先取消对应的ept映射再对QEMU侧的HVA地址使用qemu_madvise进行处理。

```c
static void balloon_page(void *addr, ram_addr_t gpa, int deflate)
{
    if (!qemu_balloon_is_inhibited() && (!kvm_enabled() ||
                                         kvm_has_sync_mmu())) {
#ifdef _WIN32
        if (!hax_enabled() || !hax_ept_set_supported()) {
            return;
        }
        // For deflation, ept entry can be rebuilt via VMX EPT VIOLATION.
        if (deflate || hax_invalid_ept_entries(gpa, BALLOON_PAGE_SIZE)) {
            return;
        }
#endif
        qemu_madvise(addr, BALLOON_PAGE_SIZE,
                deflate ? QEMU_MADV_WILLNEED : QEMU_MADV_DONTNEED);
    }
}
```

### qemu处理虚拟内存

//TODO
