---
title: 深入浅出系统虚拟化_实验复现

date: 2023-11-27 17:00:00 +0800

categories: [读书笔记, 深入浅出系统虚拟化]

tags: [virt, qemu, kvm]

description: 
---

# 0 环境说明

书中实验对过程描述很详细，但基本没提及环境配置的问题。书中用的内核以及qemu版本过低，我均替换成比较新的，为避免后续实验出现兼容性问题，还是要做到以下：

* 宿主机和客户机均使用相同的内核，我使用的环境为：
  * ubuntu 镜像：[ubuntu-releases-22.04.3安装包下载_开源镜像站-阿里云 (aliyun.com)](https://mirrors.aliyun.com/ubuntu-releases/22.04.3/)
  * `OS`：Ubuntu 22.04.3 LTS x86_64
  * `Kernel`： 6.2.0-39-generic

---

## //Failed 0.1 QEMU/KVM搭建X86_64/ubuntu虚拟机

首先需要你的系统环境支持KVM，且硬件支持VT-x。

### qemu安装

首先安装依赖：

```shell
apt-get update
# 下载依赖项
apt-get install git libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev
apt-get install libaio-dev libbluetooth-dev libbrlapi-dev libbz2-dev
apt-get install libnfs-dev libiscsi-dev
# 实际编译时还需要这个依赖
sudo apt-get install ninja-build
```

下载指定版本的qemu源码：[blfs-conglomeration-qemu安装包下载_开源镜像站-阿里云 (aliyun.com)](https://mirrors.aliyun.com/blfs/conglomeration/qemu/)

解压并编译qemu：

```shell
wget https://mirrors.aliyun.com/blfs/conglomeration/qemu/qemu-4.1.1.tar.xz
xz -d qemu-5.1.0.tar.xz
tar xvf qemu-5.1.0.tar
cd ./qemu-4.1.1
mkdir -p bin/debug/native
cd bin/debug/native
../../../configure --enable-kvm --enable-debug --enable-vnc --target-list="x86_64-softmmu"
make -j8
sudo make install
# 通过指令查看编译是否成功
qemu-system-x86_64 -M help
```

### 安装ubuntu

下载指定版本的ubuntu镜像：[Index of /releases/16.04.0 (ubuntu.com)](http://old-releases.ubuntu.com/releases/16.04.0/)

```shell
wget http://old-releases.ubuntu.com/releases/16.04.0/ubuntu-16.04-server-amd64.iso
```

创建虚拟磁盘。利用 `qemu-img` 指令可以创建1个空的虚拟硬盘，便于后面安装的时候将系统安装到虚拟硬盘上。

进入到工作空间 `qemu-4.1.1/bin/debug/native` 目录下，使用以下命令创建虚拟硬盘：

```shell
qemu-img create ubuntu.img 10G	
```

在当前目录下会生成 `ubuntu.img` 这样一个文件。

---

接下来，安装虚拟机：

将上面的镜像文件 `ubuntu-16.04-server-amd64.iso` 放 在 `qemu-4.1.1/bin/debug/native` 该目录下，然后使用以下命令安装：

```shell
qemu-system-x86_64 -m 2048 -smp 2 --enable-kvm ubuntu.img -cdrom ubuntu-16.04-server-amd64.iso
```

>-m 2048 —— 给客户机分配2G内存(也可以输入“2G”)；
>-smp 2 —— 指定客户机为对称多处理器结构并分配2个CPU；
>–enable-kvm —— 允许kvm（速度快很多）
>-cdrom * —— 分配客户机的光驱
>
>ps:输入上述命令会直接进入安装界面，如果命令行卡在那里（没有报错），大概率是没有正常安装，检查命令中的参数是否正确，**（如无特殊需求，建议只用这几个基本参数就好）**然后重新执行安装命令。

[Ubuntu 无图形界面安装 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/578074736)

---

再次打开创建的虚拟机，启动虚拟机输入命令如下：（不再加载iso镜像）

```shell
qemu-system-x86_64 -m 2048 -smp 2 --enable-kvm ubuntu.img 
```

第一次用此命令开启虚拟机可能会有点慢，需要耐心等待。至此通过QEMU工具安装无界面Ubuntu x86_64虚拟系统完成。



## 0.2 编译x86_64 Linux内核并基于QEMU运行

> `Host OS`：Ubuntu 22.04.3 LTS x86_64 6.2.0-39-generic
>
> `Guest OS`：linux-6.2.1
>
> `QEMU`：QEMU emulator version 8.0.0

[编译x86_64 Linux内核并基于QEMU运行 - Legend_Lone - 博客园 (cnblogs.com)](https://www.cnblogs.com/sun-ye/p/14983558.html)

[Linux内核学习-CSDN博客](https://blog.csdn.net/Liuqz2009/article/details/130289159)

准备好qemu环境和GDB工具后，我们需要额外提供给它们一些资源：

* 内核镜像
* 根文件系统

---

### x86内核编译

下载Linux内核源码：[Index of /sites/ftp.kernel.org/pub/linux/kernel/v6.x/ (sjtu.edu.cn)](http://ftp.sjtu.edu.cn/sites/ftp.kernel.org/pub/linux/kernel/v6.x/)

```shell
wget http://ftp.sjtu.edu.cn/sites/ftp.kernel.org/pub/linux/kernel/v6.x/linux-6.2.1.tar.xz
tar xvJf linux-6.2.1.tar.xz
```

接着，创建编译目录并配置处理器架构和交叉编译器等环境变量：

```shell
export ARCH=x86_64
mkdir build-x86_64
```

配置并编译内核：

```shell
make -C linux-6.2.1 O=`pwd`/build-x86_64 x86_64_defconfig
make -C linux-6.2.1 O=`pwd`/build-x86_64 menuconfig# 打开 `Kernel hacking -> Compile-time checks and compiler options -> Compile the kernel with debug info -> Provide GDB scripts for kernel debugging`
make -C linux-6.2.1 O=`pwd`/build-x86_64 -j`nproc`
```

> 编译过程中，出现的错误：
>
> * `gelf.h: No such file or directory`
>
>   ```shell
>   sudo apt-get install libelf-dev
>   ```

编译完成后，得到内核镜像文件 `/home/zq/objs/lab/openEuler_virt_lab/build-x86_64/arch/x86_64/boot/bzImage`。

---

### 根文件系统构建

下载 buildroot 并配置编译：

```shell
git clone https://github.com/buildroot/buildroot.git
cd buildroot
make menuconfig
# select `Target Options -> Target Architecture -> x86_64`
# select `Filesystem images -> ext2/3/4 root file system -> ext4`
$ make -j`nproc`
```

---

### gdb调试基于qemu运行的内核

```shell
#!/bin/bash

sudo qemu-system-riscv64 \
	-M virt \
	-m 2G \
	-kernel Image \
	-append "rootwait root=/dev/vda ro" \
	-drive file=rootfs.ext2,format=raw,id=hd0 \
	-device virtio-blk-device,drive=hd0 \
	-nographic \
	-virtfs local,path=/home/zq/objs/lab/openEuler_virt_lab/shared,mount_tag=host0,security_model=passthrough,id=host0 \
	-netdev user,id=net0 -device virtio-net-device,netdev=net0 \
	-s -S
```

执行上述命令启动QEMU后，root账号登录Linux系统，然后执行 `mount` 命令挂载宿主机目录，用于文件共享：

```shell
mkdir -p /mnt/shared
mount -t 9p -o trans=virtio,version=9p2000.L host0 /mnt/shared
```



## 0.3 遇到的问题

* ubuntu 内核版本变更：
  * [ubuntu 内核降级，切换linux内核版本_4.15.0-39-generic-CSDN博客](https://blog.csdn.net/u010608421/article/details/115658479)
  * [超级用户指南：轻松升级你的Ubuntu Linux内核版本_ubuntu升级内核-CSDN博客](https://blog.csdn.net/Long_xu/article/details/126710992?ops_request_misc=%7B%22request%5Fid%22%3A%22170194869216800226534730%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=170194869216800226534730&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-126710992-null-null.142^v96^pc_search_result_base3&utm_term=ubuntu升级内核版本&spm=1018.2226.3001.4187)



# 1 Lab1: CPU虚拟化实例

QEMU主要负责维护虚拟机和vCPU模型，并通过KVM模块文件描述符、虚拟机文件描述符和vCPU文件描述符调用KVM API接口，本实验实现了一个类似于QEMU的小程序，执行指定二进制代码输出“Hello，World！”。代码如下：

```c
#include <err.h>
#include <fcntl.h>
#include <linux/kvm.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <sys/types.h>

int main(void)
{
    int kvm, vmfd, vcpufd, ret;
    const uint8_t code[] = {
        0xba, 0xf8, 0x03, /* mov $0x3f8, %dx */
        0xb0, 'H',       /* mov $'H', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, 'e',       /* mov $'e', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, 'l',       /* mov $'l', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, 'l',       /* mov $'l', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, 'o',       /* mov $'o', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, ',',       /* mov $',', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, ' ',       /* mov $' ', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, 'w',       /* mov $'w', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, 'o',       /* mov $'o', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, 'r',       /* mov $'r', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, 'l',       /* mov $'l', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, 'd',       /* mov $'d', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, '!',       /* mov $'!', %al */
        0xee,             /* out %al, (%dx) */
        0xb0, '\n',       /* mov $'\n', %al */
        0xee,             /* out %al, (%dx) */
        0xf4,             /* hlt */
    };
    uint8_t *mem;
    struct kvm_sregs sregs;
    size_t mmap_size;
    struct kvm_run *run;

    kvm = open("/dev/kvm", O_RDWR | O_CLOEXEC);
    if (kvm == -1)
        err(1, "/dev/kvm");

    /* Make sure we have the stable version of the API */
    ret = ioctl(kvm, KVM_GET_API_VERSION, NULL);
    if (ret == -1)
        err(1, "KVM_GET_API_VERSION");
    if (ret != 12)
        errx(1, "KVM_GET_API_VERSION %d, expected 12", ret);

    vmfd = ioctl(kvm, KVM_CREATE_VM, (unsigned long)0);
    if (vmfd == -1)
        err(1, "KVM_CREATE_VM");

    /* Allocate one aligned page of guest memory to hold the code. */
    mem = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);
    if (!mem)
        err(1, "allocating guest memory");
    memcpy(mem, code, sizeof(code));

    /* Map it to the second page frame (to avoid the real-mode IDT at 0). */
    struct kvm_userspace_memory_region region = {
        .slot = 0,
        .guest_phys_addr = 0x1000,
        .memory_size = 0x1000,
        .userspace_addr = (uint64_t)mem,
    };
    ret = ioctl(vmfd, KVM_SET_USER_MEMORY_REGION, &region);
    if (ret == -1)
        err(1, "KVM_SET_USER_MEMORY_REGION");

    vcpufd = ioctl(vmfd, KVM_CREATE_VCPU, (unsigned long)0);
    if (vcpufd == -1)
        err(1, "KVM_CREATE_VCPU");

    /* Map the shared kvm_run structure and following data. */
    ret = ioctl(kvm, KVM_GET_VCPU_MMAP_SIZE, NULL);
    if (ret == -1)
        err(1, "KVM_GET_VCPU_MMAP_SIZE");
    mmap_size = ret;
    if (mmap_size < sizeof(*run))
        errx(1, "KVM_GET_VCPU_MMAP_SIZE unexpectedly small");
    run = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED, vcpufd, 0);
    if (!run)
        err(1, "mmap vcpu");

    /* Initialize CS to point at 0, via a read-modify-write of sregs. */
    ret = ioctl(vcpufd, KVM_GET_SREGS, &sregs);
    if (ret == -1)
        err(1, "KVM_GET_SREGS");
    sregs.cs.base = 0;
    sregs.cs.selector = 0;
    ret = ioctl(vcpufd, KVM_SET_SREGS, &sregs);
    if (ret == -1)
        err(1, "KVM_SET_SREGS");

    /* Initialize registers: instruction pointer for our code, addends, and
     * initial flags required by x86 architecture. */
    struct kvm_regs regs = {
        .rip = 0x1000,
        .rax = 2,
        .rbx = 2,
        .rflags = 0x2,
    };
    ret = ioctl(vcpufd, KVM_SET_REGS, &regs);
    if (ret == -1)
        err(1, "KVM_SET_REGS");

    /* Repeatedly run code and handle VM exits. */
    while (1) {
        ret = ioctl(vcpufd, KVM_RUN, NULL);
        if (ret == -1)
            err(1, "KVM_RUN");
        switch (run->exit_reason) {
        case KVM_EXIT_HLT:
            puts("KVM_EXIT_HLT");
            return 0;
        case KVM_EXIT_IO:
            if (run->io.direction == KVM_EXIT_IO_OUT && run->io.size == 1 && run->io.port == 0x3f8 && run->io.count == 1)
                putchar(*(((char *)run) + run->io.data_offset));
            else
                errx(1, "unhandled KVM_EXIT_IO");
            break;
        case KVM_EXIT_FAIL_ENTRY:
            errx(1, "KVM_EXIT_FAIL_ENTRY: hardware_entry_failure_reason = 0x%llx",
                 (unsigned long long)run->fail_entry.hardware_entry_failure_reason);
        case KVM_EXIT_INTERNAL_ERROR:
            errx(1, "KVM_EXIT_INTERNAL_ERROR: suberror = 0x%x", run->internal.suberror);
        default:
            errx(1, "exit_reason = 0x%x", run->exit_reason);
        }
    }
}
```

上述代码流程为：

* 通过KVM的若干ioctl调用创建并运行vCPU，将vCPU起始指令地址设置为 `0x1000(GPA)`，并将指定的二进制代码复制到相应的内存位置(HVA)，切入虚拟机后首先执行这些指令；
* 二进制代码将依次调用OUT指令向0x3f8端口写入 `Hello，World!` 包含的各个字符，触发EXIT_REASON_IO_INSTRUCTION类型的VM-Exit，这使得程序退回到用户态进行处理；
* 用户态程序调用 `putchar` 函数输出对应字符；
* 二进制代码最终执行hlt指令触发VM-Exit，系统回到用户态并退出应用程序。

结果如下：

![image-20231128100349907](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311281003930.png)



# 2 Lab2: e1000网卡中断虚拟化

前几节介绍了当中断芯片全部由KVM模拟时，外部设备中断传送的完整流程。本节将以e1000网卡为例，通过GDB调试工具回顾前述流程。

## 2.1 实验概述

为了使用GDB调试虚拟中断在KVM模块中的传送流程，本次实验需要在嵌套虚拟化的环境下进行，物理机和虚拟机使用的内核版本均为4.19.0。本节使用前述QEMU提供的 `-s` 和 `-S` 选项启动一个虚拟机。启动命令如下：

`Physical Machine Terminal 1`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231719972.png" alt="image-20231123171810813" style="zoom:33%;" />

```shell
qemu-system-x86_64 -s -S -smp 4 -m 4096 -cpu host \
	-kernel linux/arch/x86/boot/bzImage -initrd initrd.img-4.19.0 \
	-append "root=/dev/sda1 nokaslr" \
	-drive file=desktop.img,format=raw,index=0,media=disk \
	-netdev tap,id=mynet0,ifname=tap0,script=no,downscript=no \
	-device e1000,netdev=mynet0,mac=52:55:00:d1:55:01 --enable-kvm
```

> `qemu-system-x86_64` 是一个用于运行虚拟机的 QEMU 命令行工具。你提到的命令使用了 `-cdrom` 参数来指定虚拟机启动时要加载的 ISO 镜像，但没有使用 `-kernel` 和 `-initrd` 参数。`-kernel` 和 `-initrd` 参数通常用于指定启动内核和初始 RAM 文件系统（initramfs）的位置。这两个参数通常与裸金属虚拟机（Bare Metal Virtualization）相关，其中虚拟机启动时加载自定义的内核和 RAM 文件系统，而不是从预先创建的虚拟硬盘中引导。
>
> 在你提到的命令中，通过 `-cdrom` 参数指定了一个 ISO 镜像，这通常意味着虚拟机将从光盘启动。在这种情况下，虚拟机会忽略 `-kernel` 和 `-initrd` 参数，因为它会使用 ISO 镜像中包含的内核和 initramfs。
>
> 如果你想自定义虚拟机的内核和 initramfs，你可以使用 `-kernel` 和 `-initrd` 参数，将相应的文件路径传递给它们。这在一些高级的虚拟化场景中可能会有用，特别是当你需要自定义 Linux 内核和启动过程时。

在终端2启动GDB加载Linux内核调试信息并连接至1234端口，然后开始运行虚拟机。运行命令如下：

`Physical Machine Terminal 2`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231719309.png" alt="image-20231123171859047" style="zoom:33%;" />

为了加载KVM模块的调试信息，读者需要在虚拟机中通过 `/sys/module/module_name/sections` 查看 `kvm.ko` 和 `kvm-intel.ko` 模块所在的GPA，并在GDB中手动引入KVM模块的调试信息。运行命令如下。

`Virtual Machine Terminal 1`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231719366.png" alt="image-20231123171941476" style="zoom:33%;" />

`Physical Machine Terminal 2`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231720645.png" alt="image-20231123172009276" style="zoom:33%;" />

接着**在虚拟机中启动嵌套虚拟机并运行，**`-monitor` 选项指定了QEMU监视器 (QEMU monitor) 的运行端口为 `4444`。读者可以另外启动一个终端连接至QEMU监视器。QEMU监视器提供了各种命令用于查看虚拟机的当前状态。这里可以通过 `info qtree` 查看当前虚拟机的架构。可以发现虚拟机使用的中断控制器为APIC。虚拟IOAPIC直接挂载在 `main-system-bus` 上，而e1000网卡挂载名为 `pic.0` 的PCI总线上。启动命令如下：

`Virtual Machine Terminal 2`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231721921.png" alt="image-20231123172038266" style="zoom:33%;" />

`Virtual Machine Terminal 3`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231721711.png" alt="image-20231123172104898" style="zoom:33%;" />

---

在嵌套虚拟机中启动一个终端并执行 `lspci-v` 指令，可以查看当前虚拟机中的PCI设备，e1000网卡具体信息如下。

`Nested Virtual Machine Terminal 1`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231722716.png" alt="image-20231123172147932" style="zoom:33%;" />

上述信息表明e1000网卡的BDF为 `00:03.0`，即24，而e1000网卡使用的中断IRQ号为11，介绍中断传递时提到在QEMU/KVM中GSI与IRQ等价，除了0号中断引脚外，其余IOAPIC引脚与GSI满足 `GSI = GSI_base + Pin` 映射关系，故e1000网卡对应的中断引脚号为11。然后使用QEMU监视器输入 `info pic` 查看虚拟机IOAPIC信息。

`Virtual Machine Terminal 3`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231722226.png" alt="image-20231123172231931" style="zoom:33%;" />

以上输出表明虚拟IOAPIC的11号中断引脚对应的中断向量号为38，即e1000网卡使用 `38` 号中断向量。下面将通过GDB查看e1000网卡中断传送过程中的关键函数调用以及中断信息。

## 2.2 e1000网卡中断传送流程

网卡在收发包时都会产生中断，而对于e1000网卡，无论是收包中断还是发包中断，最后都会调用 `set_interrupt_cause` 函数。读者可以通过前述 `Virtual Machine Terminal 2` 中运行的GDB在 `set_interrupt_cause` 函数处设置断点并继续运行该程序。当触发该断点后，打印出e1000网卡的设备号为24，与 `lspci-v` 指令结果相符。

`Virtual Machine Terminal 2`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231725989.png" alt="image-20231123172415478" style="zoom:33%;" />

`set_interrupt_cause` 函数最终会调用PCI设备共用的中断接口 `pci_set_irq` 函数触发中断，为了区别e1000网卡与其他PCI设备，我们可以使用GDB条件断点，使得只有设备号为24时才命中 `pci_set_irq` 中的断点。终端输出表明e1000网卡使用的中断引脚号（`intx` 变量）为0，即e1000网卡使用 `INTA#` 中断引脚。

`Virtual Machine Terminal 2`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231725685.png" alt="image-20231123172450445" style="zoom:33%;" />

如前所述，`pci_irq_handler` 函数最终会调用 `pci_change_irq_level` 函数获得PCI设备中断引脚所连接的PCI总线中断请求线。`pci_change_irq_level` 函数通过调用所属总线的 `map_irq` 回调函数 `pci_slot_get_irq` 完成上述转换。在该行设置断点并打印出对应的PCI链接设备号（对应 `irq_num`）为2，故e1000网卡的 `INTA#` 中断引脚连接至 `LNKC` 中断请求线。

`Virtual Machine Terminal 2`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231726018.png" alt="image-20231123172606710" style="zoom:33%;" />

而 `pci_change_irq_level` 函数将会调用总线成员的set_irq回调函数 `piix3_set_irq`，进而调用 `piix3_set_irq_level` 函数。该函数通过PIIX3设备PCI配置空间中的 `PIRQRC[A:D]` 寄存器**获取PCI总线某条中断请求线对应的IOAPIC IRQ线。**在该函数中断点打印e1000网卡对应的IRQ线(`pci_irq`)，其值为11。

`Virtual Machine Terminal 2`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231727713.png" alt="image-20231123172659958" style="zoom:33%;" />

`piix3_set_irq_level` 函数获得PCI设备对应的IRQ线后会调用 `piix3_set_irq_pic` 函数，该函数进而调用 `qemu_set_irq` 函数经由4.4节介绍的QEMU中断路由过程后，最终调用 `kvm_set_irq` 函数将中断信号传递至KVM模拟的中断芯片。调用GDB `backtrace` 命令打印出当前函数调用栈帧，与QEMU中断路由流程相符。如下：

`Virtual Machine Terminal 2`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231727828.png" alt="image-20231123172742648" style="zoom:33%;" />

---

**中断信号传入KVM后，**经由4.4节介绍的内核中断路由表，将中断信号发送至所有可能的虚拟中断控制器。对于本实验来说，虚拟机使用的中断控制器为IOAPIC，对应的回调函数为 `kvm_set_ioapic_irq`，该函数将调用 `kvm_ioapic_set_irq` 函数处理指定中断引脚传来的中断信号。通过GDB在该函数处设置断点，可以发现传入 `kvm_ioapic_set_irq` 函数的中断引脚号为11，即e1000中断对应的中断引脚号为11。

`Physical Machine Terminal 1`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231728332.png" alt="image-20231123172820241" style="zoom:33%;" />

如前所述，`kvm_ioapic_set_irq` 函数最终调用 `ioapic_service` 函数处理指定引脚的中断请求。`ioapic_service` 函数根据传入的中断引脚号在 `PRT(ioapic->redirtbl)` 中找到对应的RTE并格式化出一条中断消息 `irqe` 发送给所有的目标LAPIC。

`irqe` 中包含供CPU最终使用的中断向量号。在 `ioapic_service` 函数中设置断点打印中断消息 `irqe`，可以发现e1000网卡对应的中断向量号为38，触发方式为水平触发 (trig_mode=1)，与通过QEMU监视器执行 `info pic` 命令得到的信息完全一致。

`Physical Machine Terminal 1`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311231729013.png" alt="image-20231123172852451" style="zoom:33%;" />

后续流程为：

1. 虚拟LAPIC收到中断消息后，将设置 `IRR` 并调用 `vcpu_kick` 函数通知vCPU；
2. 当vCPU再次调用 `vcpu_enter_guest` 函数准备进入非根模式时，发现当前有待注入的虚拟中断。最终 `vcpu_enter_guest` 函数会调用 `vmx_inject_irq` 函数设置VMCS VM-Entry控制域中的中断信息字段；
3. 当虚拟机恢复运行时，CPU会检查该字段发现有待处理的e1000网卡中断，则CPU将执行相应的中断服务例程。

至此，e1000网卡产生的虚拟中断处理流程完成。

---

本节通过GDB展示了e1000网卡虚拟中断处理流程，着重展示了**PCI设备中断引脚号与IRQ号的映射**以及**IRQ号与中断向量号的映射**关系。



# 3 Lab3: 打印MemoryRegion树

QEMU为了模拟MMIO以及物理设备的行为，形成了一套复杂的数据结构，但这些只是静态的代码。本节将QEMU代码运行起来，在动态过程中打印出MemoryRegion树，更形象地展示数据结构之间的关系。

实验使用从源代码编译的QEMU v4.1.1，以及事先准备好的客户机磁盘镜像作为QEMU的-hda参数传递给QEMU。首先，使用如下命令进入QEMU监视器：

![image-20231120094505324](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311200945365.png)

> 启动命令的含义为：将QEMU管理器的输入输出重定向到字符设备stdio(-monitor stdio)，即此处的命令行。

此命令启动了2个vCPU `-smp 2`，使用NUMA（Non-Uniform Memory Access，非统一内存访问）架构，分为两个NUMA节点 `-numa node`，分配 4 GB的“虚拟”物理内存 `-m 4096` ；开启KVM支持，并使用与宿主机一样的CPU型号 `-cpu host--enable-kvm`。

接下来，使用命令 `info mtree` 打印此客户机的MemoryRegion树，在输出中，QEMU用不同宽度的缩进表示不同树的深度，打印如下：

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311200950437.png" alt="image-20231120094846932" style="zoom: 50%;" />

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311200951306.png" alt="image-20231120094910362" style="zoom: 50%;" />

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311201012555.png" alt="image-20231120095304414" style="zoom:50%;" />

此处省略了被标为 `[disabled]` 的MR，以及一些陌生的MR。可以看到，整个虚拟机有address_space_memory作为物理内存空间的AS，有address_space_io作为PIO端口映射空间的AS。由于本实验启动了 2 个vCPU，所以这里打印出了两个CPU的AS，即 `cpu-memory-0/1`。其他的AS包括从设备角度可以观察到的AS，如e1000、VGA等设备的AS。

每个AS下显示了AS的MR树，其中非别名类型的MR只能打印出一条较短的记录，包含其地址范围。如address_space_memory的MR树根system_memory，其地址范围是0x0000000000000000~0xffffffffffffffff，即0~UINT64_MAX；而别名MR会被明确标识为alias，并追加上其alias指针指向的原MR。有关 `info mtree` 命令的实现函数，请查阅QEMU源码树memory.c文件的 `mtree_info -> mtree_print_mr` 函数。

为了与源码相对应，继续在QEMU源码中寻找这些AS和MR被创建的位置，具体方法多种多样。一种直接的方法是在源码中搜索相关的创建函数，如 `address_space_init`、`memory_region_init`，更严谨的方法是通过GDB打断点的方式寻找。

首先，在QEMU的main函数中，`cpu_exec_init_all` 函数初始化了主要的AS以及MR树根，代码如下：

qemu-4.1.1/exec.c

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311201012938.png" alt="image-20231120095411196" style="zoom:50%;" />

在这里，QEMU初始化了 `system_memory/system_io` 等静态变量，作为两个全局AS变量 `address_space_memory/address_space_io` 的MR树根。这里初始化了与体系结构无关的AS，下面进入i386的模拟部分中与初始化架构相关的部分。在不同类型的PC_MACHINE的定义函数中，也会初始化AS/MR等数据结构，以 `pc_init1` 函数为例，代码如下：

qemu-4.1.1/hw/i386/pc_piix.c

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311201006383.png" alt="image-20231120100602799" style="zoom: 33%;" />

可以看到，`pc_init1` 函数首先初始化了PCI MR，继续调用 `pc_memory_init` 函数，初始化了真实的全局物理内存pc.ram MR，并将其分为两个别名MR，即 `ram_below_4g/ram_above_4g`，并作为子MR加入了system_memory。

在解析QEMU参数时，QEMU读取到 `-m` 参数后的数字，并将其保存在machine->ram_size中，作为初始化pc.ram MR的大小，即物理内存的大小。在分配全局pc.ram MR时，QEMU将NUMA和非NUMA的情况分类。非NUMA的情况下，直接分配一个RAM类型的实体MR即可；而NUMA情况下，需要调用 `host_memory_backend_get_memory` 函数得到每个NUMA节点对应的MR，并作为子MR加入pc.ram MR中。这与之前info mtree打印出来的MR树相符合。



# 4 Lab4: 将GVA翻译为HPA

#### 实验概述

作为内存虚拟化的总结以及KVM内存虚拟化源码分析的拓展，本节进行GVA到HPA的翻译实验。内存虚拟化的核心是地址翻译，即将某一个地址空间的地址转换为下层地址空间的地址。如前文所述，地址翻译由MMU硬件完成，首先使用客户机进程页表GPT将GVA翻译为GPA，再由扩展页表EPT将GPA翻译为HPA。**由于无法观察硬件的地址翻译过程，于是本节借助内核提供的页表访问接口，通过编写软件模拟MMU的功能。**

为了证明内存翻译代码运行的正确性，首先在GVA处写入一个int类型的变量，并在最后得到的HPA处对该int类型变量进行读取，如果写入的变量和读到的变量值相同，那么证明地址翻译正确。本实验实现的软件MMU分为两部分：

1. **客户机中的地址翻译模块**作为客户机中运行的内核模块，首先在GVA处写入一个int变量 `0xdeadbeef`，再通过读取GPT的方式将GVA翻译成GPA，最后通过超级调用将GPA传递给宿主机操作系统；
2. **宿主机中的KVM内核模块**将截获到该超级调用，得到客户机传来的GPA，通过读取EPT的方式进一步翻译成HPA。为了读取HPA处的变量，还需要使用内核提供的接口做一次HPA到HVA的转化，这是因为分页模式开启，访存指令中的地址均是HVA，无法使用HPA直接访问物理内存。最终读取HVA处的变量，将读取到的值与客户机写入的值进行比较，如果也是 `0xdeadbeef`，则能够证明地址翻译的正确性。

下文分别介绍**客户机的地址翻译与宿主机的地址翻译，**形成一个整体，使读者了解地址翻译的流程，流程如下图所示。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311210946037.png" alt="image-20231121094609424" style="zoom: 50%;" />

> 注：①调用kmalloc函数得到GVA；②查询GPT；③打印GPT相关表项；④查询GPT，得到GPA；⑤通过超级调用将GPA传入KVM，查询EPT；⑥打印EPT相关表项；⑦查询EPT，得到HPA。

首先说明实验的运行环境：宿主机CPU型号是IntelCore i7-6500U，频率2.50GHz，QEMU为客户机提供了与宿主机相同型号的CPU。宿主机物理内存大小为7.5GB，客户机物理内存为4GB。本节宿主机和客户机均使用Linux v4.19.0内核，并使用从源码编译的QEMU v4.1.1。再说明实验准备，读者应该事先制作好一个客户机磁盘镜像，作为QEMU的-hda参数进行读取，使用QEMU启动一个客户机。

实验相关过程与脚本见代码仓库：https://github.com/GiantVM/book 。

#### 客户机中的地址转换: GVA到GPA

为了读取Linux操作系统的页表，需要在内核态编写代码。使用Linux内核模块是编写内核代码的一种简单的方式。整体操作流程如下：

1. 内核模块的源码可以仅由一个.c文件组成，在实验里是gpt-dump.c，只需要编写一个Makefile（编译命令文件）对gpt-dump.c进行编译，得到gpt-dump.ko文件；
2. 再使用命令`sudo insmod gpt-dump.ko` 即可将内核模块插入内核；
3. 内核模块插入内核后，就会调用gpt-dump.c中定义的 `init_module` 函数；而使用命令`sudo insmod gpt-dump.ko` 将内核模块移出内核时，则会调用 `cleanup_module` 函数。在 `init_module` 函数中即可编写内核代码在内核态运行，可以访问页表。

虚拟地址和物理地址在Linux内核中均使用64位的变量表示，而在Intel Core i7系统中，虚拟地址空间为48位，共256TB大小，物理地址空间为52位，共4PB(4096TB)大小。**一个页表项的大小为64位，即8字节，一个页表页有512个页表项，页表页大小为4KB，故需要虚拟地址中的log2(512)=9 位索引每级的页表页。**

> Linux内核使用4级页表，用虚拟地址的第47：39位索引第4级页表，第38：30位索引第3级页表，第29：21位索引第2级页表，第20：12位索引第1级页表。这样就形成了前文所述的 `9+9+9+9` 形式的四级页表。

---

虚拟地址使用第47：12位存储页表的索引，第11：0位存储虚拟地址在页中的偏移，因此查询页表只使用了虚拟地址的前48位，即访问虚拟地址空间仅使用了48位的虚拟地址。此处定义宏 `UL_TO_VADDR以/VADDR_PATTERN` 打印虚拟地址，包含页表的索引位（共36位），以及偏移（共12位）。部分相关宏定义如下。

gpt-dump/gpt-dump.c

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211000229.png" alt="image-20231121100026341" style="zoom: 33%;" />

由于内核中尚没有将变量转化为二进制表示的函数，此处编写 `pr_info` 函数，接收不同的 `%c` 格式化字符串和 `0/1` 字符的组合来打印虚拟地址、物理地址以及页表项。以虚拟地址的打印为例，首先定义输出3个位的字符组合，即 `TBYTE_TO_BINARY`，还有对应的打印三个char类型的格式化字符串 `TBYTE_TO_BINARY_PATTERN`。接下来，需要将虚拟地址的低12位（即偏移的12个位）全部打印出来。继续对代表虚拟地址的ulong（64位的变量）使用 `TBYTE_TO_BINARY`，得到其低3位；再将ulong右移3位，并使用宏 `TBYTE_TO_BINARY`，得到其5：3位，以此类推，可以得到所有形式的 `0/1` 字符组合，包括宏 `UL_TO_PTE_OFFSET` 负责输出12位，`UL_TO_PTE_INDEX` 负责输出9位。对于 `pr_info` 的格式化字符串，实现方式类似，只需在合适的位置加上空格，便于观察。在内核模块初始化函数中，首先调用 `print_ptr_vaddr` 打印虚拟地址，输出如下：

gpt-dump/gpt-dump.txt

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211001803.png" alt="image-20231121100122051" style="zoom:33%;" />

可以看到四级页表的四个索引值，以及页内偏移的二进制表示。

---

为了获得一个GVA，在模块初始化函数中，首先调用 `kmalloc` 函数生成一个int类型变量的指针，由于内核模块运行在客户机内核中，所以该指针包含一个GVA。在上面的代码片段中，客户机内核模块在该指针处写入数字：`0xdeadbeef`，期望在GVA对应的HVA处读到该数字。在输出中可以看到四级页表的索引，得知在页表页中应该读取第几个页表项。接下来，内核模块找到客户机内核线程的 `CR3`，并将其传入页表打印函数。下面是客户机内核模块的相关代码：

gpt-dump/gpt-dump.c

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211002733.png" alt="image-20231121100204419" style="zoom:33%;" />

`current->mm->pgd` 是当前进程current的CR3，指向PGD（Page Global Directory，页全局目录）页表页的起始地址。`dump_pgd` 函数负责遍历pgd页表页的PTRS_PER_PGD个页表项，这里是512个页表项。

> pgd是一个pgd_t类型的变量，内核还提供了类似的pud_t、pmd_t、pte_t数据结构表示每级页表的页表项，以及操作这些数据结构的接口。

此处使用这些接口获取页表项的含义，如 `pgd_val` 函数返回该页表项的值，即该页表项对应的unsigned long变量；`pgd_present` 函数检查该页表项的第0位，返回该页表项是否有效；`pgd_large` 函数返回该页表项是否指向1GB的大页。

本节忽略大页的情况，我们可以使用内核参数关闭大页。这里定义了全局变量保存的虚拟地址和对应的物理地址，以及各级页表索引。`pgd_idx`表示从64位虚拟地址中获得的PGD页表索引。接下来，如果PGD页表页的第 `pgd_idx` 个页表项存在，那么调用 `pr_pte` 函数打印该页表项，此函数定义如下：

gpt-dump/gpt-dump.c

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211002339.png" alt="image-20231121100249251" style="zoom:33%;" />

参数 `address` 表示本级页表页的起始地址，从上一级页表项获得，`pte` 表示本级页表项。宏PTE_PATTREN用于打印一个页表项。继续查询下一级页表的函数调用链为 `dump_pgd -> dump_pud -> dump_pmd -> dump_pte`，其中每一步的逻辑大致相同。address和pte的打印结果如下。

gpt-dump/gpt-dump.txt

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211005105.png" alt="image-20231121100329761" style="zoom:33%;" />

从加粗的部分可以看到，客户机内核模块对页表页的地址调用 `_pa` 函数获得其物理地址，和上一级页表项中保存的下一级页表页的物理地址完全相同，这符合预期。最终，就可以从PTE中获得物理地址，为了验证正确性，客户机内核模块对vaddr调用 `__pa` 函数，打印出GPA开头的行，具体在 `print_pa_check` 函数中实现，代码如下。

gpt-dump/gpt-dump.c

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211004899.png" alt="image-20231121100450101" style="zoom:33%;" />

可以看到，和之前获得的PTE中的物理地址相同，说明读取页表的结果正确。其中低12位是页内偏移，物理地址和虚拟地址中的页内偏移完全相同。**最后，客户机内核模块将调用 `kvm_hypercall1(22,paddr)` 函数，将paddr传给KVM。**

#### KVM中的地址转换: GPA到HPA

> KVM负责维护EPT，具有读取EPT的权限。本实验修改宿主机内核的KVM模块，**增加一个超级调用处理函数，**接收从客户机传来的GPA，模拟GPA到HPA的翻译。

当客户机执行了一个敏感非特权指令时，会引起CPU的VM-Exit，CPU的执行模式从非根模式转换为根模式，并进入KVM的VM-Exit处理函数。`kvm_hypercall1` 函数最终执行vmcall指令，陷入KVM，KVM得知VM-Exit的原因是客户机执行了vmcall指令，编号为EXIT_REASON_VMCALL，于是调用如下 `handle_vmcall` 处理函数。

linux-4.19.0/arch/x86/kvm/vmx.c

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211110467.png" alt="image-20231121111004958" style="zoom:33%;" />

函数 `handle_vmcall` 调用超级调用模拟函数 `kvm_emulate_hypercall`，代码如下。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211110116.png" alt="image-20231121111035833" style="zoom:33%;" />

其中，nr是 `kvm_hypercall1` 函数的第一个参数，a0是第二个参数。nr表示应该调用哪个超级调用模拟函数，定义KVM_HC_DUMP_SPT为22，表示打印客户机内核线程页表对应的EPT。首先调用 `print_gpa_from_guest` 函数打印客户机传来的GPA，并从GPA中获取每级EPT页表的索引，保存在全局变量 `pxx_idx[4]` 中，`print_gpa_from_guest` 函数的打印结果如下。

gpt-dump/ept-dump.txt

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211111667.png" alt="image-20231121111115038" style="zoom:33%;" />

可以看到，此处的GPA与在客户机中读取GPT得到的GPA相同，说明客户机内核模块的超级调用成功。接下来调用 `mmu_spte_walk` 函数遍历此vCPU的EPT，在代码中称作spt，这是为了和影子页表共用一套代码。传入 `mmu_spte_walk` 函数的参数有vcpu，以及遍历到一个页表项时所调用函数的指针pr_spte，负责打印页表项。遍历代码如下。

linux-4.19.0/arch/x86/kvm/mmu.c

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211112468.png" alt="image-20231121111156730" style="zoom:33%;" />

`vcpu->arch.mmu.root_hpa` 保存了EPT的基地址，初始化时被设为INVALID_PAGE。`mmu_spte_walk` 函数首先判断vcpu->arch.mmu.root_hpa是否是无效页INVALID_PAGE，如果是，则说明vCPU对应的EPT尚未建立，无法遍历。如果EPT的级数大于PT64_ROOT_4LEVEL，则调用递归函数`__mmu_spte_walk` 遍历页表。

`page_header` 函数返回一个hpa_t变量所指向页表页的 `kvm_mmu_page` 结构的指针。于是，KVM将EPT第4级页表页的`kvm_mmu_page`结构传入`_mmu_spte_walk` 函数，并且从level=1开始遍历EPT。

和查询GPT一样，KVM遍历页表页中的每一个页表项，如果页表项的索引等于之前 `print_gpa_from_guest` 函数中获得的pxx_idx中对应的索引，那么此页表项就是目标页表项，并将它传入fn函数进行打印。如果查询到的页表项不是最后一级，则继续递归调用 `__mmu_spte_walk` 函数查询下一级页表。在这里，将fn置为打印EPT页表项的函数，如下打印格式与GPT相同。

gpt-dump/ept-dump.txt

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211113999.png" alt="image-20231121111259191" style="zoom:33%;" />

> 由于本实验没有关闭宿主机上的大页，KVM在查询到最后一级页表时做了两种处理：如果遍历到大页，则调用 `print_huge_pte` 函数打印最后获取HPA的过程；否则调用 `print_pte` 函数。

具体的打印代码不再赘述，下面只展示最后如何使用代码得到HPA。

linux-4.19.0/arch/x86/kvm/mmu.c

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211114649.png" alt="image-20231121111413371" style="zoom:33%;" />

* **对于2MB大页的情况，**最后一级页表项是PDE，这类页表项的第51：21位表示大页的起始物理地址，操作如下：
  * 这里使用PT64_DIR_BASE_ADDR_MASK宏从ent页表项中获取大页的起始物理地址；
  * 接下来，使用 `PT64_LVL_OFFSET_MASK(2)|(PAGE_SIZE-1)` 获取GPA中的大页偏移的部分，即20：0位；
  * 最终，结合大页起始地址和大页偏移得到HPA，最后调用 `__va` 函数获取对应的HVA，并解引用该HVA，读取到数据0xdeadbeef，符合预期。

* **对于普通4KB页的情况，**PTE中的第51：12位表示其指向的物理页的起始地址，操作如下：
  * 使用PT64_BASE_ADDR_MASK宏从PTE中获取页的物理地址；
  * 再使用PAGE_SIZE-1从GPA中获取页偏移，即11：0位；
  * 最后结合页的物理地址和页偏移得到GPA，最后调用 `__va` 函数得到HVA，并解引用该HVA，读取到数据0xdeadbeef，符合预期。

综上所述，客户机内核模块在一个GVA处写入了0xdeadbeef，读取客户机页表得到GVA对应的GPA，通过超级调用传递GPA到KVM模块，KVM读取EPT将GPA翻译成HPA，最后通过_va函数找到HPA对应的HVA，并读取到0xdeadbeef，表明HPA处确实存储了GVA处的数据，地址翻译成功。在翻译过程中，实验代码打印了地址翻译所涉及的页表项、GPA、HPA等，环环相扣。其中，ept-dump.txt文件存储了完整的输出信息。









# 5 //TODO: Lab5: 为edu设备添加设备驱动

实验第一步是在QEMU中启动带有edu设备的虚拟机，本次实验的启动参数如下：

```shell
#/bin/sh
qemu-system-x86_64  -smp 2 -m 4096 -enable-kvm ubuntu.img -device edu,dma_mask=0xFFFFFFFF -net nic -net user,hostfwd=tcp::2222-:22
```

磁盘空间不足...

---

> Masaryk大学编写edu设备的初衷是用于内核设备驱动的教学，Linux内核中并不存在edu设备的驱动程序。

本节将以实验的形式**为edu设备编写相应的驱动程序，**目的是为了更加清晰直观地展示虚拟设备背后的运行原理。本节分为三部分：

* 第一部分：分析与edu设备功能相关的寄存器；
* 第二部分：介绍在驱动中如何访问edu设备的配置空间和MMIO空间、发起DMA请求以及处理设备中断；
* 第三部分：演示实验的整体流程。

上文提到的 `edu_mmio_read` 函数和 `edu_mmio_write` 函数是edu设备的核心，当访问的地址在MMIO内存区域的偏移小于 `0x80` 时，只允许4字节大小的访问；当地址偏移大于或等于 `0x80` 时，允许4字节或8字节的数据访问。

## 5.1 edu设备功能模拟

edu设备在MMIO内存区域内会设置一些特殊的地址并赋予这些地址不同的读写权限，驱动读写这些地址时会触发相应的功能。下文会介绍这些特殊的地址。具体代码如下：

`qemu-4.1.1/hw/misc/edu.c`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221329672.png" alt="image-20231122132934509" style="zoom:33%;" />

`qemu-4.1.1/hw/misc/edu.c`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221331501.png" alt="image-20231122133036435" style="zoom:33%;" />

* `0x00(RO)`：0x00权限为只读，读0x00时会返回edu设备的标识符 `0x010000edu`；

* `0x04(RW)`：0x04权限为可读可写，读0x04时会返回edu设备中 `addr4` 成员变量的值，写0x04时会将写入的数据取反之后赋值给 `addr4`；

* `0x08(RW)`：0x08用于阶乘计算，权限为可读可写。

  * 读0x08时，会返回edu设备中 `fact` 成员变量的值，`fact` 表示阶乘结果；

  * 写0x08时，会将写入的数据赋值给 `fact`，然后edu设备的 `status` 会与宏变量 `EDU_STATUS_COMPUTING` 做或运算，并将运算结果赋值给 `status` 。这个宏变量的值为0x1，代表此时edu设备处于阶乘计算状态。

    * 之后会通过 `qemu_cond_signal` 函数唤醒在edu设备具象化时创建的 `edu_fact_thread` 线程，该线程用于阶乘计算。`edu_fact_thread` 函数在阶乘计算结束后会执行 `atomic_and(&edu->status,~EDU_STATUS_COMPUTING)`，改变edu设备的`status`。

      * 之后 `edu_fact_thread` 函数会检查 `status` 和 `EDU_STATUS_IRQFACT(0x80)` 与运算的结果，等价于检查 `status` 的第7位是否为1。若为1，代表edu设备被设置为执行完一次阶乘后需要发送中断，此时 `edu_fact_thread` 函数会调用 `edu_raise_irq` 函数向虚拟机发送中断。

        `edu_fact_thread` 函数代码如下：

        `qemu-4.1.1/hw/misc/edu.c`

        <img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221333437.png" alt="image-20231122133307817" style="zoom:33%;" />

* `0x20(RW)`：0x20权限为可读可写。读0x20时会返回edu设备的 `status`。对0x20写时，会将写入数据第7位的值赋给 `status` 的第7位，用于决定**每次阶乘后是否向虚拟机发送中断。**

* `0x24(RO)`：0x24权限为只读。对0x24读时会返回edu设备的 `irq_status`，代表中断产生的原因。驱动中的中断处理程序可以通过读0x24来获取 `irq_status`。

* `0x60(WO)`：0x60权限为只写。向0x60写数据时，会调用 `edu_raise_irq` 函数，`edu_raise_irq` 函数通过 `pci_set_irq` 接口向虚拟机发送中断，同时会把 `irq_status` 和写入数据的或运算结果赋值给 `irq_status`。具体代码如下：

  `qemu-4.1.1/hw/misc/edu.c`

  <img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221334205.png" alt="image-20231122133416887" style="zoom:33%;" />

* `0x64(WO)`：0x64权限为只写。0x64用于中断应答，将中断在 `irq_status` 中清除，停止生成该中断。向0x64写数据时，会调用`edu_lower_irq` 函数。`edu_lower_irq` 函数会把写入的数据取反后和 `irq_status` 进行与运算，并将最终结果赋值给 `irq_status`。通常驱动中的中断处理程序向0x64端口写入的值为 `irq_status`，这样便可以将 `irq_status` 的值置零。具体代码如下：

  `qemu-4.1.1/hw/misc/edu.c`

  <img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221336914.png" alt="image-20231122133600566" style="zoom:33%;" />

* `0x80(RW)`：0x80的权限为可读可写。读0x80时，会返回edu设备的 `dma.src`。对0x80写时，会将写入数据赋值给edu设备的 `dma.src`。`dma.src` 代表DMA的源地址。

* `0x88(RW)`：0x88的权限为可读可写。读0x88时，会返回edu设备的 `dma.dst`。对0x88写时，会将写入数据赋值给edu设备的 `dma.dst`。`dma.dst` 代表DMA的目的地址。

* `0x90(RW)`：0x90的权限为可读写。读0x90时，会返回edu设备的 `dma.cnt`。对0x90写时，会将写入数据赋值给edu设备的 `dma.cnt`。`dma.cnt` 代表DMA传输的字节数。

* `0x98(RW)`：0x98的权限为可读可写，被用作**DMA命令寄存器。**

  * 第0位为1代表开始DMA传输；
  * 第1位决定DMA数据传输的方向：
    * 0代表从RAM到edu设备；
    * 1代表edu设备到RAM；
  * 第2位决定是否在DMA结束之后向虚拟机发起中断，并将 `irq_status` 设置为0x100。

## 5.2 为edu设备添加设备驱动

> 本节将介绍如何在虚拟机中为edu设备添加相应的设备驱动，并设计测试程序使用edu设备。

当加载edu设备驱动模块时，PCI总线会遍历总线上已经注册的设备，调用总线的match函数判断是否有匹配的设备，匹配的依据是驱动提供的`pci_device_id`。edu设备源码中定义的 `vendor_id` 和edu设备id会被加入驱动代码中的 `pci_device_id` 数组 `pci_ids[]` 中，以实现**驱动和edu设备的匹配。**具体代码如下：

`edu_driver.c`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221337579.png" alt="image-20231122133703795" style="zoom:33%;" />

为edu设备驱动编写 `file_operations` 中的 `write` 函数和 `read` 函数可以按照PCI设备驱动编写的一般方法，代码如下：

```c
static ssize_t edu_read(struct file *filp, char __user *buf, size_t len, loff_t *off)
{
	ssize_t ret;
	u32 kbuf;

	if (*off % 4 || len == 0) {
		ret = 0;
	} else {
		kbuf = ioread32(mmio + *off);
		if (copy_to_user(buf, (void *)&kbuf, 4)) {
			ret = -EFAULT;
		} else {
			ret = 4;
			(*off)++;
		}
	}
	return ret;
}

static ssize_t edu_write(struct file *filp, const char __user *buf, size_t len, loff_t *off)
{
	ssize_t ret;
	u32 kbuf;

	ret = len;
	if (!(*off % 4)) {
		if (copy_from_user((void *)&kbuf, buf, 4) || len != 4) {
			ret = -EFAULT;
		} else {
			iowrite32(kbuf, mmio + *off);
		}
	}
	return ret;
}
```

---

由于edu设备的特殊性，edu设备驱动需要为edu设备设计专门的中断处理函数与 `probe` 函数以及用于控制edu设备的多种功能的 `ioctl` 函数。

### pci_probe

当驱动和设备完成匹配之后会调用 `probe` 函数执行设备的相关初始化工作。

1. `pci_probe` 函数中首先使用 `register_chrdev` 函数来注册edu设备，第一个参数为0代表使用系统动态分配的主设备号。
2. `pci_iomap` 函数会返回用于表示edu设备的PCI BAR的I/O地址空间的 `__iomem` 类型指针。后续 `iowrite*` 和 `ioread*` 函数会通过获得的`__iomem` 地址对edu设备的MMIO区域进行读写。
3. `pci_probe` 函数最后会向内核注册edu设备的中断服务函数 `irq_handler`，该函数是一个回调函数。当中断注入虚拟机时会调用 `irq_handler` 函数，并将设备号传递给它。具体代码如下：

`edu_driver.c`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221339374.png" alt="image-20231122133908748" style="zoom:33%;" />

### irq_handler

`pci_probe` 函数注册了 `irq_handler` 函数用于中断处理。该函数首先根据主设备号判断中断是否属于edu设备，之后 `irq_handler` 函数会读取edu设备的 `irq_status` 并判断产生该中断的原因。**为了区分DMA读中断、DMA写中断以及阶乘运算产生的中断，edu设备源码需要被修改。**具体改动如下：

* 当产生DMA读中断时会将edu设备的 `irq_status` 设置为0x100；
* 当产生DMA写中断时会将 `irq_status` 设置为0x101；
* 当 `irq_status` 等于0x1时代表阶乘运算中断。

打印出edu设备中断的原因后，`irq_handler` 函数会调用 `iowrite32` 函数，将 `irq_status` 写入上文提到的x64寄存器，以此向edu设备发送一个中断应答。edu设备会按照上文介绍的方式将 `irq_status` 置零，并拉低中断线电平。具体代码如下：

`edu_driver.c`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221339653.png" alt="image-20231122133947040" style="zoom:33%;" />

### edu_ioctl

> edu设备的特殊功能较多，为了使代码结构更清晰，需要设计 `edu_ioctl` 函数对edu设备的特性进行控制。用户程序的 `ioctl` 函数与驱动层的`edu_ioctl` 函数配合，实现向设备传递控制命令。

`ioctl` 函数的cmd参数具有以下五种控制命令，每条命令分别控制edu设备的一项功能：

* `DMA_WRITE_CMD` 代表发起一次DMA写操作，主要设置DMA的源地址、DMA的目的地址、DMA传输的数据长度以及edu设备定义的DMA组合指令。`DMA_CMD|DMA_TO_MEM|DMA_IRQ` 这一组合指令代表进行DMA写，并且在DMA结束后向虚拟机发送中断。
* `DMA_READ_CMD` 代表发起一次读操作，过程与 `DMA_WRITE_CMD` 类似。
* `PRINT_EDUINFO_CMD` 代表打印edu设备的基本信息，包括edu设备MMIO区域的大小、配置空间前64字节的信息、edu设备申请的硬件中断号、MMIO区域部分初始化的值。
* `SEND_INTERRUPT_CMD` 命令会写0x60寄存器，此时edu设备会发送中断，并将 `irq_status` 设置为0x12345678。
* `FACTORIAL_CMD` 命令代表发起一次阶乘运算，首先edu驱动会向 `0x20` 寄存器写入值0x80，这步的作用是设置edu设备在阶乘结束后发送中断。之后用于阶乘运算的值会被写入0x8寄存器，实验中用于阶乘计算的值是10。

`edu_ioctl` 代码如下：

`edu_driver.c`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221342332.png" alt="image-20231122134119769" style="zoom:33%;" />

## 5.3 整体实验流程

> 为了更好地展示实验结果，本节设计了一个简单的用户态测试程序，并在edu设备源码的关键位置添加了相应的输出信息。

实验第一步是在QEMU中启动带有edu设备的虚拟机，本次实验的启动参数如下：

`boot.sh`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221342184.png" alt="image-20231122134231280" style="zoom:33%;" />

```shell
#/bin/sh
qemu-system-x86_64  -smp 2 -m 4096 -enable-kvm ubuntu.img -device edu,dma_mask=0xFFFFFFFF -net nic -net user,hostfwd=tcp::2222-:22
```

进入虚拟机后，在终端输入 `lspci` 命令，根据edu的设备号以及 `vendor ID` 在PCI设备列表中可以查询到edu设备被挂载到了0号总线的04号槽：

`lspci`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221345306.png" style="zoom: 50%;" />

接着输入 `lspci -s 00:04.0 -vvv -xxxx` 命令，会显示edu设备的基本信息，包括edu设备的中断信息、MMIO地址空间信息以及设备配置空间信息等。

`lspci-s 00:04.0-vvv-xxxx`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221346763.png" alt="image-20231122134620958" style="zoom:33%;" />

---

~~~markdown
# IO虚拟化实验部分

I/O 虚拟化章节列举了多种常见的 I/O 虚拟化技术以及其原理。在该章节的实验中，要为 edu 虚拟设备添加设备驱动，并使用测试脚本对虚拟设备进行访问。

linux-image-unsigned-6.5.0-14-generic

使用方法：在客户机中使用 `make` 编译驱动程序与测试程序
git@github.com:GiantVM/Book.git

```
sudo insmod pci.ko

dmesg

cat /proc/devices | grep edu
# 245 pci-edu

sudo mknod /dev/edu c <major> 0

ls /dev/edu

# testing
sudo ./test
dmesg
```
~~~

上文介绍了 `edu_mmio_read` 函数的回调过程，所以实验的第一步首先对 `edu_mmio_read` 函数的调用过程进行验证。具体过程如下：

1. 在QEMU中启动虚拟机后，打开另外一个超级终端，通过 `ps -aux | grep qemu` 命令来查询QEMU的进程号；
2. 之后在终端中启动GDB调试，在GDB命令行中输入 `attach` + QEMU进程号，调试正在运行的QEMU程序；
3. 接着在GDB中为 `edu_mmio_read` 函数设置断点，输入 `c` 继续执行QEMU程序；
4. 然后在虚拟机中加载edu设备的驱动模块，此时GDB会显示QEMU的执行停在了 `edu_mmio_read` 函数处。

GDB中提供了 `backtrace` 命令用于查看函数的调用栈。最后输入 `backtrace` 命令，GDB会显示以下结果：

`GDB backtrace`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221348087.png" alt="image-20231122134735967" style="zoom:33%;" />

`backtrace` 的输出结果展示了地址转换的过程，我们可以将该过程与内存虚拟化文档中介绍的QEMU中内存地址转换过程对比。经过对比可以发现，GDB中显示的 `edu_mmio_read` 函数的函数调用栈与上节描述的一致。

---

然后将在GDB中设置的断点取消，继续运行QEMU进程。虚拟机随后运行先前编写的用户态测试程序，虚拟机的 `dmesg` 输出显示edu设备的配置空间信息与 `lspci-s 00:04.0-vvv-xxxx` 打印的结果一致。以下是 `dmesg` 中的部分信息：

`dmesg`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221351572.png" alt="image-20231122134922503" style="zoom:33%;" />

`dmesg` 的最后几条输出信息展示了edu功能的执行结果。驱动首先接收到了 `irq_status` 为0x12345679的设备中断，`irq_status` 与 `pci_ioctl ` 函数中设置的一致。QEMU输出了以下对应信息，包括 `irq_status` 的设置以及中断状态清除等。

`Log in QEMU`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221351686.png" alt="image-20231122135004570" style="zoom:33%;" />

然后驱动再次接收到了 `irq_status` 为0x1的设备中断，并判断该中断为阶乘计算产生的，最后输出了阶乘计算的结果 `0x375f00`。在QEMU中的信息展示了设置设备状态、分配阶乘对象 `fact` 以及发出阶乘运算中断的过程。

`Log in QEMU`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221350905.png" alt="image-20231122135050026" style="zoom:33%;" />

紧接着测试程序会发起DMA命令，会在edu设备中设置与DMA相关的信息。edu设备的 `edu_timer` 定时器检测到 `dma.cmd` 被设置为可运行状态后，会先根据 `dma.cmd` 第1位的值判断出DMA数据的传输方向，之后会检查DMA操作是否越界，如果未越界，则将DMA信息传入`pci_dma_read/pci_dma_write` 函数发起DMA操作。

`pci_dma_read/pci_dma_write` 函数的返回值用于判断DMA操作是否成功完成。当DMA操作完成后，edu设备会返回相应的DMA中断。如下QEMU中的输出信息展示了这一系列过程。

`Log in QEMU`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221352949.png" alt="image-20231122135155431" style="zoom:33%;" />

`Log in QEMU`

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311221355494.png" alt="image-20231122135510458" style="zoom:33%;" />

























