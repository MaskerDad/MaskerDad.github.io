---
title: kvm_riscv分析

date: 2023-12-27 17:00:00 +0800

categories: [kvm]

tags: [riscv, kvm]

description: 
---

# 0 参考

> 转载整理：[Sherlock's blog - 宗教般的信仰，初恋般的热情 (wangzhou.github.io)](https://wangzhou.github.io/)

* https://github.com/riscv/riscv-isa-manual/releases/download/Priv-v1.12/riscv-privileged-20211203.pdf

* [riscv/riscv-aia (github.com)](https://github.com/riscv/riscv-aia)

* [riscv中断异常委托关系分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv中断异常委托关系分析/)
* [riscv KVM虚拟化分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-KVM虚拟化分析/)
* [riscv kvm中断虚拟化的基本逻辑 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-kvm中断虚拟化的基本逻辑/)
* [riscv AIA逻辑分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-AIA逻辑分析/)
* [riscv AIA基本逻辑分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-AIA基本逻辑分析/)
* [CPU核中断设计基本逻辑 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/CPU核中断设计基本逻辑/)
* [riscv aclint逻辑分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-aclint逻辑分析/)
* [qemu tcg翻译执行核心逻辑分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/qemu-tcg翻译执行核心逻辑分析/)



# 1 qemu-kvm基本框架

## 1.1 qemu层

从qemu构架上看，kvm和tcg处于同一个层面上，都是cpu模拟的一种加速器。

> 虚拟机初始化逻辑

```c
/* accel/kvm/kvm-all.c */
kvm_init
  +-> qemu_open_old("/dev/kvm", O_RDWR)
      /* 创建虚拟机 */
  +-> kvm_ioctl(s, KVM_CREATE_VM, type);
      /* 虚拟机内存配置入口 */
  +-> kvm_memory_listener_register
    +-> kvm_region_add
      +-> kvm_set_phys_mem
        +-> kvm_set_user_memory_region
          +-> kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &mem)
```

> kvm vcpu线程启动的逻辑

```c
riscv_cpu_realize
  +-> qemu_init_vcpu(cs)
        /* kvm对应的回调函数在：accel/kvm/kvm-accel-ops.c: kvm_vcpu_thread_fn */
    +-> cpus_accel->create_vcpu_thread(cpu)
        (kvm_vcpu_thread_fn)
      +-> kvm_init_vcpu
        +-> kvm_get_vcpu
              /* 创建vcpu */
          +-> kvm_vm_ioctl(s, KVM_CREATE_VCPU, (void *)vcpu_id)
      +-> kvm_cpu_exec(cpu)
            /* 运行vcpu */
        +-> kvm_vcpu_ioctl(cpu, KVM_RUN, 0)
```

## 1.2 kvm层

kvm的入口函数在体系构架相关的代码里，riscv在 `arch/riscv/kvm/main.c` 里，`riscv_kvm_init` 直接调用到KVM的总入口函数 `kvm_init`，该函数将创建一个 `/dev/kvm` 的字符设备，随后所有kvm相关的操作都依赖这个字符设备。

---

> kvm模块初始化

`kvm_init` 大致流程：

```c
kvm_init
      /*
       * 以riscv为例, 主要是做一些基本的硬件检测，比较重要的是gstage mode和vmid
       * 的检测。riscv里的两级地址翻译，第一级叫VS stage，第二级叫G stage，这里
       * 检测的gstage mode就是第二级翻译的配置。
       */
  +-> kvm_arch_init
  [...]
      /* 注册/dev/kvm的字符设备 */
  +-> misc_register
  +-> kvm_preempt_ops.sched_in = kvm_sched_in;
  +-> kvm_preempt_ops.sched_out = kvm_sched_out;
```

---

> kvm创建虚拟机

`/dev/kvm` 这个字符设备只定义了对应的ioctl，这个ioctl支持的最主要的功能是创建一个虚拟机。我们看下KVM_CREATE_VM的逻辑：

```c
ioctl(KVM_CREATE_VM, kvm_fd)
kvm_dev_ioctl_create_vm
  +-> kvm_create_vm
        /* 分配gstage的pgd，vmid，guest的timer */
    +-> kvm_arch_init_vm
      /*
       * 这个ioctl会创建一个匿名文件，ioctl返回值是文件的fd, 这个fd就代表新创建的虚拟机，
       * 这个fd只实现了ioctl和release回调，release就是销毁虚拟机，ioctl用来配置虚拟机
       * 的各种资源，比如创建虚拟机的CPU(KVM_CREATE_VCPU)、给虚拟机配置内存		(KVM_SET_USER_MEMORY_REGION)
       * 等等。
       */
  +-> file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR)
```

---

> kvm创建虚拟机的vCPU

`kvm_vm_ioctl_create_vcpu` 函数将HSTAUS_SPV、HSTATUS_SPVP、HSTATUS_VTW配置到虚拟机vcpu的软件结构里。这里SPV比较有意思，虚拟机启动的时候，会先根据如上软件结构里的HSTATUS更新hstatus寄存器，然后sret跳转到虚拟机启动的第一条指令，sret会根据SPV寄存器的值配置机器的V状态，这里SPV是1，sret指令会先把V状态配置成1，然后跳到虚拟机启动的第一条指令。这里描述的是虚拟机最开始启动时候的逻辑。

```c
ioctl(KVM_CREATE_VCPU, kvm_vm_fd)
kvm_vm_ioctl_create_vcpu
      // riscv的实现在arch/riscv/kvm/vcpu.c
  +-> kvm_arch_vcpu_create
        /* 配置vcpu的timer，实现为配置一个hrtimer，在定时到时，注入时钟中断 */
    +-> kvm_riscv_vcpu_timer_init
    +-> kvm_riscv_rest_vcpu
          /* 软件之前配置好的信息，在这个函数里写到硬件里 */
      +-> kvm_arch_vcpu_load
        +-> csr_write更新CSR寄存器
        +-> kvm_riscv_gstage_update_hgatp  更新hgatp
        +-> kvm_riscv_vcpu_timer_restore   更新htimedelta
        /*
         * 为每个vcpu创建一个匿名的fd，这个fd实现的回调函数有：release、ioctl和mmap，
         * ioctl提供vcpu的控制接口，比如，运行vcpu(KVM_RUN)等等。
         */
  +-> create_vcpu_fd
```

---

> kvm配置虚拟机内存

```c
/* kvm_userspace_mem是从用户态传进来的虚拟机内存的配置信息 */
struct kvm_userspace_memory_region kvm_userspace_mem;

kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem)
  +-> kvm_set_memory_region
    +-> __kvm_set_memory_region
      +-> kvm_prepare_memory_region
            /* arch/riscv/kvm/mmu.c */
        +-> kvm_arch_prepare_memory_region
          +-> gstage_ioremap
                /* 配置第二级的页表 */
            +-> gstage_set_pte
      +-> kvm_create_memslot
      +-> kvm_commit_memory_region
```

虚拟机的物理地址是host的用户态分配的一段虚拟内存，这里面有三个地址: 

* 这段虚拟地址的 `hva`；
* 这段虚拟地址对应的物理地址 `hpa`；
* 虚拟机的物理地址 `gpa`；

这三个地址对应的实际内存是相同的，但是各自的数值是不同的。实际上，第2级翻译是 `gpa->hpa`，但是host上申请到的 `hva` 在host S-Mode上的翻译是 `hva->hpa` (页表基地址是satp)，所以，我们就要把 `gpa->hpa` 的映射插到第2级翻译对应的页表里 (hgatp)。

我们自然会联想第2级翻译缺页在哪里处理，这个逻辑单独在下面看。

---

> kvm运行vCPU

```c
kvm_vcpu_ioctl
  +-> case KVM_RUN
    +-> kvm_arch_vcpu_ioctl_run
          /* arch/riscv/kvm/vcpu.c */
      +-> kvm_riscv_vcpu_enter_exit
            /* arch/riscv/kvm/vcpu_switch.S */
        +-> __kvm_riscv_switch_to
```

`__kvm_riscv_switch_to` 函数将HS-Mode的相关寄存器保存起来，换上VS-Mode的寄存器，然后 `sret` 跳到虚拟机代码入口运行。vCPU的初始状态在如上创建vCPU的逻辑中配置的vCPU的软件结构，通过这里的`__kvm_riscv_switch_to` 配置到硬件CSR寄存器。此处和 `kvm_arch_vcpu_load` 恢复的寄存器不同，要注意这两个函数在整体流程中的位置：

* `vcpu_load -> kvm_arch_vcpu_load`
  * `fp/vector` 寄存器
  * `CSR_VS*` 寄存器，这些CSR仅影响了Guest_OS，和Host OS状态无任何关系
* `kvm_riscv_vcpu_enter_exit -> __kvm_riscv_switch_to`
  * Save Host GPRs (except A0 and T0-T6)
  * `sstatus/hstatus/sepc`，这些CSR直接影响了Host/Guest_world_switch

第2级翻译缺页的逻辑可以从vcpu_switch.S里的 `__kvm_riscv_switch_to` 入手看，这个函数是vCPU运行的入口函数。在sret投入运行前，这个函数里把 `__kvm_switch_return` 函数的地址配置给了stvec，当vCPU运行出现异常时，就会跳到 `__kvm_switch_return` 继续执行，这样就会从上面的 `kvm_riscv_vcpu_enter_exit` 出来，继续执行 `kvm_riscv_vcpu_exit`, **第2级缺页异常在这个函数里处理：**

```c
kvm_riscv_vcpu_exit
  +-> gstage_page_fault
        /*
         * 这个函数里会用hva(不是gpa)，判断是不是有合法的vma存在，如果有合法
         * 的vma存在，就可以分配内存，并且创建第二级页表，创建第2级map的时候使用
         * gpa->pa
         */
    +-> kvm_riscv_gstage_map
      [...]
```

以上就是虚拟机进入以及运行的逻辑，在用户态看，就是进入一个ioctl，停在里面运行代码，直到运行不下去了，ioctl就返回了，返回值以及ioctl的输出参数将携带退出的原因和参数。从kvm内部看，虚拟机退出是他执行指令的时候遇到了异常或者中断，异常或中断处理后再返回虚拟机。

触发虚拟机退出的原因还包括外设的MMIO访问，在构建虚拟机的地址空间时，没有对外设的MMIO GPA做第二级映射，这样第二级翻译的时候就会触发缺页异常，kvm的处理缺页的代码处理完缺页后不会立即返回虚拟机，而是进一步退出kvm层到qemu用户态， 即 `iotcl(KVM_RUN)` 返回。发生异常的指令的PC保存在sepc里，qemu会再次通过 `ioctl(KVM_RUN)` 进来，然后通过sret从sepc处继续进入虚拟机。

```c
/* arch/riscv/kvm/vcpu.c */
kvm_arch_vcpu_ioctl_run
      /* 这里一进来run vcpu就处理MMIO，可能是上次时MMIO原因退出的，这样当然要接着MMIO的上下文继续跑 */
  +-> if (run->exit_reason == KVM_EXIT_MMIO)
              kvm_riscv_vcpu_mmio_return(vcpu, vcpu->run)
      /* 投入运行虚拟机, 异常后也从这里退出来 */
  +-> kvm_riscv_vcpu_enter_exit
      /* 处理异常*/
  +-> kvm_riscv_vcpu_exit
    +-> gstage_page_fault
      +-> emulate_load
            /* 在这里配置退出条件 */
        +-> run->exit_reason = KVM_EXIT_MMIO

```

# 2 riscv-H扩展

riscv H扩展的目的是在硬件层面创建出一个虚拟的机器出来，基于此可以支持各种类型的虚拟化。比如，可以在linux上支持KVM。先不考虑中断和外设，我们看看要创建一个虚拟机需要些什么，我们需要GPR寄存器、系统寄存器以及一个 “物理” 地址空间，在这个虚拟机里运行的程序认为这就是他们的全部世界。我们可以把Host的GPRs和Host的系统寄存器给虚拟机里的程序用，对于每个虚拟机和Host，当他们需要运行的时候，由一个更底层的程序把在内存中保存的GPRs值和系统寄存器值换到物理GPR和系统寄存器上，这样每次虚拟机和虚拟机切换、虚拟机和Host切换都要切全部寄存器。

不同虚拟机不能直接使用Host物理地址作为他们的 “物理” 地址空间，为避免虚拟机物理地址之间相互影响，我们会再加一个层翻译，这层翻译把虚拟机物理地址翻译成Host物理地址，虚拟机自身看不到这层翻译，虚拟机正常做 `load/store` 访问 (先假设 `load/store` 访问的是虚拟机物理地址)，`load/store` 执行的时候会查TLB，可能做page walk，还可能报缺页异常，这些在虚拟机的世界里都不感知，查TLB和做page talk是硬件自己搞定的，处理缺页是更加底层的程序搞定的 (hypervisor)。为了支持这层翻译以及相关的异常，就需要在给硬件加相关的寄存器，可以想象，我们需要增加这层翻译对应的页表的基地址寄存器，还要增加对应的异常上下文寄存器，这些寄存器在虚拟机切换的时候都要切换成对应虚拟机的。

---

只有Host的时候，只要一层翻译就好，如果是运行在虚拟机里的系统，就需要两级翻译。运行在虚拟机里的系统自己不感知是运行在虚拟机上的，但硬件需要知道某个时刻是运行的是Guest还是Host的系统，这样硬件需要有一个状态表示，标识当前运行的是Guest还是Host的系统。

因此，riscv的H扩展增加了CPU的状态，增加了一个隐式的V状态：

* 当 `V=0` 的时候，CPU的U/M状态还和之前是一样的，S状态处在HS状态；
* 当 `V=1` 的时候，CPU原来的U/S状态变成了VU/VS状态；

V状态在中断或异常时由硬件改变，还有一个改变的地方是 `sret/mret` 指令。具体的变化逻辑是: 

1. 当在V状态trap进HS时，硬件会把V配置成0；
2. CPU Trap进入M状态，硬件会把V配置成0；
3. `sret` 返回时, 恢复到之前的V状态；
4. `mret` 返回时, 恢复到之前的V状态；

“之前的V状态“ 通过hstatus寄存器上的SPV (Supervisor Previous Virtualization mode) 记录，上述的 `sret` 和`mret` 就是从这个寄存器中得到之前的V状态。

> 如前所述，kvm在启动虚拟机之前会配置 `hstatu.SPV` 为1，这样使用sret启动虚拟机后，V状态被置为1。

---

H扩展新增了Hypervisor和Guest对应的两组CSRs，其中：

* ***Hypervisor对应的寄存器有:*** `hstatus`、`hedeleg/hideleg`、`hvip`、`hip/hie`、`hgeip/hgeie`、`henvcfg/henvcfgh`、`hounteren`、`htimedelta/htimedeltah`、`htval`、`htinst`、`hgatp`；
* ***Guest对应的寄存器有：***`vsstatus`、`vsip/vsie`、`vstvec`、`vsscratch`、`vsepc`、`vscause`、`vstval`、`vsatp`；

对于这些系统寄存器，我们可以大概分为两类，一类是配置hypervisor行为的，一类是VS/VU的映射寄存器。我们一个一个寄存器看下。

* VS/VU的映射寄存器就是CPU在运行在V状态时使用的寄存器，这些寄存器基本上是S-Mode寄存器的翻版，riscv spec中提到，当系统运行在V状态时，硬件的控制逻辑依赖这组 `vs*` 开头的寄存器，这时对S-Mode相同寄存器的读写被映射到 `vs*` 开头的这组寄存器上。

* `hedeleg/hideleg` 表示是否要把HS的中断继续委托到VS去处理，在进入V模式前，如果需要，就要提前配置好。具体的委托情况可以参考[这里](https://wangzhou.github.io/riscv中断异常委托关系分析/)。需要注意的是，RV协议上提到，当H扩展实现时，VS的几个中断的`mideleg` 对应域段硬件直接就配置成1了，也就是说默认被代理到HS处理。如果GEILEN非零，也就是有SGEI，那么SGEI也会直接硬件默认代理到HS处理。
* `hgatp` 是第二级页表的基地址寄存器。
* `hvip` 用于给虚拟机VS-Mode注入中断，写 `VSEIP/VSTIP/VSSIP` 域段，会给VS-Mode注入相关中断，
  riscv spec里没有说，注入的中断在什么状态下会的到响应？如果有多个VM实例，中断注入了哪个实例里?
* `hip/hie` 是hypervisor下中断相关的pending和enable控制。`hip/hie` 包含 `hvip` 的各个域段，除了如上的域段，还有一个SGEIP域段。协议上这里写的比较绕，先是总述了 `hip/hie` 里各个域段在不同读写属性下对应的逻辑是怎么样的，然后分开bit去描述。细节的逻辑是：
  * `hip.SGEIP` 是只读的，只有在 `hgeip/hgeie` 表示的vCPU里有中断可以处理时才是1，所以这个域段表示运行在这个物理CPU上的vCPU是否有外部中断需要处理; 
  * `hip.VSEIP` 也是只读的，在 `hvip.VSEIP` 是1或者 `hgeip` 有pending bit时，`hip.VSEIP` 为1。

* `hgeip/hgeie` 是SGEI的pending和enable控制，如果 `hgeip/hgeie` 是一个64bit的寄存器，那么它的 `1~63` bit可以表示 `1~63` 个vCPU的SGEI pending/enable bit，每个bit描述直通到该vCPU上的中断，所以，协议上说要配合中断控制器使用。注意，`hgeip` 是一个只读寄存器。

  罗列出riscv上所有的中断类型，S-Mode/M-Mode/VS-Mode的外部中断/时钟中断/软件中断，这些一共下来就是9种中断类型，再加上 `SGEI (Supervisor Guest External Interrupt`。

  > 所以，VSEIP是一个SEIP的对照中断，而SGEI是一个直通中断的汇集信号。
  >
  > vCPU怎么响应这个直通的中断？我们后面把这个逻辑独立出来描述。

* `htval/htinst` 是HS异常时的参数寄存器。`htval` 用来存放guest page fault的IPA，其他情况暂时为0，留给以后扩展。

---

新增加的虚拟化相关的指令大概分两类，一类是和虚拟化相关的TLB指令，一类是虚拟化相关的访存指令。

* 虚拟化扩展和TLB相关的指令有：`hfence.vvma` 和 `hfence.gvma`；
* 虚拟化相关的访存指令有：`hlv.xxx`、`hsv.xxx`；

这些指令提供在U/M/HS-Mode下的带两级地址翻译的访存功能，也就是虽然V状态没有使能，用这些指令依然可以得到 `gva` 两级翻译后的 `gpa`。

# 3 riscv中断虚拟化

> 中断虚拟化单独列一章，比较复杂。

## 3.1 kvm-riscv中断虚拟化基本逻辑

kvm虚拟化的时候，Guest的代码直接运行在Host上，怎么样触发虚拟机中断是一个问题。在非虚拟化的时候，中断的触发是一个物理过程，中断被触发后，跳到异常向量，异常向量先保存被中断的上下文，然后执行异常向量的业务代码。但是，kvm虚拟化的场景，所谓虚拟机只是运行的vCPU线程，我们假设硬件可以直接触发中断，但是触发中断的时候，物理CPU都可能运行的是其他的虚拟机，**怎么把特定虚拟机上的中断投递正确，这是一个需要解决的基本问题。**

我们再看另一个场景，在kvm虚拟化的时候，系统里有一个完全用软件模拟的IO设备，比如一个网卡，那这个网卡的中断怎么传递给正在运行的虚拟机。从上帝视角看，运行kvm虚拟机就是在 `ioctl(KVM_RUN)` 里切到虚拟机的代码去执行，要打断它，一个自然的想法就是从qemu里再通过一定的方法 “注入 `inject`” 一个中断。可以想象，所谓的 “注入” 中断，就是写一个虚拟机对应的寄存器，触发这个虚拟机上的执行流跳转到异常向量。对于核内的中断，比如时钟中断，还可以在Host kvm里就直接做注入。比如，可以在kvm里起一个定时器，当定时到了的时候给虚拟机注入一个时钟中断。

下面具体看下当前riscv的具体实现是怎么样的。

### 时钟中断

在kvm vCPU创建的时候，为vCPU创建 `timer`，实现上就是创建一个高精度定时器，定时到了的时候给vCPU注入一个时钟中断。

```c
/* linux/arch/riscv/kvm/vcpu.c */
kvm_arch_vcpu_create
  +-> kvm_riscv_vcpu_timer_init
        /* 中断注入的接口被封装到这个函数里，中断注入接口是kvm_riscv_vcpu_set_interrupt */
    +-> t->hrt.function = kvm_riscv_vcpu_hrtimer_expired
```

可以看到注入中断实际上是给kvm管理的vCPU软件结构体的 `irqs_pending/irqs_pending_mask` 置一，后面这个vCPU实际换到物理cpu上执行的时候，再写相应的寄存器触发cpu中断，具体工作在以下两个函数中完成：

```c
/*
 * We might have got VCPU interrupts updated asynchronously
 * so update it in HW.
 */
kvm_riscv_vcpu_flush_interrupts(vcpu);

/* Update HVIP CSR for current CPU */
kvm_riscv_update_hvip(vcpu);
```

kvm同时通过ioctl接口向qemu提供一组 `timer` 寄存器的读写接口，看起来，qemu会在vm stop的时候把 `timer` 的这组寄存器读上来，在vm resume的时候把这组寄存器写下去，这样在vm停止的时候，vm的 `timer` 状态就是没有变化的。

```c
/* linux/virt/kvm/kvm_main.c */
kvm_vcpu_ioctl
      /* linux/arch/riscv/kvm/vcpu.c, riscv里这个ioctl用来配置和读取寄存器，其中就包括timer相关寄存器的操作 */
  +-> kvm_arch_vcpu_ioctl
    [...]
          /*
           * 这里只看下timer寄存器的配置接口, 如下的函数里就涵盖了
           * frequency/time/compare/timer启动的配置，其中timer启动的实现就是启动
           * 上面提到的高精度定时器
           */
      +-> kvm_riscv_vcpu_set_reg_timer
```

qemu里在 `target/riscv/kvm.c` 里把 `timer` 寄存器配置的函数注册到了qemu里：

```c
kvm_arch_init_vcpu
  +-> qemu_add_vm_change_state_handler(kvm_riscv_state_change, cs)
```

在 `vm_state_notify` 函数里调用，看起来 `vm_state_notify` 是在vm启动停止的时候才会使用，用来做kvm和qemu里的信息的同步。

---

我们再考虑一个问题，怎么做到虚拟机的时间和实际时间相等？可以想象，只要让模拟 `vCPU_timer` 的那个定时器一直跑就可以，每次都把时间更新到vcpu的数据结构里就好。在每次停止和启动vm的时候，把时间在kvm和qemu之间做同步，vm停止的时候，vm的 `timer` 就也是停下来的。在qemu中date下获取当前时间，然后在qemu monitor里停止vm，过一会后启动vm，可以看见，时间基本是没有改变的。

### 外设中断

完全虚拟的设备向kvm注入中断时，虚拟设备这一层发起中断的流程还和tcg下的一样，到了向vCPU触发中断这一步，没有向tcg那样写到vCPU在qemu的结构体里，因为写到这个结构体里丝毫不会对kvm运行的指令有影响，这一步使用 `kvm_riscv_set_irq` 函数向kvm里注入中断，这个函数的封装函数在 `target/riscv/cpu.c` 里注册：

```c
/* qemu/target/riscv/cpu.c */
riscv_cpu_init
      /* riscv_cpu_set_irq里会调用kvm_riscv_set_irq */
  +-> qdev_init_gpio_in(DEVICE(cpu), riscv_cpu_set_irq, ...)
```

`kvm_riscv_set_irq` 使用 `ioctl(VM_INTERRUPT)` 注入中断，kvm里的调用流程如下:

```c
/* linux/virt/kvm/kvm_main.c */
kvm_vcpu_ioctl
      /* linux/arch/riscv/kvm/vcpu.c */
  +-> kvm_arch_vcpu_async_ioctl
    +-> kvm_riscv_vcpu_set_interrupt

int kvm_riscv_vcpu_set_interrupt(struct kvm_vcpu *vcpu, unsigned int irq)
{
	/*
	 * We only allow VS-mode software, timer, and external
	 * interrupts when irq is one of the local interrupts
	 * defined by RISC-V privilege specification.
	 */
	if (irq < IRQ_LOCAL_MAX &&
	    irq != IRQ_VS_SOFT &&
	    irq != IRQ_VS_TIMER &&
	    irq != IRQ_VS_EXT)
		return -EINVAL;

	set_bit(irq, vcpu->arch.irqs_pending);
	smp_mb__before_atomic();
	set_bit(irq, vcpu->arch.irqs_pending_mask);

	kvm_vcpu_kick(vcpu); //让对应vCPU切出后再注入中断

	return 0;
}
```

riscv-aia支持中断直通，接下来单独分析。

## 3.2 riscv-aia: 高级中断架构

### aia概述

* [riscv_aia_aplic | Personal Blog (org.edu.kg)](https://zq.org.edu.kg/2023/12/10/riscv_aia_aplic/)
* [riscv_MSIs | Personal Blog (org.edu.kg)](https://zq.org.edu.kg/2023/12/10/riscv_MSIs/)

如下是AIA中APLIC和IMSIC的一个示意图：

![image-20231229094422264](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312290944299.png)

#### IMSIC

一个hart上M-Mode、S-Mode以及不同的vCPU都有不同的IMSIC interrupt file，每个IMSIC interrupt file对下游设备提供一个MSI `doorbell` 接口。PCIe设备写这个MSI `doorbell` 接口触发MSI中断，APLIC写这个MSI `doorbell` 接口也可以触发MSI中断。APLIC作为次一级的中断控制器可以把下游设备的线中断汇集到一个MSI中断上。

标识一个MSI中断需要两个信息：

* 一个CPU的外部中断，比如 `S mode external interrupt` ；
* 另外一个是写入MSI `doorbell` 的MSI message，对应的中断编号；

前者叫`major identity`，后者叫 `minor identity`。所谓interrupt file就是 `minor identity` 的线性表，里面保存着对应中断的配置情况，比如：enable/pending等状态。各个 `minor identity` 标识的中断的优先级随编号增大而降低。

具体上看，关于这个interrupt file：

* 每个interrupt file包含一个enable表和一个pending表，表中每个bit表示每个MSI中的enable和pending状态；
* 一个interrupt file支持的MSI中断个数，最小是63，最大是2047，从下面 `eip/eie` 寄存器的定义也可以得到这里的最大最小值：
  * 当 `eip/eie` 是32bit时，64个 `eip/eie` 寄存器可以表示的最大值是2048；
  * 当 `eip/eie` 是64bit时，协议定义奇数 `eip/eie` 是不存在的，这样可以表示的最大值也是2048；

---

IMSIC通过一组CSR寄存器向外暴露信息或者接收CPU的配置。拿S-Mode的对应寄存器举例，相关的寄存器有：

- `siselect/sireg`

  AIA使用 `siselect` 将如下寄存器映射到 `sireg` 上，这样通过两个CSR就可以访问一堆寄存器，通过这种间接方式访问的寄存器有: `eidelivery/eithreshold/eip0-eip63/eie0-eie63`。

  * `eidelivery` 控制imsic是否可以报中断给hart，其中有一个可选配置项是可以控制是否把来自APLIC的中断直接报给hart；
  * `eithreshold` 可以设置优先级，比这个优先级高的中断才能报给hart；
  * `eip0-eip63/eie0-eie63` 就是相关中断的pending/enable状态，一个bit表示一个中断的相关状态；

- `stopi (S mode top interrupt)`

  读这个寄存器可以得到S-Mode下当前要处理的最高优先级的中断，包括major中断号和中断优先级编号；

- `sseteipnum/sclreipnum/sseteienum/sclreienum`

  如上寄存器名字里，第一个s表示是S mode，set表示置1，clr表示清0，ei表示是外部中断，p表示pending bit，num之前的e表示是enable bit，num表示操作的对象是中断的minor identity编号。所以，这几个寄存器直接操作interrupt file里具体中断的pending和enable状态。

- `stopei (S mode top external interrupt)`

  读这个寄存器可以得到S-Mode下当前要处理的最高优先级的外部中断的 `minor interrupt` 号，注意和 `stopi` 对比；

- `seteipnum_le/seteipnum_be`

  这两个寄存器是MSI doorbell寄存器，在对应MSI doorbell page的最开始，一个是小端格式，一个是大端格式，根据系统大小端配置，使用对应的寄存器。

可以看到riscv的MSI支持和ARM的GICv3(ITS)很不一样，imsic用一个表 (逻辑上我们把 `pending/enable` 看成一个表) 来表示所有支持的MSI中断，这样PCI设备发出的MSI message其实对应的是 `minor interrupt identity`，imsic收到 `minor interrupt identity` 后，直接配置对应的bit并且根据相关逻辑配置 `stopei`，`sseteipnum/sclreipnum/sseteienum/sclreienum` 也可以直接配置interrupt file里的对应bit。

而GICv3 ITS使用PCI设备相关的表格保存设备MSI中断对应的中断号，而且这些表格保存在内存里，可以想象GICv3在收到MSI message(ARM系统上一般一个PCI设备的MSI message从0开始依此递增)后应该从硬件报文里把设备信息(BDF)提取出来，然后再用设备信息去查找相关的表格得到MSI中断的硬件中断号，为了把这样的信息配置给ITS，GICv3里就还需要设计各种command以及附带的command queue。从如上的分析中，我们可以看出为啥AIA设计比GICv3简单很多但是基本功能都支持的一些原因。

#### //TODO: APLIC



### kvm-aia中断直通支持

#### aia中断直通基本原理

先看看要完成虚拟机中断，我们可以怎么做：

* **第一个问题：**假设所有的虚拟机中断都由hypervisor也就是kvm来注入，这样注入中断需要qemu发kvm的ioctl，但虚拟机里收到中断也处理不了，因为没有给虚拟机模拟guest内核可以看见的中断控制器，还要退出到qemu里处理中断。
* **第二个问题：**如上的方式，中断注入和处理都需要qemu的参与，性能比较低。我们考虑怎么可以直接把中断送到虚拟机里，并且在虚拟机里就可以处理相关的中断。

先看第一个问题，只需要让guest内核可以直接访问到中断控制器的接口就好，直观的理解，就是在hypervisor(kvm)里**<font color='red'>给guest模拟一个中断控制器</font>**就好。实现上：

* 一方面要把中断控制器的信息在dtb里描述，这样guest内核才能获取中断控制器的信息；
* 另一方面要在第二层的地址翻译里加上中断控制器MMIO到实际物理MMIO的映射，这样guest内核里的中断控制器驱动才能物理上使用中断控制器；

具体做法，就是在qemu里通过kvm的ioctl把上面的动作落实在硬件上。如果guest里有些控制是通过csr寄存器的，那么还要考虑csr的支持，这个要么硬件上就直接支持，否则需要trap进hypervisor去处理。

再看第二个问题：**<font color='red'>怎么把具体的中断送到虚拟机上。</font>**riscv的AIA imsic的拓扑大概是这样的：(AIA可以支持中断直通)

![image-20231229100825825](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312291008861.png)

从上图可以看出来，物理的core上，对于每个可以支持虚拟机的hart来说，是存在物理的连接的。PCIe设备发出一个MSI中断 (实际上是对一个地址的写操作)，进过IOMMU翻译得到物理地址，如果写在Guest interrupt file对应的地址上，中断信号就会送到Hart1 (假设没有受IMSIC上配置
的影响)。

到了这里，后面的逻辑就比较有意思了，关键是Hart1现在可能运行在不同的实例。比如，现在是Guest N的中断来了，但是Hart1可能跑Guest 1的实例，也可以跑Host系统。如果，Hart1跑的是Guest N的实例，那么直接中断现在CPU的运行就好，也就是说，硬件需要知道两个信息：

* **Hart1上跑的是哪个实例？**
* **相关中断是发给哪个实例的？**

只有知道这两个信息，硬件才知道当前中断是不是发给当前实例的。具体上只要给中断和Hart1上都加上VMID这个信息就好。如果，中断和当前CPU上运行的实例不匹配，直白的做法是把这个中断记录在虚拟机管理器 (也就是hypervisor里，hypervisor管理这个虚拟机，必然要维护虚拟机的状态)，等到对应虚拟机投入Hart1上运行的时候，就可以响应这个中断。但如果这样做，虚拟机上中断的响应完全依赖于hypervisor里虚拟机的调度，中断响应可能会不及时，一个可以想到的做法是，硬件识别到不是给当前实例的中断时，就把这个信息报到hypervisor上，hypervisor可以调度对应的guest实例运行，具体实现上，可以用VMID去做这个识别。

---

> 到此为止一切都好，但是你去看riscv协议，就会发现里面VMID这个概念只局限在第二层地址翻译常，并没有用VMID识别虚拟机。那riscv是怎么搞定上面的问题的？

S_GEXT被硬件直接配置mideleg代理到了HS，所以一旦有这个中断就在HS-Mode做中断处理 (虚拟机拉起之前并没有做继续委托)。看起来riscv的逻辑是这样的：

`hstatus.VGEIN` 可以实现类似过滤器的功能，当 `hstatus.VGEIN` 域段的数值和 `hgeip` 表示的vCPU相等时，`hip.VSEIP` 才能被配置上。这样当一个特定的vCPU被调度运行时，hypervisor在投入vCPU运行之前把vCPU对应的VGEIN打开，这样这个vCPU上的VS中断就可以直通到vCPU。

但是，依照之前的分析，S_GEXT中断也会上报到hypervisor，那就需要有机制可以做到以下两点：

* **当VS中断对应的vCPU在位的时候，中断只直通到vCPU；**
* **当VS中断对应的vCPU不在位的时候，中断投递到hypervisor；**

前者可以通过VGEIN过滤掉，针对后者，riscv上定义了 `hgeie`，这个寄存器决定哪个vCPU的S_GEXT是有效的。所以，在一个vCPU投入运行之前，hypervisor可以配置VGEIN的值是这个vCPU的编号，配置 `hgeie` 对于这个vCPU无效。在这样的配置下，当这个vCPU对应的VS中断到来时，中断被直通到guest，当来的不是这个vCPU的VS中断时，在HS触发S_GEXT中断。

>如何理解 `S_GEXT` ？
>
>`S_GEXT` 为HS-Mode下的虚拟机外部中断，根据riscv中断语义，`S_GEXT` 应该解释为在HS-Mode下处理的虚拟机外部中断。你或许听起来比较疑惑，虚拟机外部中断不应该在Guest (VS-Mode)中处理吗，为什么会路由到HS-Mode？实际上这正是为了处理上文中所说的一种情况：**当外设中断投递到pCPU时，对应vCPU不在位。**此时该中断应该转发到HS-Mode下的hypervisor，由hypervisor通过软件中断 (IPI) 的方式让对应vCPU切出，从而即时响应该中断。
>
>以上就是 `SGEI (Supervisor Guest External Interrupt)` 这种中断类型为什么存在的原因。

#### 打通riscv-h-aia硬件逻辑

> 整个中断虚拟化需要riscv的H扩展和AIA中断控制器的配合完成，但是H扩展和AIA的逻辑是独立的，各自的逻辑都可以自圆其说。

最核心的地方是，一个物理Hart上，定义了 `hgeie/hgeip` 寄存器，这个寄存器上的每个bit都对应这个物理Hart上一个虚拟机上的外部中断：

* `hgeip` 表示对应虚拟机上有没有外部中断上报；
* `hgeie` 表示对应的虚拟机外部中断会不会触发SGEI中断；

H扩展的定义不关心外部的中断控制器。H扩展增加了SGEI这个中断类型，当接收到虚拟机外部中断时，硬件通过SGEI中断把这个信息报给hypervisor，hypervisor就可以去调度虚拟机投入运行。`hgeie` 可以控制针对具体虚拟机的外部中断，是否上报SGEI。`hstatus.VGEIN` 控制一个具体虚拟机的外部中断是否可以直通到虚拟机，所谓直通到虚拟机，就是这个中断会触发CPU直接进入vCPU的中断上下文里。整个逻辑怎么串起来，在上面的章节里已经说明。

---

下面再看AIA的逻辑，riscv的aclint和plic是不支持PCI的MSI中断的，也不支持虚拟化中断直通，AIA主要是补齐了相关功能。

> 具体看，AIA新增了IMSIC(incoming MSI controller)、APLIC(Advanced plic) 以及MSI中断直通对于IOMMU的要求。

IMSIC是一个可以独立使用的可以接收MSI的中断控制器，从上面章节中的示意图上可以看到，每个物理的HART都有一个独立的IMSIC，这个IMSCI在M-Mode、S-Mode以及对于虚拟化都有独立的资源，针对虚拟机的资源是每个虚拟机都有一份的。所谓资源，IMSIC上叫做interrupt file，每个interrupt file有一个物理的MSI `doorbell` 接口，而一个interrupt file被用来记录所有通过它上报的中断。

我们从单个中断的视角再走一遍，也就是说假设中断写了guest 1 interrupt file的MSI doorbell，那么 `hgeip` 的对应bit就会置1，接下来根据 `hgeie` 的配置有两条不同的执行流：

* 如果这时 `hgeie` 对应bit置1，SGEI中断就会被触发，SGEI会被硬件代理到HS，那么就会进入hypervisor处理这个中断；
* 如果 `hgeie` 对应bit是0，那么SGEI中断不会被触发，如果这时VGEIN配置成1，那么VS中断被触发。一般在拉起虚拟机之前，hypervisor已经把VS中断代理到VS，此时这个中断就直接导致CPU进入vCPU的中断上下文；

> 需要注意的是，我们看问题的时候，一般不要这样顺着状态变迁分析，但是这样看一遍会叫我们对这个问题有一个感性直观的问题。

APLIC可以单独使用，APLIC也可以配合IMSIC使用。如果APLIC配合IMSIC使用，那么APLIC的输出必须被连在IMSIC的输入上，这样一个线中断被转成一个MSI中断。APLIC单独使用的时候，并不支持虚拟化中断直通。

理论上，我们对一个guest interrupt file的 `MSI doorbell` 写数据就可以触发对应的虚拟机机外部中断处于pending。但是，虚拟机里的直通设备并不能直接看到guest interrupt file 的`MSI doorbell` 的物理地址，所以需要在guest的地址空间上为guest interrupt file的 `MSI doorbell` 建立对应的映射，实际上就是在第二级页表里添加虚拟 `MSI doorbell` 到物理 `MSI doorbell` 的映射。

### KVM支持AIA

#### kvmtool使用AIA

如下是kvmtool里创建KVM里AIA的逻辑。首先是创建AIA设备，以及配置AIA设备:

```c
/* kvmtool/riscv/aia.c */
aia__create
  +-> ioctl(..., KVM_CREATE_DEVICE, ...)

/* kvmtool/riscv/aia.c */
aia__init
  +-> //使用一组ioctl获取或者设置AIA device的属性。
       /*
        * 其中重要的一步是给AIA这个设备配置MMIO空间，可以想象，要让guest的内核可以
        * 直接访问这个MMIO空间，kvm里是需要给这个MMIO做stage 2的页表映射的。
        *
        * 我们从KVM_DEV_RISCV_AIA_GRP_ADDR这个kvm ioctl接口跟进去，会看到AIA的MMIO
        * 地址被保存在了vcpu_aia->imsic_addr域段，查imsic_addr，可以发现它是在
        * kvm_riscv_vcpu_aia_imsic_update里被更新到硬件，也就是把imsic_addr到实际
        * 物理MMIO的映射加到stage2页表里。
        * 
        * 可以看到这个映射是在vcpu投入运行之前加上的，调用逻辑是:
        * kvm_arch_vcpu_ioctl_run
        *   +-> kvm_riscv_vcpu_aia_update
        *     +-> kvm_riscv_vcpu_aia_imsic_update
        *
        * 不过为啥要每次拉起虚拟机都做一次？
        */
  +-> ioctl(aia_fd, KVM_SET_DEVICE_ATTR, &aia_addr_attr)
```

另外，kvmtool需要根据具体情况生成guest的dtb，其中就包括AIA的dtb，这个dtb里描述的AIA和上面硬件定义的AIA匹配，guest内核用这个dtb的到AIA的信息，然后驱动上面配置好的AIA设备。

#### Linux KVM

```c
/* linux/arch/riscv/kvm/vcpu.c */
kvm_arch_vcpu_create
      /* 没有做什么 */
  +-> kvm_riscv_vcpu_aia_init 

/* linux/arch/riscv/kvm/main.c */
kvm_arch_init
      /* 初始化AIA以及中断虚拟化的一些全局参数 */
  +-> kvm_riscv_aia_init
    +-> csr_write CSR_HGEIE 的到hgeie的bit？
        /*
         * 每个物理CPU上维护一个aia_hgei_control的结构，在kvm这个层面管理这个物理
         * CPU上vCPU的外部中断。
         * 
         * 把IRQ_SEXT作为入参，调用irq_create_mapping得到一个hgei_parent_irq的中断号，
         * 再给这个中断挂上中断处理函数 hgei_interrupt。这里没有看懂?
         * 
         * 似乎这个中断是直接报给kvm的，中断处理函数里通过CSR_HGEIE/CSR_HGEIP知道
         * 中断发给哪个vCPU，对相应的vCPU做下kvm_vcpu_kick，这里没有看懂?
         */
    +-> aia_hgei_init
        /*
         * 把AIA device注册一下，这样用户态下发ioctl创建AIA device直接在kvm公共
         * 代码里调用AIA的回调函数就好。
         */
    +-> kvm_register_device_ops(&kvm_riscv_aia_device_ops, KVM_DEV_TYPE_RISCV_AIA)
    
static irqreturn_t hgei_interrupt(int irq, void *dev_id)
{
	int i;
	unsigned long hgei_mask, flags;
	struct aia_hgei_control *hgctrl = get_cpu_ptr(&aia_hgei);

	hgei_mask = csr_read(CSR_HGEIP) & csr_read(CSR_HGEIE);
	csr_clear(CSR_HGEIE, hgei_mask);

	raw_spin_lock_irqsave(&hgctrl->lock, flags);

	for_each_set_bit(i, &hgei_mask, BITS_PER_LONG) {
		if (hgctrl->owners[i])
			kvm_vcpu_kick(hgctrl->owners[i]);
	}

	raw_spin_unlock_irqrestore(&hgctrl->lock, flags);

	put_cpu_ptr(&aia_hgei);
	return IRQ_HANDLED;
}
```
