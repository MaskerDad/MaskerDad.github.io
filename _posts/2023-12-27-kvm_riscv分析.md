---
title: kvm_riscv分析

date: 2023-12-27 17:00:00 +0800

categories: [kvm]

tags: [riscv, kvm]

description: 
---

# 0 参考

> 转载整理：[Sherlock's blog - 宗教般的信仰，初恋般的热情 (wangzhou.github.io)](https://wangzhou.github.io/)

* https://github.com/riscv/riscv-isa-manual/releases/download/Priv-v1.12/riscv-privileged-20211203.pdf

* [riscv/riscv-aia (github.com)](https://github.com/riscv/riscv-aia)

* [riscv中断异常委托关系分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv中断异常委托关系分析/)
* [riscv KVM虚拟化分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-KVM虚拟化分析/)
* [riscv kvm中断虚拟化的基本逻辑 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-kvm中断虚拟化的基本逻辑/)
* [riscv AIA逻辑分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-AIA逻辑分析/)
* [riscv AIA基本逻辑分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-AIA基本逻辑分析/)
* [CPU核中断设计基本逻辑 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/CPU核中断设计基本逻辑/)
* [riscv aclint逻辑分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/riscv-aclint逻辑分析/)
* [qemu tcg翻译执行核心逻辑分析 | Sherlock's blog (wangzhou.github.io)](https://wangzhou.github.io/qemu-tcg翻译执行核心逻辑分析/)



# 1 qemu-kvm基本框架

## 1.1 qemu层

从qemu构架上看，kvm和tcg处于同一个层面上，都是cpu模拟的一种加速器。

> 虚拟机初始化逻辑

```c
/* accel/kvm/kvm-all.c */
kvm_init
  +-> qemu_open_old("/dev/kvm", O_RDWR)
      /* 创建虚拟机 */
  +-> kvm_ioctl(s, KVM_CREATE_VM, type);
      /* 虚拟机内存配置入口 */
  +-> kvm_memory_listener_register
    +-> kvm_region_add
      +-> kvm_set_phys_mem
        +-> kvm_set_user_memory_region
          +-> kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &mem)
```

> kvm vcpu线程启动的逻辑

```c
riscv_cpu_realize
  +-> qemu_init_vcpu(cs)
        /* kvm对应的回调函数在：accel/kvm/kvm-accel-ops.c: kvm_vcpu_thread_fn */
    +-> cpus_accel->create_vcpu_thread(cpu)
        (kvm_vcpu_thread_fn)
      +-> kvm_init_vcpu
        +-> kvm_get_vcpu
              /* 创建vcpu */
          +-> kvm_vm_ioctl(s, KVM_CREATE_VCPU, (void *)vcpu_id)
      +-> kvm_cpu_exec(cpu)
            /* 运行vcpu */
        +-> kvm_vcpu_ioctl(cpu, KVM_RUN, 0)
```

## 1.2 kvm层

kvm的入口函数在体系构架相关的代码里，riscv在 `arch/riscv/kvm/main.c` 里，`riscv_kvm_init` 直接调用到KVM的总入口函数 `kvm_init`，该函数将创建一个 `/dev/kvm` 的字符设备，随后所有kvm相关的操作都依赖这个字符设备。

---

> kvm模块初始化

`kvm_init` 大致流程：

```c
kvm_init
      /*
       * 以riscv为例, 主要是做一些基本的硬件检测，比较重要的是gstage mode和vmid
       * 的检测。riscv里的两级地址翻译，第一级叫VS stage，第二级叫G stage，这里
       * 检测的gstage mode就是第二级翻译的配置。
       */
  +-> kvm_arch_init
  [...]
      /* 注册/dev/kvm的字符设备 */
  +-> misc_register
  +-> kvm_preempt_ops.sched_in = kvm_sched_in;
  +-> kvm_preempt_ops.sched_out = kvm_sched_out;
```

---

> kvm创建虚拟机

`/dev/kvm` 这个字符设备只定义了对应的ioctl，这个ioctl支持的最主要的功能是创建一个虚拟机。我们看下KVM_CREATE_VM的逻辑：

```c
ioctl(KVM_CREATE_VM, kvm_fd)
kvm_dev_ioctl_create_vm
  +-> kvm_create_vm
        /* 分配gstage的pgd，vmid，guest的timer */
    +-> kvm_arch_init_vm
      /*
       * 这个ioctl会创建一个匿名文件，ioctl返回值是文件的fd, 这个fd就代表新创建的虚拟机，
       * 这个fd只实现了ioctl和release回调，release就是销毁虚拟机，ioctl用来配置虚拟机
       * 的各种资源，比如创建虚拟机的CPU(KVM_CREATE_VCPU)、给虚拟机配置内存		(KVM_SET_USER_MEMORY_REGION)
       * 等等。
       */
  +-> file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR)
```

---

> kvm创建虚拟机的vCPU

`kvm_vm_ioctl_create_vcpu` 函数将HSTAUS_SPV、HSTATUS_SPVP、HSTATUS_VTW配置到虚拟机vcpu的软件结构里。这里SPV比较有意思，虚拟机启动的时候，会先根据如上软件结构里的HSTATUS更新hstatus寄存器，然后sret跳转到虚拟机启动的第一条指令，sret会根据SPV寄存器的值配置机器的V状态，这里SPV是1，sret指令会先把V状态配置成1，然后跳到虚拟机启动的第一条指令。这里描述的是虚拟机最开始启动时候的逻辑。

```c
ioctl(KVM_CREATE_VCPU, kvm_vm_fd)
kvm_vm_ioctl_create_vcpu
      // riscv的实现在arch/riscv/kvm/vcpu.c
  +-> kvm_arch_vcpu_create
        /* 配置vcpu的timer，实现为配置一个hrtimer，在定时到时，注入时钟中断 */
    +-> kvm_riscv_vcpu_timer_init
    +-> kvm_riscv_rest_vcpu
          /* 软件之前配置好的信息，在这个函数里写到硬件里 */
      +-> kvm_arch_vcpu_load
        +-> csr_write更新CSR寄存器
        +-> kvm_riscv_gstage_update_hgatp  更新hgatp
        +-> kvm_riscv_vcpu_timer_restore   更新htimedelta
        /*
         * 为每个vcpu创建一个匿名的fd，这个fd实现的回调函数有：release、ioctl和mmap，
         * ioctl提供vcpu的控制接口，比如，运行vcpu(KVM_RUN)等等。
         */
  +-> create_vcpu_fd
```

---

> kvm配置虚拟机内存

```c
/* kvm_userspace_mem是从用户态传进来的虚拟机内存的配置信息 */
struct kvm_userspace_memory_region kvm_userspace_mem;

kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem)
  +-> kvm_set_memory_region
    +-> __kvm_set_memory_region
      +-> kvm_prepare_memory_region
            /* arch/riscv/kvm/mmu.c */
        +-> kvm_arch_prepare_memory_region
          +-> gstage_ioremap
                /* 配置第二级的页表 */
            +-> gstage_set_pte
      +-> kvm_create_memslot
      +-> kvm_commit_memory_region
```

虚拟机的物理地址是host的用户态分配的一段虚拟内存，这里面有三个地址: 

* 这段虚拟地址的 `hva`；
* 这段虚拟地址对应的物理地址 `hpa`；
* 虚拟机的物理地址 `gpa`；

这三个地址对应的实际内存是相同的，但是各自的数值是不同的。实际上，第2级翻译是 `gpa->hpa`，但是host上申请到的 `hva` 在host S-Mode上的翻译是 `hva->hpa` (页表基地址是satp)，所以，我们就要把 `gpa->hpa` 的映射插到第2级翻译对应的页表里 (hgatp)。

我们自然会联想第2级翻译缺页在哪里处理，这个逻辑单独在下面看。

---

> kvm运行vCPU

```c
kvm_vcpu_ioctl
  +-> case KVM_RUN
    +-> kvm_arch_vcpu_ioctl_run
          /* arch/riscv/kvm/vcpu.c */
      +-> kvm_riscv_vcpu_enter_exit
            /* arch/riscv/kvm/vcpu_switch.S */
        +-> __kvm_riscv_switch_to
```

`__kvm_riscv_switch_to` 函数将HS-Mode的相关寄存器保存起来，换上VS-Mode的寄存器，然后 `sret` 跳到虚拟机代码入口运行。vCPU的初始状态在如上创建vCPU的逻辑中配置的vCPU的软件结构，通过这里的`__kvm_riscv_switch_to` 配置到硬件CSR寄存器。此处和 `kvm_arch_vcpu_load` 恢复的寄存器不同，要注意这两个函数在整体流程中的位置：

* `vcpu_load -> kvm_arch_vcpu_load`
  * `fp/vector` 寄存器
  * `CSR_VS*` 寄存器，这些CSR仅影响了Guest_OS，和Host OS状态无任何关系
* `kvm_riscv_vcpu_enter_exit -> __kvm_riscv_switch_to`
  * Save Host GPRs (except A0 and T0-T6)
  * `sstatus/hstatus/sepc`，这些CSR直接影响了Host/Guest_world_switch

第2级翻译缺页的逻辑可以从vcpu_switch.S里的 `__kvm_riscv_switch_to` 入手看，这个函数是vCPU运行的入口函数。在sret投入运行前，这个函数里把 `__kvm_switch_return` 函数的地址配置给了stvec，当vCPU运行出现异常时，就会跳到 `__kvm_switch_return` 继续执行，这样就会从上面的 `kvm_riscv_vcpu_enter_exit` 出来，继续执行 `kvm_riscv_vcpu_exit`, **第2级缺页异常在这个函数里处理：**

```c
kvm_riscv_vcpu_exit
  +-> gstage_page_fault
        /*
         * 这个函数里会用hva(不是gpa)，判断是不是有合法的vma存在，如果有合法
         * 的vma存在，就可以分配内存，并且创建第二级页表，创建第2级map的时候使用
         * gpa->pa
         */
    +-> kvm_riscv_gstage_map
      [...]
```

以上就是虚拟机进入以及运行的逻辑，在用户态看，就是进入一个ioctl，停在里面运行代码，直到运行不下去了，ioctl就返回了，返回值以及ioctl的输出参数将携带退出的原因和参数。从kvm内部看，虚拟机退出是他执行指令的时候遇到了异常或者中断，异常或中断处理后再返回虚拟机。

触发虚拟机退出的原因还包括外设的MMIO访问，在构建虚拟机的地址空间时，没有对外设的MMIO GPA做第二级映射，这样第二级翻译的时候就会触发缺页异常，kvm的处理缺页的代码处理完缺页后不会立即返回虚拟机，而是进一步退出kvm层到qemu用户态， 即 `iotcl(KVM_RUN)` 返回。发生异常的指令的PC保存在sepc里，qemu会再次通过 `ioctl(KVM_RUN)` 进来，然后通过sret从sepc处继续进入虚拟机。

```c
/* arch/riscv/kvm/vcpu.c */
kvm_arch_vcpu_ioctl_run
      /* 这里一进来run vcpu就处理MMIO，可能是上次时MMIO原因退出的，这样当然要接着MMIO的上下文继续跑 */
  +-> if (run->exit_reason == KVM_EXIT_MMIO)
              kvm_riscv_vcpu_mmio_return(vcpu, vcpu->run)
      /* 投入运行虚拟机, 异常后也从这里退出来 */
  +-> kvm_riscv_vcpu_enter_exit
      /* 处理异常*/
  +-> kvm_riscv_vcpu_exit
    +-> gstage_page_fault
      +-> emulate_load
            /* 在这里配置退出条件 */
        +-> run->exit_reason = KVM_EXIT_MMIO

```

# 2 riscv-H扩展

riscv H扩展的目的是在硬件层面创建出一个虚拟的机器出来，基于此可以支持各种类型的虚拟化。比如，可以在linux上支持KVM。先不考虑中断和外设，我们看看要创建一个虚拟机需要些什么，我们需要GPR寄存器、系统寄存器以及一个 “物理” 地址空间，在这个虚拟机里运行的程序认为这就是他们的全部世界。我们可以把Host的GPRs和Host的系统寄存器给虚拟机里的程序用，对于每个虚拟机和Host，当他们需要运行的时候，由一个更底层的程序把在内存中保存的GPRs值和系统寄存器值换到物理GPR和系统寄存器上，这样每次虚拟机和虚拟机切换、虚拟机和Host切换都要切全部寄存器。

不同虚拟机不能直接使用Host物理地址作为他们的 “物理” 地址空间，为避免虚拟机物理地址之间相互影响，我们会再加一个层翻译，这层翻译把虚拟机物理地址翻译成Host物理地址，虚拟机自身看不到这层翻译，虚拟机正常做 `load/store` 访问 (先假设 `load/store` 访问的是虚拟机物理地址)，`load/store` 执行的时候会查TLB，可能做page walk，还可能报缺页异常，这些在虚拟机的世界里都不感知，查TLB和做page talk是硬件自己搞定的，处理缺页是更加底层的程序搞定的 (hypervisor)。为了支持这层翻译以及相关的异常，就需要在给硬件加相关的寄存器，可以想象，我们需要增加这层翻译对应的页表的基地址寄存器，还要增加对应的异常上下文寄存器，这些寄存器在虚拟机切换的时候都要切换成对应虚拟机的。

---

只有Host的时候，只要一层翻译就好，如果是运行在虚拟机里的系统，就需要两级翻译。运行在虚拟机里的系统自己不感知是运行在虚拟机上的，但硬件需要知道某个时刻是运行的是Guest还是Host的系统，这样硬件需要有一个状态表示，标识当前运行的是Guest还是Host的系统。

因此，riscv的H扩展增加了CPU的状态，增加了一个隐式的V状态：

* 当 `V=0` 的时候，CPU的U/M状态还和之前是一样的，S状态处在HS状态；
* 当 `V=1` 的时候，CPU原来的U/S状态变成了VU/VS状态；

V状态在中断或异常时由硬件改变，还有一个改变的地方是 `sret/mret` 指令。具体的变化逻辑是: 

1. 当在V状态trap进HS时，硬件会把V配置成0；
2. CPU Trap进入M状态，硬件会把V配置成0；
3. `sret` 返回时, 恢复到之前的V状态；
4. `mret` 返回时, 恢复到之前的V状态；

“之前的V状态“ 通过hstatus寄存器上的SPV (Supervisor Previous Virtualization mode) 记录，上述的 `sret` 和`mret` 就是从这个寄存器中得到之前的V状态。

> 如前所述，kvm在启动虚拟机之前会配置 `hstatu.SPV` 为1，这样使用sret启动虚拟机后，V状态被置为1。

---

H扩展新增了Hypervisor和Guest对应的两组CSRs，其中：

* ***Hypervisor对应的寄存器有:*** `hstatus`、`hedeleg/hideleg`、`hvip`、`hip/hie`、`hgeip/hgeie`、`henvcfg/henvcfgh`、`hounteren`、`htimedelta/htimedeltah`、`htval`、`htinst`、`hgatp`；
* ***Guest对应的寄存器有：***`vsstatus`、`vsip/vsie`、`vstvec`、`vsscratch`、`vsepc`、`vscause`、`vstval`、`vsatp`；

对于这些系统寄存器，我们可以大概分为两类，一类是配置hypervisor行为的，一类是VS/VU的映射寄存器。我们一个一个寄存器看下。

* VS/VU的映射寄存器就是CPU在运行在V状态时使用的寄存器，这些寄存器基本上是S-Mode寄存器的翻版，riscv spec中提到，当系统运行在V状态时，硬件的控制逻辑依赖这组 `vs*` 开头的寄存器，这时对S-Mode相同寄存器的读写被映射到 `vs*` 开头的这组寄存器上。

* `hedeleg/hideleg` 表示是否要把HS的中断继续委托到VS去处理，在进入V模式前，如果需要，就要提前配置好。具体的委托情况可以参考[这里](https://wangzhou.github.io/riscv中断异常委托关系分析/)。需要注意的是，RV协议上提到，当H扩展实现时，VS的几个中断的`mideleg` 对应域段硬件直接就配置成1了，也就是说默认被代理到HS处理。如果GEILEN非零，也就是有SGEI，那么SGEI也会直接硬件默认代理到HS处理。
* `hgatp` 是第二级页表的基地址寄存器。
* `hvip` 用于给虚拟机VS-Mode注入中断，写 `VSEIP/VSTIP/VSSIP` 域段，会给VS-Mode注入相关中断，
  riscv spec里没有说，注入的中断在什么状态下会的到响应？如果有多个VM实例，中断注入了哪个实例里?
* `hip/hie` 是hypervisor下中断相关的pending和enable控制。`hip/hie` 包含 `hvip` 的各个域段，除了如上的域段，还有一个SGEIP域段。协议上这里写的比较绕，先是总述了 `hip/hie` 里各个域段在不同读写属性下对应的逻辑是怎么样的，然后分开bit去描述。细节的逻辑是：
  * `hip.SGEIP` 是只读的，只有在 `hgeip/hgeie` 表示的vCPU里有中断可以处理时才是1，所以这个域段表示运行在这个物理CPU上的vCPU是否有外部中断需要处理; 
  * `hip.VSEIP` 也是只读的，在 `hvip.VSEIP` 是1或者 `hgeip` 有pending bit时，`hip.VSEIP` 为1。

* `hgeip/hgeie` 是SGEI的pending和enable控制，如果 `hgeip/hgeie` 是一个64bit的寄存器，那么它的 `1~63` bit可以表示 `1~63` 个vCPU的SGEI pending/enable bit，每个bit描述直通到该vCPU上的中断，所以，协议上说要配合中断控制器使用。注意，`hgeip` 是一个只读寄存器。

  罗列出riscv上所有的中断类型，S-Mode/M-Mode/VS-Mode的外部中断/时钟中断/软件中断，这些一共下来就是9种中断类型，再加上 `SGEI (Supervisor Guest External Interrupt`。

  > 所以，VSEIP是一个SEIP的对照中断，而SGEI是一个直通中断的汇集信号。
  >
  > vCPU怎么响应这个直通的中断？我们后面把这个逻辑独立出来描述。

* `htval/htinst` 是HS异常时的参数寄存器。`htval` 用来存放guest page fault的IPA，其他情况暂时为0，留给以后扩展。

---

新增加的虚拟化相关的指令大概分两类，一类是和虚拟化相关的TLB指令，一类是虚拟化相关的访存指令。

* 虚拟化扩展和TLB相关的指令有：`hfence.vvma` 和 `hfence.gvma`；
* 虚拟化相关的访存指令有：`hlv.xxx`、`hsv.xxx`；

这些指令提供在U/M/HS-Mode下的带两级地址翻译的访存功能，也就是虽然V状态没有使能，用这些指令依然可以得到 `gva` 两级翻译后的 `gpa`。

# 3 riscv中断虚拟化

> 中断虚拟化单独列一章，比较复杂。

## 3.1 kvm-riscv中断虚拟化基本逻辑

kvm虚拟化的时候，guest的代码直接运行在host上，怎么样触发虚拟机中断是一个问题。在非虚拟化的时候，中断的触发是一个物理过程，中断被触发后，跳到异常向量，异常向量
先保存被中断的上下文，然后执行异常向量的业务代码。但是，kvm虚拟化的场景，所谓虚拟机
只是运行的线程，我们假设硬件可以直接触发中断，但是触发中断的时候，物理CPU都可能
运行的是其他的虚拟机，怎么把特定虚拟机上的中断投递正确，这是一个需要解决的基本问题。

我们再看另一个场景，在kvm虚拟化的时候，系统里有一个完全用软件模拟的IO设备，比如，
一个网卡，那这个网卡的中断怎么传递给正在运行的虚拟机。从上帝视角看，运行kvm虚拟机
就是在kvm ioctl(KVM_RUN)里切到虚拟机的代码去执行，要打断它，一个自然的想法就是从
qemu里再通过一定的方法“注入”一个中断，可以想象，所谓的“注入”中断，就是写一个虚拟机
对应的寄存器，触发这个虚拟机上的执行流跳转到异常向量。

对于核内的中断，比如时钟中断，还可以在host kvm里就直接做注入。比如，可以在kvm里
起一个定时器，当定时到了的时候给虚拟机注入一个时钟中断。

下面具体看下当前riscv的具体实现是怎么样的。

### 时钟中断

在kvm vcpu创建的时候，为vcpu创建timer，实现上就是创建一个高精度定时器，定时到了
的时候给vcpu注入一个时钟中断。

```c
/* linux/arch/riscv/kvm/vcpu.c */
kvm_arch_vcpu_create
  +-> kvm_riscv_vcpu_timer_init
        /* 中断注入的接口被封装到这个函数里，中断注入接口是kvm_riscv_vcpu_set_interrupt */
    +-> t->hrt.function = kvm_riscv_vcpu_hrtimer_expired
```

可以看到注入中断实际上是给kvm管理的vcpu软件结构体的irqs_pending/irqs_pending_mask
bit置一，后面这个vcpu实际换到物理cpu上执行的时候，再写相应的寄存器触发cpu中断。

kvm同时通过ioctl接口向qemu提供一组timer寄存器的读写接口，看起来，qemu会在vm stop
的时候把timer的这组寄存器读上来，在vm resume的时候把这组寄存器写下去，这样在vm
停止的时候，vm的timer状态就是没有变化的。

```c
/* linux/virt/kvm/kvm_main.c */
kvm_vcpu_ioctl
      /* linux/arch/riscv/kvm/vcpu.c, riscv里这个ioctl用来配置和读取寄存器，其中就包括timer相关寄存器的操作 */
  +-> kvm_arch_vcpu_ioctl
    [...]
          /*
           * 这里只看下timer寄存器的配置接口, 如下的函数里就涵盖了
           * frequency/time/compare/timer启动的配置，其中timer启动的实现就是启动
           * 上面提到的高精度定时器
           */
      +-> kvm_riscv_vcpu_set_reg_timer
```

qemu里在target/riscv/kvm.c里把timer寄存器配置的函数注册到了qemu里：

```c
kvm_arch_init_vcpu
  +-> qemu_add_vm_change_state_handler(kvm_riscv_state_change, cs)
```

在vm_state_notify里调用，看起来vm_state_notify是在vm启动停止的时候才会使用，用来
做kvm和qemu里的信息的同步。

我们再考虑一个问题，怎么做到虚拟机的时间和实际时间相等。可以想象，只要让模拟vcpu
timer的那个定时器一直跑就可以，每次都把时间更新到vcpu的数据结构里就好。在每次停止
和启动vm的时候，把时间在kvm和qemu之间做同步，vm停止的时候，vm的timer就也是停下来
的。在qemu中date下获取当前时间，然后在qemu monitor里停止vm，过一会后启动vm，可以
看见，时间基本是没有改变的。

### 外设中断

完全虚拟的设备向kvm注入中断时，虚拟设备这一层发起中断的流程还和tcg下的一样，到了
向vcpu触发中断这一步，没有向tcg那样写到vcpu在qemu的结构体里，因为写到这个结构体
里丝毫不会对kvm运行的指令有影响，这一步使用kvm_riscv_set_irq向kvm里注入中断，
这个函数的封装函数在target/riscv/cpu.c里注册：

```c
/* qemu/target/riscv/cpu.c */
riscv_cpu_init
      /* riscv_cpu_set_irq里会调用kvm_riscv_set_irq */
  +-> qdev_init_gpio_in(DEVICE(cpu), riscv_cpu_set_irq, ...)
```

kvm_riscv_set_irq使用ioctl VM_INTERRUPT注入中断，基本的kvm里的调用流程如下:

```c
/* linux/virt/kvm/kvm_main.c */
kvm_vcpu_ioctl
      /* linux/arch/riscv/kvm/vcpu.c */
  +-> kvm_arch_vcpu_async_ioctl
    +-> kvm_riscv_vcpu_set_interrupt
```

riscv-aia支持中断直通，接下来单独分析。

## 3.2 riscv-aia: 高级中断架构

### aia概述

* https://wangzhou.github.io/riscv-AIA%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/

  



### kvm-aia中断直通支持

* https://wangzhou.github.io/riscv-AIA%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/













