---
title: KVM虚拟机热迁移_2

date: 2023-12-24 17:00:00 +0800

categories: [QEMU/KVM, 虚拟机热迁移]

tags: [virt, qemu, kvm]

description: 
---

# 0 资料

* [KVM虚拟机热迁移优化策略研究 - 中国知网 (cnki.net)](https://kns.cnki.net/kcms2/article/abstract?v=ipUboLYjcOXEHBN0AlEk_KKXSUxnt10wQMFI85wTMherf27eA0kpd4CogdsiGR6Oh0yCY6E0cbCbPOEcQkcBeqIe_AGtiV2QadH6UpDqaa4jPWxgZWFBI2_8X4rlM1m4iz_VGKy452BPKrX_PHNW0w==&uniplatform=NZKPT&language=CHS)
* [KVM虚拟机热迁移算法分析及优化 - 中国知网 (cnki.net)](https://kns.cnki.net/kcms2/article/abstract?v=ipUboLYjcOUCpMbra6pvsVq6vfUC3xSaczpcxkC7iD14FlMzTjVq81horFHho8-VUXQklynIDqCKm31k1YH4pJys3obPPGAaTsRQGS9M6j9QHWcNiUy6prYUoUROzDYxaWW9pBB9WVwabbyUB2SC_3inSINdJyKrHHF9O1Dx8fY=&uniplatform=NZKPT&language=CHS)
* [QEMU/KVM源码解析与应用-李强编著-微信读书 (qq.com)](https://weread.qq.com/web/reader/ec132be07263ffc1ec1dc10ka1d32a6022aa1d0c6e83eb4)
* [x86 kvm和qemu虚拟化介绍-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1792349)
* [qemu live migration代码分析-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1792351)



# 1 热迁移用法和基本原理

## 1.1 热迁移使用

虚拟化环境下的热迁移指的是在**虚拟机运行的过程中透明地从源宿主机迁移到目的宿主机，**热迁移的好处是很明显的，QEMU/KVM很早就支持热迁移了。早期的QEMU热迁移仅支持内存热迁移，也就是迁移前后的虚拟机使用一个共享存储，现在的QEMU热迁移已经支持存储的热迁移了。

首先来看热迁移是怎么使用的。一般来说需要迁移的虚拟机所在的源宿主机（src）和目的宿主机（dst）需要能够同时访问虚拟机镜像，为了简单起见，这里只在两台宿主机上使用同一个虚拟机镜像。

* 在src启动一个虚拟机vm1：

  ![image-20231227093855811](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312270938903.png)

* 在dst启动另一个虚拟机vm2：

  ![image-20231227093939350](C:/Users/26896/AppData/Roaming/Typora/typora-user-images/image-20231227093939350.png)

* 在vm1的monitor里面输入：

  ![image-20231227094356189](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312270943211.png)

隔了十几秒可以看到，vm2已经成为了vm1的状态，vm1则处于stop状态。

> 总结一下：
>
> src启动qemu-kvm增加参数-incoming tcp:0:6666
>
> dst进去qemu monitor执行migrate tcp:$ip:6666，可以用info migrate查看信息
>
> 如果是post copy执行migrate_set_capability postcopy-ram on，然后执行migrate_start_postcopy
>
> openstack环境nova-compute通过libvirt操作qemu，可以用virsh qemu-monitor-command  domain−−hmpcommand执行

## 1.2 热迁移整体概述

在热迁移过程中，Guest OS完全无感，其运行的任务，在快速迁移过后能继续运行。

首先，对于Guest OS从一个VM迁移到其他VM，涉及到对register配置，dirty cache，runtime context等数据的迁移。

如下图Guest OS1从VM1到VM2迁移过程：

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312270943255.png" alt="img" style="zoom:67%;" />

先由qemu monitor发起live migration，PC1的QEMU进程执行precopy，对源Guest OS的register，dirty cache bitmap，runtime context进行备份，创建传输channel，包括RDMA, tcp socket（网络），unix socket（进程间），exec stdin/stdout（进程间），fd（虚拟文件）等类型。迁移过程中 dirty page bitmap由QEMU copy，send，recv，restore，SRC和DST的QEMU进程间负责交互。

---

qemu中有两个概念 `save_vm` 和 `load_vm`，migration和snaptshot等都用到。

* `save_vm` 把qemu cpu state，mem，device state保存在一个fd，这个fd可以本地文件也可以是socket等；
* `load_vm` 正好相反，把保存的信息恢复到虚拟机；

热迁移就是在dst启动一个虚拟机，把src虚拟机的发送过来的状态都恢复到对应位置。因为mem比较大，发送时间长，根据发送时机不同分为`pre_copy` 和 `post_copy` 。

* `pre_copy` 先发送mem，达到一定阈值，停止src虚拟机运行，发送cpu state和device state，dst收到，然后运行；
* `post_copy` 先发送cpu state和device state，停止src虚拟机运行，dst标志page都无效，开始运行，qemu捕捉pagefault，然后从src请求page，src把page加入发送队列，dst等到这个page通知内核处理pagefualt然后继续运行。

`postcopy` 模式src背后还是一直默默给dst发送page的，只是dst等不着了一些page时插个队，优先发送要的page。总体来说，live migration状态多，线程多 (数据传递加保护，同步)，和kvm交互多 (log dirty page和userfault)，容易出错，可优化地方多。

## 1.3 基本原理

热迁移要处理的内容氛围三部分：cpu state, ram和device state。

* cpu就一堆register和stack什么，VMCS定义的那些状态；
* ram是大头，包括ROM，PCI mem和DIMM等，要求按page访问的；
* device就多了，有寄存器，队列等，千差万别，肯定得自己实现save和load函数，然后register给migration流程。

### 数据结构

```c
typedef struct SaveStateEntry {
    QTAILQ_ENTRY(SaveStateEntry) entry;
    char idstr[256];
    int instance_id;
    int alias_id;
    int version_id;
    /* version id read from the stream */
    int load_version_id;
    int section_id;
    /* section id read from the stream */
    int load_section_id;
    SaveVMHandlers *ops;
    const VMStateDescription *vmsd;
    void *opaque;
    CompatEntry *compat;
    int is_ram;
} SaveStateEntry;

typedef struct SaveState {
    QTAILQ_HEAD(, SaveStateEntry) handlers;
    int global_section_id;
    uint32_t len;
    const char *name;
    uint32_t target_page_bits;
} SaveState;

static SaveState savevm_state = {
    .handlers = QTAILQ_HEAD_INITIALIZER(savevm_state.handlers),
    .global_section_id = 0,
};
typedef struct SaveVMHandlers {
    /* This runs inside the iothread lock.  */
    SaveStateHandler *save_state;

    void (*save_cleanup)(void *opaque);
    int (*save_live_complete_postcopy)(QEMUFile *f, void *opaque);
    int (*save_live_complete_precopy)(QEMUFile *f, void *opaque);

    /* This runs both outside and inside the iothread lock.  */
    bool (*is_active)(void *opaque);
    bool (*has_postcopy)(void *opaque);

    /* is_active_iterate
     * If it is not NULL then qemu_savevm_state_iterate will skip iteration if
     * it returns false. For example, it is needed for only-postcopy-states,
     * which needs to be handled by qemu_savevm_state_setup and
     * qemu_savevm_state_pending, but do not need iterations until not in
     * postcopy stage.
     */
    bool (*is_active_iterate)(void *opaque);

    /* This runs outside the iothread lock in the migration case, and
     * within the lock in the savevm case.  The callback had better only
     * use data that is local to the migration thread or protected
     * by other locks.
     */
    int (*save_live_iterate)(QEMUFile *f, void *opaque);

    /* This runs outside the iothread lock!  */
    int (*save_setup)(QEMUFile *f, void *opaque);
    void (*save_live_pending)(QEMUFile *f, void *opaque,
                              uint64_t threshold_size,
                              uint64_t *res_precopy_only,
                              uint64_t *res_compatible,
                              uint64_t *res_postcopy_only);
    /* Note for save_live_pending:
     * - res_precopy_only is for data which must be migrated in precopy phase
     *     or in stopped state, in other words - before target vm start
     * - res_compatible is for data which may be migrated in any phase
     * - res_postcopy_only is for data which must be migrated in postcopy phase
     *     or in stopped state, in other words - after source vm stop
     *
     * Sum of res_postcopy_only, res_compatible and res_postcopy_only is the
     * whole amount of pending data.
     */


    LoadStateHandler *load_state;
    int (*load_setup)(QEMUFile *f, void *opaque);
    int (*load_cleanup)(void *opaque);
} SaveVMHandlers;

static SaveVMHandlers savevm_ram_handlers = {
    .save_setup = ram_save_setup,
    .save_live_iterate = ram_save_iterate,
    .save_live_complete_postcopy = ram_save_complete,
    .save_live_complete_precopy = ram_save_complete,
    .has_postcopy = ram_has_postcopy,
    .save_live_pending = ram_save_pending,
    .load_state = ram_load,
    .save_cleanup = ram_save_cleanup,
    .load_setup = ram_load_setup,
    .load_cleanup = ram_load_cleanup,
};

void ram_mig_init(void)
{
    qemu_mutex_init(&XBZRLE.lock);
    register_savevm_live(NULL, "ram", 0, 4, &savevm_ram_handlers, &ram_state);
}
struct VMStateField {
    const char *name;
    const char *err_hint;
    size_t offset;
    size_t size;
    size_t start;
    int num;
    size_t num_offset;
    size_t size_offset;
    const VMStateInfo *info;
    enum VMStateFlags flags;
    const VMStateDescription *vmsd;
    int version_id;
    bool (*field_exists)(void *opaque, int version_id);
};

struct VMStateDescription {
    const char *name;
    int unmigratable;
    int version_id;
    int minimum_version_id;
    int minimum_version_id_old;
    MigrationPriority priority;
    LoadStateHandler *load_state_old;
    int (*pre_load)(void *opaque);
    int (*post_load)(void *opaque, int version_id);
    int (*pre_save)(void *opaque);
    bool (*needed)(void *opaque);
    VMStateField *fields;
    const VMStateDescription **subsections;
};

static const VMStateDescription vmstate_e1000；
static void e1000_class_init(ObjectClass *klass, void *data)
{
    dc->vmsd = &vmstate_e1000;
}
int vmstate_register_with_alias_id(DeviceState *dev, int instance_id,
                                   const VMStateDescription *vmsd,
                                   void *base, int alias_id,
                                   int required_for_version,
                                   Error **errp);

/* Returns: 0 on success, -1 on failure */
static inline int vmstate_register(DeviceState *dev, int instance_id,
                                   const VMStateDescription *vmsd,
                                   void *opaque)
{
    return vmstate_register_with_alias_id(dev, instance_id, vmsd,
                                          opaque, -1, 0, NULL);
}
```

其中，全局变量 `savevm_state` 是链表，ram和device把实现的save和load函数放在链表节点上，迁移时遍历链表执行一遍就OK了。但ram和device不同之处在于，ram用 `SaveVMHandlers` 结构，device用 `VMStateDescription` 结构，`VMStateDescription` 可以嵌套，实现基本数据类型的save和load操作。

### pre_copy

`pre_copy` 先处理ram，开始标志所有ram为dirty page，循环发送ram，同时CPU在执行写ram，每次循环从kvm获取CPU写过的ram，直到达到一个条件，停止CPU，发送剩下的ram，再发送CPU和device state。

```c
migrate_fd_connect
{   
	//创建cleanup bh用于migration结束时，结束时触发执行
    s->cleanup_bh = qemu_bh_new(migrate_fd_cleanup, s);
    //创建migration工作线程
    qemu_thread_create(migration_thread)
}

migration_thread
{
    qemu_savevm_state_setup
    {
        ram_save_setup
        {
            ram_init_all->ram_init_bitmaps
                          {
                              ram_list_init_bitmaps
                              memory_global_dirty_log_start
                              migration_bitmap_sync->kvm_physical_sync_dirty_bitmap
                          }              
            创建线程compress_threads_save_setup
         }
    }
    while(true) 
    {
        qemu_savevm_state_pending->ram_save_pending->migration_bitmap_sync
        migration_iteration_run
        {
             if(!threshhold)
                 qemu_savevm_state_iterate
                     ram_save_iterate->ram_find_and_save_block
              else
                  qemu_savevm_state_complete_precopy->ram_save_complete
                  //其它设备状态
                  vmstate_save_state
               
        }
    }

    migration_iteration_finish->qemu_bh_schedule(s->cleanup_bh);
}

migrate_fd_cleanup
{
    qemu_savevm_state_cleanup->ran_save_cleanup
    停止线程 migration_thread
}


process_incoming_migration_co
{
    qemu_loadvm_state
     {
          qemu_loadvm_state_setup->ram_load_setup->compress_threads_load_setup
          //创建线程do_data_decompress和compress_threads_save_setup对应
          vmstate_load_state
          qemu_loadvm_state_main
          {
               case:    qemu_loadvm_section_start_full->vmstate_load_state
               case:    qemu_loadvm_section_part_end->vmstate_load_state
          }
          qemu_loadvm_state_cleanup
      }
    process_incoming_migration_bh
}
```

### post_copy

为什么需要 `postcopy`，因为 `pre_copy` 有可能无法收敛，虚拟机运行的飞快，不断产生dirty page，fd比较慢发送不完，无法达到预定的条件。`postcopy` 就是先发送cpu和device state，停止执行，再慢慢发送ram，如果dst发现少page，再从src请求这个page。

```c
migrate_fd_connect
{   
    //创建cleanup bh用于migration结束时，结束时触发执行
    s->cleanup_bh = qemu_bh_new(migrate_fd_cleanup, s);
    /****************************************/
    //如果是postcopy，创建收到page请求处理的线程
    open_return_path_on_source->qemu_thread_create(source_return_path_thread)
    /***************************************/
    //创建migration工作线程
    qemu_thread_create(migration_thread)
}
migration_thread
{
    /******************************/
    qemu_savevm_send_open_return_path
    qemu_savevm_send_ping
    qemu_savevm_send_postcopy_advise
    /******************************/
    qemu_savevm_state_setup
    {
        ram_save_setup
        {
            ram_init_all->ram_init_bitmaps
                          {
                              ram_list_init_bitmaps
                              memory_global_dirty_log_start
                              migration_bitmap_sync->kvm_physical_sync_dirty_bitmap
                          }              
            创建线程compress_threads_save_setup
         }
    }
    while(true) 
    {
        qemu_savevm_state_pending->ram_save_pending->migration_bitmap_sync
        
        migration_iteration_run
        {
             if(!threshhold&!post_copy)
                 /******************************/
                 if (postcopy_start()&&first)
                     return;
                 /*****************************/
                 qemu_savevm_state_iterate
                     ram_save_iterate
              else
                  migration_completion
                  {   
                  	  if pre_copy
                          qemu_savevm_state_complete_precopy->ram_save_complete
                          //其它设备状态
                          vmstate_save_state
                      /******************************/
                      else if post_copy
                          qemu_savevm_state_complete_postcopy->->ram_save_complete
                      /******************************/
                  }
        }
    }
    migration_iteration_finish->qemu_bh_schedule(s->cleanup_bh);
}

migrate_fd_cleanup
{
    qemu_savevm_state_cleanup->ran_save_cleanup
    停止线程 migration_thread
}


process_incoming_migration_co
{
    qemu_loadvm_state
     {
          qemu_loadvm_state_setup->ram_load_setup->compress_threads_load_setup
          //创建线程do_data_decompress和compress_threads_save_setup对应
          vmstate_load_state
          qemu_loadvm_state_main
          {
               case:    qemu_loadvm_section_start_full->vmstate_load_state
               case:    qemu_loadvm_section_part_end->vmstate_load_state
               /******************************/
               //只有post_copy才执行这个case
               case:    loadvm_process_command
               {
                    case:    loadvm_handle_cmd_packaged
                    {
                        //这儿递归了，只执行前两个case
                        qemu_loadvm_state_main
                    }
                    case:    loadvm_postcopy_handle_advise
                    //用于接收请求返回的page
                    case:    loadvm_postcopy_handle_listen
                    {
                        //从kernel接收pagefault，然后发送src请求page
                        qemu_thread_create(postcopy_ram_fault_thread)
                        ///接收src给的page
                        qemu_thread_create(postcopy_ram_listen_thread)
                    }
                    case:    loadvm_postcopy_handle_run->loadvm_postcopy_handle_run_bh
                    //src启动发送page thread，src修改page然后停止，dst cpu执行
                    case:    loadvm_postcopy_ram_handle_discard
               }
               /******************************/
          }
          qemu_loadvm_state_cleanup
      }
    process_incoming_migration_bh
}
postcopy_ram_listen_thread
{ 
    //只执行前两个case 
    qemu_loadvm_state_main
    qemu_loadvm_state_cleanup
}
```

---

`post_copy` 相比 `pre_copy` 多了如下过程：

`src`

```c
source_return_path_thread
qemu_savevm_send_open_return_path
qemu_savevm_send_ping
qemu_savevm_send_postcopy_advise
postcopy_start
qemu_savevm_state_complete_postcopy
```

`dst`

```c
loadvm_process_command和src同步，src告诉dst进入那一个步骤。
postcopy_ram_fault_thread(利用了USERFAULT机制让qemu知道了pagefault)->migrate_send_rp_req_pages
postcopy_ram_listen_thread
//恢复ram，通过userfaultfd通知内核pagefault处理完成，还有其它fd通知共享内存的其它进程等流程
ram_load->ram_load_postcopy->postcopy_place_page->qemu_ufd_copy_ioctl
```

* `pre_copy` 有可能无法收敛，达到一定时间就失败了，无法收敛是由于CPU dirty page产生过快，有人就想着让CPU执行慢一点；
* `post_copy` 的问题是一些情况是失败了无法恢复，pre_copy迁移失败，在src节点上照样可以恢复执行，而post_copy在dst上修改了状态，无法再同步给src，失败就恢复不了了。

# //TODO: 2 热迁移流程分析





# 3 热迁移中的脏页同步





# 4 热迁移中的相关参数控制





