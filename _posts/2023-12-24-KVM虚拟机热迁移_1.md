---
title: KVM虚拟机热迁移_1

date: 2023-12-24 17:00:00 +0800

categories: [QEMU/KVM, 虚拟机热迁移]

tags: [virt, qemu, kvm]

toc: true

description: 
---

# 0 资料

* [KVM虚拟机热迁移优化策略研究 - 中国知网 (cnki.net)](https://kns.cnki.net/kcms2/article/abstract?v=ipUboLYjcOXEHBN0AlEk_KKXSUxnt10wQMFI85wTMherf27eA0kpd4CogdsiGR6Oh0yCY6E0cbCbPOEcQkcBeqIe_AGtiV2QadH6UpDqaa4jPWxgZWFBI2_8X4rlM1m4iz_VGKy452BPKrX_PHNW0w==&uniplatform=NZKPT&language=CHS)
* [KVM虚拟机热迁移算法分析及优化 - 中国知网 (cnki.net)](https://kns.cnki.net/kcms2/article/abstract?v=ipUboLYjcOUCpMbra6pvsVq6vfUC3xSaczpcxkC7iD14FlMzTjVq81horFHho8-VUXQklynIDqCKm31k1YH4pJys3obPPGAaTsRQGS9M6j9QHWcNiUy6prYUoUROzDYxaWW9pBB9WVwabbyUB2SC_3inSINdJyKrHHF9O1Dx8fY=&uniplatform=NZKPT&language=CHS)
* [QEMU/KVM源码解析与应用-李强编著-微信读书 (qq.com)](https://weread.qq.com/web/reader/ec132be07263ffc1ec1dc10ka1d32a6022aa1d0c6e83eb4)
* [x86 kvm和qemu虚拟化介绍-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1792349)
* [qemu live migration代码分析-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1792351)



# 1 虚拟化热迁移技术概述

虚拟机迁移包括两种 ， 即 虚拟机冷迁移 和 虚拟机热迁移 。 

* **虚拟机冷迁移，**顾名思义 ，迁移时需要关闭虚拟机，停机时间比较漫长，业务连续性得不到保证。虚拟机冷迁移，首先需要关闭虚拟机， 然后通过离线网络将虚拟机的配置和磁盘镜像文件拷贝至目的主机，然后在目的宿主机上开启虚拟机，服务重新启动。由于业务中断的时间比较长，因此常用于非共享存储、跨数据中心的迁移。
* **虚拟机热迁移，**又称为虚拟机动态迁移，主要指将一台虚拟机从一台服务器迁移至另外一台服务器上。在迁移过程中，虚拟机仍然正常运行，服务依然正常对外提供，用户感觉不到服务的中， 也就是说热迁移对于操作系统、应用 软件、虚拟机的用户来说，都是完全透明的。

## 1.1 热迁移性能指标

虚拟机热迁移给虚拟机的管理、维护带来了方便，但虚拟机热迁移本身也是需要占用资源的，我们该如何来评估热迁移呢？评估热迁移的指标主要包括总迁移时间 (`total migration time`) 、停机时间 (`migration downtime`) 、迁移传输数据量、对应用程序的性能影响等。

1. **<font color='red'>总迁移时间。</font>**总迁移时间是指，从在源宿主机上发起迁移命令到虚拟机在目的宿主机上恢复运行所耗费的总时间。热迁移包括多个阶段，如内 存迭代拷贝、停机拷贝、虚拟机恢复等阶段，总迁移时间包括了这些阶段的总时间。由于热迁移技术往往用于负载均衡、服务器在线维护等， 这要求总迁移的时间能够尽可能短。热迁移本身对外界条件要求也高，一些突发情况（如网络中断）往往会导致迁移失败，这也要求迁移的时 间尽可能短。
2. **<font color='red'>停机时间。</font>**停机时间主要指，虚拟机在热迁移过程中，对外提供服务中断的时间。在这段时间內，虚拟机内部的应用程序、服务、网络处于停止状态，虚拟机不会响应外界的任何请求。停机时间是评估虚拟机热迁移的重要指标，特别是在互联网时代，很多业务的连续性、实时性要求都很高，这对虚拟机热迁移提出了更高的要求。
3. <font color='red'>**迁移数据传输量。**</font>迁移数据传输量，是指在热迁移过程中，在源宿主机和目的宿主机之间需要传输的数据总量。传输的数据主要包括迁移协议控制信息、内存数据、CPU状态数据、外设状态数据等，假如是块设备热迁移的话，还包括虚拟机虚拟磁盘数据。由于数据中心往往托管了上万台虚拟机，一台服务器上往往也有二三十台虚拟机，他们对网络资源的竞争较为激烈，迁移过程中，需要占用网络资源，假如同时迁移多台虚巧机，将占用更多的带宽资源。因此迁移时应尽量减少数据传输量。
4. **<font color='red'>对应用程序的性能影响。</font>**热迁移本身除了占用网络带宽资源，也需要占用一定的CPU资源，因此在迁移过程中，可能影响被迁移虚拟机的性能，也有可能影响源宿主机上的其他虚拟机。迁移过程中，应尽量减少对虚拟机应用程序的性能影响。

## 1.2 热迁移应用场景

虚拟机热迁移作为虚拟化技术的重要功能，主要用于负载均衡、能耗控制、服务器在线维护等领域。

1. **<font color='red'>负载均衡。</font>**在数据中心，应该尽量保证服务的性能均衡，避免负载的过高或过低。负载假如过高，虚拟机竞争激烈，性能受到严重影响。反之负载过低，物理资源使用率不高，浪费计算机资源。通过虚拟机热迁移，我们可将虚拟机从负载高的机器迁移至负载低的机器，这样保证了虚拟机的性能，也提高了资源利用率。
2. **<font color='red'>能耗控制。</font>**利用热迁移，我们可将低负载服务器上的虚拟机迁移至同一物理机，然后关闭空闲出来的物理服务器，这样既可减少物理机的损耗，也可降低整个数据中心的能耗，提高了数据中心的资源利用率，从而达到节能减排的目的。
3. <font color='red'>**服务器在线维护。**</font>在数据中心，往往需要对硬件进行维修，如增加网卡、磁盘、内存等，有时候还需要对操作系统、应用软件进行升级，这时可先将上面的虚拟机热迁移至其他机器，使服务器在线维护成为可能，同时，也保证了虚拟机的性能稳定。

## 1.3 两种主流的热迁移算法

热迁移主要包括內存迁移、设备状态迁移、关闭虚拟机、恢复虚拟机几个阶段，根据这几个阶段的不同顺序，热迁移算法主要可分为两类，即预拷贝（pre-copy）热迁移算法和后拷贝（post-copy）热迁移算法。 

### 预拷贝热迁移算法: pre_copy

2005年，Clark等人首先在Xen平台上实现了虚拟机热迁移功能，它就属于预拷贝热迁移算法。预拷贝热迁移算法是目前学术界、工业界最成熟的虚拟机热迁移算法，主流的虚拟化平台Xen、KVM其热迁移算法都是基于预拷贝算法实现的。

一台计算机的资源主要包括CPU、内存、外设，因此对虚拟机进行迁移也就是对这些资源进行迁移。预拷贝热迁移机制如下图所示：

![image-20231227143908597](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280910747.png)

首先将所有的内存都拷贝过去，在拷贝过程中，虚拟机仍然在运行，会不停地修改内存页，记录这些被修改的内存页，进入迭代预拷贝阶段。在这一阶段，重复拷贝上一轮的脏页。随着迭代的进行，脏页将越来越少，当脏页低于某个阀值时，暂停源宿主机上的虚拟机，将剩余的脏页、CPU状态以及外设状态拷贝至目的宿主机，送一阶段称为停机拷贝阶段。最后在目的宿主机上恢复虚拟机的运行。假如在迁移过程中发生故障，则立即在源宿主机上恢复虚拟机运行。

通常情况下，在迭代拷贝阶段，脏页会越来越少，但假如内存被频繁访问，或是网络带宽较低、网络拥堵，脏页往往达不到停止拷贝的阀值，这将导致迁移时间过长，甚至迁移永不结束等问题。

### 后拷贝热迁移算法: post_copy

后拷贝热迁移算法最早于2009年，由Hinesiw在Xen平台上实现了，后拷贝热迁移算法更多的只是在学术界进行讨论，在工业界并没有大规模使用。后拷贝热迁移算法，相比于预拷贝热迁移算法，它将内存拷贝这一阶段放置在CPU状态的拷贝之后。如下图所示：

![image-20231227145733264](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280910249.png)

迁移时首先暂停虚拟机运行，将虚拟机的CPU、外设状态拷贝至目的宿主机，然后直接在宿主机上恢复虚拟机的运行，将虚拟机的内存逐步拷贝至目的宿主机，在这过程中，虚拟机经常会出现缺页（PAGE_FAULT），这是由于所访问的页面还没有拷贝至目的宿主机，缺页时应该优先把所缺内存页拷贝至目的宿主机。后拷贝迁移算法，保证了所有的内存页最多拷贝一次，这避免了预拷贝时内存页的重复传输。

后拷贝迁移算法，由于首先将CPU、外设状态拷贝至目的宿主机，随后直接在目的宿主机上恢复虚拟机的运行，因此其停机时间很短。但由于内存未拷贝至目的宿主机，因此运行过程中会产生缺页，缺页需要通过网络传输，这一过程中，虚拟机必须暂停运行。而网络速度与内存访问速度相差巨大，这势必导致迁移初期，虚拟机上的应用程序性能损失巨大，当然也有人提出了内存页预缓存等技术进行改善。后拷贝热迁移算法还存在一个严重的问题，由于迁移时，虚拟机的状态己经分成两部分，目的宿主机拥有CPU、外设状态、部分内存状态，而源宿主机拥有部分内存状态，假如网络中断、或是目的宿主机、源宿主机出现停机情况，不仅热迁移过程失败，虚拟机由于状态不完整，将会出现崩溃。

# 2 基于x86的KVM虚拟化技术

虚拟机的迁移主要渉及到CPU、内存、外设三方面的迁移，而也正是这三方面构建了一台完整的虚拟机。毫无疑问，剖析KVM的虚拟化实现将有助于我们更好地理解虚拟机热迁移实现，为虚拟机热迁移优化做准备。KVM虚拟化的实现主要依靠内核KVM模块以及QEMU，其中KVM模块负责虚拟CPU、内存及部分中断功能，而QEMU则负责IO的虚拟化，本章节首先将剖析QEMU，介绍QEMU的多线程事件驱动模型，接着我们将剖析KVM模块，介绍KVM的CPU、内存虚拟化，最后介绍QEMU是如何实现IO虚拟化的。 

## 2.1 QEMU在KVM中的应用

QEMU是一个开源的快速模拟计算机，类似的模拟器还有Bochs、PearPC等，但QEMU具有高速度以及跨平台等特征。它支持在一种体系结构上（x86、ARM等）模拟另外一种体系结构，并在上面运行相应体系结构的操作系统、应用软件。QEMU支持两种工作模式，分别为系统模式（system mode）和用户模式（user mode）。

* 在系统模式下，QEMU可模拟多种不同体系结构下完整的硬件平台，包括CPU、内存以及外围设备，在上面可运行未经修改的操作系统，是一台完整的虚拟机；
* 在用户模式下，主要应用于Linux环境下，它支持将某些CPU体系结构下的Linux程序运行在其他不同体系结构的CPU上，常用于交叉编译器的测试以及CPU模拟器的测试。

我们将主要讨论系统模式下的QEMU。

### QEMU多线程事件驱动模型

运行一台完整的虚拟机，需要运行客户机代码、处理时钟、处理IO，同时对用户的命令进行及时响应等。要及时完成这些工作，这需要一种良好的软件架构能够合理地协调各种资源，当一些比较耗时的任务（如磁盘IO模拟、热迁移虚拟机等）正在进行时，虚拟机不能停止，客户机的代码依然正常执行。

对于这类需要处理多种资源请求的软件，业界有两种比较流行的架构：并行架构（Parallel architecture）和事件驱动架构（Event-driven archtecture）。QEMU使用了混合模式，即它的架构既采用了多线程，也采用了事件驱动，我们称它为**多线程事件驱动模型。**

* 利用事件驱动，可以同时对多种事件进行监听响应；
* 利用多线程，可以充分利用服务器的多核结构，将一些耗时的任务，如虚拟机迁移，委托给专门的线程执行，这样就不会阻塞了整台虚拟机的运行。

#### QEMU事件驱动

QEMU的主线程，负责整个软件的事件驱动，主要借助函数 `main_loop_wait`。它主要用来执行以下任务：

1. **执行poll操作，**查询文件描述符是否可读或可写。QEMU中检测的文件描述符主要包括以下几类：
   * block io，虚拟磁盘相关的io，为了保证高性能，主要采用异步IO实现；
   * eventfd，主要用于QEMU与KVM之间的通信交流；
   * socket，主要用于虚拟机迁移、虚拟机生命周期管理等。
2. **执行到期的定时器。**QEMU里面有三种时钟：realtime clock、virtual clock、host clock。QEMU利用这三种时钟的定时器，执行一些周期性的任务，如利用定时器定期发出SIGALRM信号，通知QEMU处理异常和中断。
3. **执行下半部（bottom-halves）。**下半部本质上是一种回调函数。QEMU通过异步IO来模拟磁盘IO，QEMU截获了虚拟机的磁盘访问请求后，使用独立的线程模拟IO，当线程完成IO时，需要通知虚揪机，通知虚拟机的工作就放在下半部完成。虚拟机迁移任务，当虚拟机迁移完成时，需要做关闭套接字等清理工作，这部分工作也放在下半部中。 

#### QEMU多线程

QEMU是一个多线程模型，如图所示：

![image-20231227152015813](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280910466.png)

一个QEMU进程往往包括多个线程，如主线程、VCPU线程、VNC线程、工作线程、迁移线程：

1. **主线程：**主要负责虚拟机的初始化，各种虚拟机设备的初始化，其他线程都由这个线程创建的。主线程在虚拟机运行过程中，运行着main loop，负责监听、响应各类文件描述符。
2. **VCPU线程：**负责模拟CPU，VCPU线程往往有多个，VCPU线程的个数等于虚拟机核数。VCPU线程负责客户机代码运行，既可以通过TCG机制翻译执行客户机代码，也可以通过KVM、Xen等硬件辅助虚拟化的方式执行。
3. **VNC线程**：主要负责计算密集型图像加码解码工作，鼠标、键盘的输入捕捉以及虚拟机的图形化界面都是由这个线程完成的，这算是一个特殊的工作线程。
4. **工作线程：**即worker thread，这一类线程主要负责IO模拟，QEMU通过线程池的方式模拟磁盘IO，磁盘IO请求组成一个队列，工作线程取出队列中的请求，进行模拟，工作线程根据负载情况有多个，动态创建、销毁。
5. **迁移线程：**当用户需要迁移虚拟机时，主线程会创建一个迁移线程，专门负责完成迁移工作。

### 客户机代码的执行

客户机代码的执行，指虚拟机操作系统、应用软件代码的执行，也就是CPU的虚拟化。QEMU中，客户机代码的执行主要由两种机制：TCG和KVM。

TCG，全称为Tiny Code Generator，即微指令翻译器，最初是作为一个Ｃ编译器的通用后端，后面被简化后用于QEMU中。如下图所示：

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312271551429.png)

TCG可以分为三部分，即前端解码器、中端分析器、以及后端翻译器。其中：

* 前端解码器读取客户机指令，根据客户机指令架构，将其翻译为中间码（TCG instructions，类似RISC指令的微指令）；
* 中端分析器读取翻译的中间码，进行优化，如去除一些冗余的指令；
* 优化后的中间码，输入后端翻译器，后端翻译器根据宿主机的CPU体系架构，将中间码翻译为宿主机目标代码；

TCG借助动态翻译执行客户机源代码，可以实现跨CPU体系架构，但由于是翻译执行，因此QEMU的效率并不是很高，假如客户机的代码能直接在物理CPU上运行，效率可以极大提高。KVM就是以这种方式模拟CPU，KVM主要借助硬件辅助虚拟化技术，让客户机的代码直接在物理ＣPU上运行，性能接近裸机性能，我们接下来将剖析KVM的CPU虚拟化实现。

## 2.2 CPU虚拟化

### Intel的VT-x技术

传统的x86架构对虚拟化并不友好，x86处理器通过Ring级别控制访问权限，分为Ring0、Ring1、Ring2、Ring3，其中Ring0级别最高，可以执行特权指令，Ring3级别最低。如图所示：

![image-20231227160019292](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280910432.png)

操作系统运行于Ring0级别，可以控制各种物理资源，而应用软件运行在Ring3级别，对资源的访问需要向操作系统请求。为了实现虚拟化，通常采用特权等级下降（Ring deprivileging）的方式实现。如上图所示，其中VMM运行于RING0级别，客户机操作系统运行于RING1级别，而客户机应用软件运行于RING3级别，但这种虚拟化实现方式存在内存地址空间缩小、部分指令不能虚拟化、RING Aliasing等问题。为了解决上述问题，提高虚拟化效率，Intel和AMD分别扩展了x86处理器的指令集，在硬件层面添加了部分虚拟化指令，称为硬件辅助虚拟化，Intel推出了VT-x技术，而AMD相应的推出了AMD-v技术，这里我们将主要分析Intel的VT-x技术。

Intel拓展了原有的x86架构，引入了两种专门为虚拟化打造的操作模式，即VMX根操作模式和VMX非根操作模式，如图所示：

![image-20231227160818072](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280911177.png)

VMX根操作模式类似于原有的x86架构，可像原先一样运行操作系统和应用软件，也可以运行虚拟机监视器；而VMX非根操作模式，则专门用于运行客户机操作系统，里面许多指令的特权级别都下降了。这两种CPU操作模式都有完整的四个RING级别，这样客户机的所有指令可以在原先的RING级别下执行，而虚拟机监控器也有足够的灵活度控制各类资源。

由于CPU拥有了多种操作模式，因此VT-x技术也定义了与此相对应的状态切换指令，包括VMXON、VMXOFF、VMENTRY和VMEXIT。其中：

* VMXON和VMXOFF是分别用来开启和关闭该项技术；
* VMENTRY是指CPU从VMX根操作模式切换到VMX非根操作模式，客户机占有CPU，客户机操作系统及应用软件开始执行；
* VMEXIT是指CPU由于异常从VMX非根操作模式退回到VMX根操作模式，这时虚拟机监视器开始执行；

---

就像Linux进程切换时需要将上下文信息保存在进程控制块中，虚拟机监控器与客户机由于共享了处理器，因此也需要一个内存区域来自动保存和恢复执行客户机和宿主机的运行上下文。在Intel VT-x技术中，我们称该内存区域为虚拟机控制块（VMCS），送是一个极为关键的结构，通常占用一个page的大小，它由虚拟机监控器（VMM）分配，一个虚拟CPU（VCPU）需要分配一个VMCS，VMCS由硬件进行读写，类似于页表，因此速度极快。VMCS主要由以下六个部分组成：客户机状态区、宿主机状态区、执行控制域、VM-Entry控制域、VM-Exit控制域、VM-Exit信息域。

* 其中客户机状态区和宿主机状态区用于保存客户机和宿主机上下文的状态；
* 执行控制域可以灵活的指定哪些指令和事件将导致VM Exit，通过IO bitmap可以自定义哪些IO访问将导致VM Exit；
* VM-Entry控制域和VM-Exit控制域主要用来控制发生VM-Entry和VM-Exit时对CPU模式的一些操作，如是处于32位还是64位模式；
* VM-Exit信息域主要记录VM-Exit产生的原因和具体信息。

通过VMCS，当发生VM-Entry时，硬件将自动从VMCS的客户状态区加载客户机操作系统的上下文，恢复客户机的执行；同时根据VM-Entry控制域的内容，决定向客户机塞入哪种中断或异常；在客户机执行期间，将根据执行控制域的信息，决定哪些指令和事件将触发VM-Exit；VMX-Exit时，将退出原因写入VM-Exit信息域，将虚拟机上下文保存至客户机状态域，加载宿主机状态域信息，然后根据VM-Exit的原因进行相应处理。

### KVM: CPU虚拟化实现

#### KVM用户接口

KVM是作为Linux内核的一个字符设备提供出来的，它包含两个内核模块 `kvm.ko` 和 `kvm-intel.ko` 或 `kvm-amd.ko`，其中`kvm.ko` 提供了核心虚拟化架构，而 `kvm-intel.ko` 和 `kvm-amd.ko` 与处理器的架构相关。

当内核加载KVM模块时，将出现一个 `/dev/kvm` 的字符设备，一台完整的虚拟机由QEMU和KVM构成，如下图所示：

​        								![image-20231227170301379](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280912368.png)

QEMU 在用户空间模拟PCI总线、BIOS、VGA、键盘、鼠标、网卡、磁盘等外设，而KVM主要虚拟CPU和内存，QEMU通过一系列的 `ioctl` 系统调用来操作 `/dev/kvm` 完成对CPU和内存的虚拟化。

---

KVM主要提供了三类接口：`kvm_dev_ioctl/kvm_vm_ioctl/kvm_vcpu_ioctl`，如下表所示：

![image-20231227170908176](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280912171.png)

* QEMU打开 `/dev/kvm` 设备后会获得一个KVM文件描述符，将这个文件描述符及相应的命令（如KVM_CREATE_VM）传入 `kvm_dev_ioctl`，会创建一台虚拟机，建立相应的数据结构，同时返回一个 kvm-vm 文件描述符；
* 通过 `kvm_vm_ioctl` 可以对这台虚拟机进行操控，比如创建VCPU、设置其内存空间、创建IRQCHIP、PIT、时钟等。创建VCPU后，会返回一个 kvm-vcpu 文件描述符；
* 将 kvm-vcpu 文件描述符传入 `kvm_vcpu_ioctl` 可以对虚拟CPU进行控制，比如可以运行CPU、获取设置CPU的设备寄存器、给CPU注入中断和事件等。

#### KVM虚拟机的创建和运行

QEMU属于用户态，它初始化完各种外设后，将开始调用KVM提供的用户态接口，创建虚拟机的CPU部分。QEMU首先会打开 `/dev/kvm`，会返回一个kvm fd，通过传递这个文件描述符以及KVM_CREATE_VM给 `kvm_dev_ioctl`，KVM将创建一台虚拟机，执行函数为 `kvm_dev_ioctl_create_vm` 函数，该函数会分配一个 kvm 结构体，一个 kvm 结构体就代表一台虚拟机，初始化该结构体，并将该结构体加入虚拟机链表中，同时返回用户态一个 kvm-vm 文件描述符。

这时的虚拟机是一台缺少CPU的虚拟机。QEMU会根据虚拟机的CPU个数创建多个VCPU线程。每个VCPU线程将kvm-vm fd和KVM_CREATE_VCPU传递给 `kvm_vm_ioctl` 创建 vcpu，内核中将调用 `kvm_vm_ioctl_create_vcpu` 函数创建 vcpu，该函数将为每一个vcpu分配一个VMCS的结构，并进行初始化，并返回一个kvm-vcpu文件描述符给用户态。							

VCPU创建完成后，虚拟机就开始运行了。QEMU的VCPU线程主要包含一个主循环，QEMU通过KVM_RUN这个 `ioctl` 调用进入客户机。内核中的KVM部分具体调用函数 `vcpu_enter_guest` 进入客户机，其底层是通过虚拟化指令VMRESUME切换到VMX非根操作模式。客户机代码开始运行，直到遇到敏感指令或IO指令，这时将产生VM-Exit事件，客户机退出，由KVM进行异常处理。一般的异常，如对某些控制寄存器的访问、EPT缺页异常等在KVM这一层就可以得到处理，CPU将重新进入客户机模式执行，但还有一类异常，如部分IO指令，KVM不能处理，需要QEMU进行处理，因此将返回用户态，由用户态的QEMU进行处理，QEMU处理完IO后，将进入内核态，客户机又重新恢复运行，永远循环下去，整个执行过程如下图所示。

![image-20231227170759251](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312271707279.png)

### KVM: CPU热迁移

VMCS这个结构体的客户机状态域保存了客户机CPU的上下文，因此CPU的迁移思路很简单。我们可以从VMCS获取所有客户机所有寄存器的状态，传递到目标机器后，再写入VMCS相应的域，恢复虚拟机的运行即可，而KVM也提供了相应的接口KVM_GET_REGS、KVM_SET_REGS等，可以供QEMU的迁移线程调用。

## 2.3 内存虚拟化

### Intel的EPT技术

为了保证客户机操作系统看到的内存空间布局与实际物理环境相一致，也就是客户机拥有从零开始、连续的内存空间，同时与其他虚拟机隔离，我们需要实现内存的虚拟化。在内存的虚拟化中，主要涉及到四种地址：客户机虚拟地址GVA、客户机物理地址GPA、宿主机虚拟地址HVA和宿主机物理地址HPA。对于虚拟机来说，它的内存物理地址空间要求必须是连续的，但对于宿主机来说，却并不是连续的。如下图所示，虚拟机的内存空间是由宿主机的若干段不连续的内存片段拼凑而成。

![image-20231228091953038](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280919080.png)

客户机页表可以将GVA转换为GPA，但GPA不能直接用于宿主机的MMU（内存管理单元）进行寻址，必须将GPA转换为HPA，在KVM中有一个 `kvm_memory_slot` 的数据结构体，它记录了GPA与HVA的对应关系，可以实现GPA到HVA的转换，然后再通过宿主机的页表转换为HPA，进行实际的内存访问。

传统的内存虚拟化是通过纯软件的方式实现的，即通过影子页表（Shadow Page Table）实现的，影子页表直接完成客户机虚拟地址空间到宿主机物理空间的映射。客户机在访存的时候，宿主机的MMU将直接加载客户机页表所对应的影子页表，从而实现GVA到HPA的直接转换，TLB缓存的也是GVA到HPA的转换。影子页表存在很大的缺陷，如：

* 需要为客户机的每个进程都维护一套影子页表，因此会消耗较大的内存空间；
* 每一次发生VM-Exit、VM-Entry时，都会导致TLB的清空；
* 此外影子页表与客户机页表间的同步也比较复杂；

为此Intel推出了EPT技术，AMD也相应推出了NPT技术，为内存虚拟化提供硬件支持，二者原理相同。

---

EPT技术在原有的客户机页表基础上，引入了EPT页表。客户机虚拟地址到客户机物理地址的转换借助客户机页表来完成，而客户机物理地址到宿主机物理地址的转换则借助EPT页表来完成，两次转换的机制相同，且都是由硬件自动完成的。如下图所示：

![image-20231228093405407](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280934431.png)

客户机运行时，`CR3` 寄存器保存有客户机页表基址，负责载入客户机页表，`EPTP` 寄存器保存有EPT页表基址，负责载入EPT页表，两套页表实现了对内存地址的转换。在客户机运行过程中，客户机页表由客户机操作系统进行维护，客户机内部产生的缺页并不会导致客户机退出；而EPT页表则由虚拟机监视器（VMM）维护，EPT页表产生的异常，如缺页、写权限不足，将导致客户机退出，产生EPT异常，由虚拟机监视器（VMM）进行处理。EPT技术由于借助硬件方式实现，相比影子页表的软件方式，优势在：

* 实现复杂度降低很多，且效率高；
* 客户机内部的缺页异常并不会导致客户机退出，提高了客户机的运行性能；
* 每一台虚拟机只需要维护一套EPT页表，也节省了许多内存空间； 

### KVM: 内存虚拟化实现

KVM的內存虚拟机有两种方式实现，即影子页表和EPT技术，我们将主要探讨KVM如何使用EPT技术完成内存的虚拟化。

#### QEMU内存模型

QEMU的内存模型，是指KVM虚拟机的内存是由QEMU在用户态进行申请管理，并将部分申请的内存注册到KVM模块中。这种模型的好处在于：

* 策略与机制分离，加速的机制可以由KVM实现，QEMU只需负责如何调用；
* QEMU可以设置多种内存模型，如UMA、NUMA等；
* 方便QEMU对特殊内存的管理，如MMIO；
* 内存的分配、回收、交换都可以采用Linux原有的内存管理机制，KVM不需要单独开发。

QEMU主要通过AddressSpace这个数据结构来维护内存。AddressSpace包含一个MemoryRegion类型的变量，MemoryRegion存储了一个内存块的具体信息，包括该内存块的宿主机虚拟地址、该内存块的大小以及该内存块所对应的客户机物理地址。QEMU两个主要的MemoryRegion为 `system_memory` 和 `system_io`，即系统内存和MMIO。这些内存块通过KVM_SET_USER_MEMORY_REGION这个ioctl调用传递给KVM，KVM根据传递的信息建立一个kvm_memory_slot结构体，该结构体中的 `base_gfn`、`npages`、`userspace_addr` 分别代表客户机地址、页面数以及QEMU的用户态地址，这样就建立起客户机物理地址和宿主机虚拟机地址的映射关系，KVM可以开始管理这些内存块。

#### EPT页表的构建

> 通过EPT技术，虚拟机访存时，借助客户机页表和EPT页表进行地址转换，且这种转换是由硬件自动完成的，效率极高。那么如何构建上述两种页表呢？

其中客户机页表是由客户机操作系统负责构建，而EPT页表则由KVM负责构建。和普通页表的建立过程类似，EPT页表主要通过缺页异常进行处理逐步建立的。

客户机正常运行，当遇到缺页异常时，客户机并不会退出非根操作模式，而是由客户机缺页异常处理程序进行处理，它将为客户机分配一个客户机物理内存页，并将客户机物理地址填写至客户机页表，这时候GVA和GPA的映射关系建立了，但完成一次完整的地址转换，还需要将GPA转换为HPA，因此将访问EPT页表，这时EPT页表为空，虚拟机将退出非根操作模式，并产生一个 `ept_violtion` 异常，KVM截获到该异常，将调用相应的异常处理函数，完成EPT页表的建立。KVM建立EPT页表的过程如下图所示：

![image-20231228095454059](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312280954087.png)

可以看到，虚拟机由于EPT缺页异常退出后，将由 `handle_ept_violation` 这个函数去处理该异常，该函数将调用 `tdp_page_fault` 这个函数去处理EPT缺页异常，具体的处理过程由 `gfn_to_pfn` 和 `__direct_map` 函数完成。其中：

* `gfn_pfn` 完成GPA到HPA的转换，该过程由 `gfn_to_hva` 和 `hva_to_pfn` 两个函数完成。
  * `gfn_to_hva` 通过kvm_memory_slot得到GPA到HVA的映射；
  *  `hva_to_pfn` 可完成HVA到HPA的转换，最终得到GPA到HPA的映射。

* `__direct_map` 负责建立具体的EPT页表项，EPT页表和客户机页表一样也是四级页表，流程如下：
  * `__direct_map` 将从EPT页表基地址，即第４级页表开始遍历，假如当前层是最后一层（第１级），则直接调用 `mmu_set_spte` 设置页表项，`__direct_map` 函数结束；
  * 反之假如当前层不是最后一层，且该层EPT页表项没有初始化，则调用 `kvm_mmu_get_page` 申请一个EPT页表，调用 `link_shadow_page` 新申请的EPT页表添加到未初始化的页表项，接着遍历下一级页表，直至遍历至最后一级页表（第１级）。

### KVM: 内存热迁移

> 内存的热迁移是热迁移中最耗费时间、最容易出错的地方。

内存热迁移最简单的方式，可以选择开始时就挂起虚拟机，然后拷贝所有的内存到目的宿主机上，再恢复运行虚拟机，但这样会导致停机时间较长。为了减少停机时间，我们必须在虚拟机挂起前就完成大部分内存的拷贝任务。典型思路是首先将虚拟机的所有内存拷贝过去，但在拷贝过程中，虚拟机访问内存产生脏页，为了保证虚拟机的状态一致，这部分脏页必须重新拷贝过去，随着一轮一轮的拷贝，写脏的内存页越来越少，当低于某个阀值时，挂起虚拟机，将脏页传输过去，在目的宿主机上恢复虚拟机的运行。这个思路的关键在于**如何记录哪些内存页被写脏了，**KVM为我们提供了相应的用户接口：

* 通过传递KVM_MEM_LOG_DIRTY_PAGES给这个 `ioctl(KVM_SET_USER_MEMORY_REGION)` 调用，就可以开启内存脏页记录，本质上是通过对客户机页表写保护（write protection），当客户机写内存页时，会产生write fault，从而记录脏页；
* 此外KVM还提供获取脏页位图信息的接口，即通过KVM_GET_DIRTY_LOG这个ioctl调用，我们可以获取虚拟机的的脏页位图。

## 2.4 IO虚拟化

### IO全虚拟化

KVM的IO虚拟化由QEMU和KVM模块配合完成。其中IO设备，如显示器、鼠标、键盘、磁盘、网卡、声卡、光驱等设备由QEMU在用户态进行模拟，QEMU为这些设备建立相应的数据结构，而KVM则负责拦截虚拟机的IO请求，将这些请求传递给QEMU，由QEMU去模拟IO执行，整个IO处理流程如下图所示：

![image-20231228101905230](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312281019256.png)

首先KVM虚拟机执行IO指令，如读取虚拟磁盘的某个sector，发生VM-Exit，CPU同时会将退出的原因写入VMCS结构体。KVM捕获到该IO指令，通过VMCS知道是由于IO操作引起虚拟机退出。KVM首先会在内核态遍历注册的设备，看看能否在内核态处理该请求，假如能在内核态完成处理，处理后返回虚拟机内部。若不能处理，则返回到用户态，由QEMU去模拟相关的IO请求。最后QEMU将通过KVM_RUN这个ioctl重新进入虚拟机。

### IO半虚拟化

上述传统的IO虚拟化方法，我们需要为虚拟机提供一个完整的虚拟设备。虚拟机操作系统通过标准的IO接口对虚拟设备进行操作，KVM截获IO指令，发生VM-Exit，由QEMU去模拟。这种方式的好处是，由于QEMU模拟的设备都是现在物理机已有的通用设备，因此虚拟机操作系统可以使用原先的驱动直接操作虚拟设备，但由于需要截获每一条IO请求，当虚拟机IO繁忙时，VMX模式会发生频繁的切换，导致IO性能低下。

为了解决上述问题，virtio驱动被引入，这是一种通用的半虚拟化驱动。如下图所示：

![image-20231228102347806](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202312281023861.png)

virtio主要由三部分组成：virtio前端驱动、virtio后端驱动以及virtio_ring。

* virtio前端驱动位于虚拟机操作系统内部，Linux内核默认集成了virtio前端驱动，而windows虚拟机需要手动安装virtio前端驱动；
* virtio后端驱动位于VMM中，准确来说virtio后端驱动位于QEMU中；
* virtio_ring是一个共享缓冲区，对于虚拟机操作系统和VMM来说都是可见的；

当虚拟机需要做IO（磁盘、网络等）时，通过virtio前端驱动将IO Request放入virtio_ring，虚拟机并不会立即发生VM-Exit，当IO Request达到一定数目或virtio_ring满时，虚拟机执行一次kick，虚拟机发生VM-Exit，通知VMM处理IO Requests。随后virtio后端驱动从virtio_ring取出IO请求，逐一进行模拟，完成请求后再通知虚拟机。

virtio驱动避免了虚拟机的频繁退出，性能得到显著提升。值得注意的是，virtio是一套通用框架，针对每一种外设（磁盘、网络设备），都有具体的实现。目前己经实现了磁盘 `virtio-blk`、网络设备 `virtio-net`、pci设备 `virtio-pci`、控制台设备 `virtio-console`、balloon设备 `virtio-balloon` 等，你也可以利用这套框架自己实现一个设备。

### KVM: IO热迁移

KVM虚拟机的外设是由QEMU模拟的，外设的状态都有相应的数据结构保存，因此IO的迁移比较简单，只需要保存设备的状态后，传递到目的宿主机上恢复即可，值得注意的是两类IO资源：**磁盘和网络。**

* 首先磁盘，磁盘拥有大量的数据，我们可以采用共享存储的方式，避免了磁盘数据的大量迂移，对于停机后剩余的IO请求，QEMU会完成这些IO请求后再进行设备迁移；
* 网络迁移要解决的问题是，迁移后整个局域网要迅速感知到这种变化，将数据包重定向到新的宿主机上，这个可以通过目的宿主机主动发送ＡARP，实现虚拟机IP与目的宿主机MAC地址的重新绑定；

# 3 KVM虚拟机热迁移实现

## 3.1 KVM热迁移的内容







## 3.2 KVM热迁移的具体实现









