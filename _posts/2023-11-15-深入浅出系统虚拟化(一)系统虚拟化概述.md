---
title: 深入浅出系统虚拟化(一)系统虚拟化概述

date: 2023-11-15 17:00:00 +0800

categories: [读书笔记, 深入浅出系统虚拟化]

tags: [virt, qemu, kvm]

description: 
---

[TOC]

# 0 前言

虚拟化技术是一门“古老”的新技术。早在1959年，牛津大学Christopher Strachey就提出了具有虚拟化概念的高效分时复用方案，意在解决当时大型机器使用效率低下的问题。到20世纪60至70年代，虚拟化研究进入了第一个高速发展时期，出现了以IBM CP-67/CMS为代表的大型机虚拟化技术，并提出了硬件架构可虚拟化的理论和准则（如敏感指令应属于特权指令）。但到了20世纪80年代，随着操作系统的成熟，资源管理不再以虚拟化为中心，MIPS和x86等CPU厂商出于成本和商业考虑，设计的硬件架构不再满足可虚拟化准则，形成了所谓的“虚拟化漏洞”（例如，x86有5类共17条敏感指令，但不属于特权指令）。随着个人计算机的普及，研究者提出了一系列弥补x86架构“虚拟化漏洞”的方法，代表性的技术有斯坦福大学Mendel Rosenblum等在SOSP 1997国际学术会议发表的DISCO系统，虚拟化技术重新兴起。随后，InteI和AMD等硬件厂商提出了硬件辅助虚拟化，使得x86硬件平台满足可虚拟化准则。从2006年亚马逊以虚拟机形式向企业提供IaaS（Infrastructure as a Service，基础设施即服务）平台开始，虚拟化技术成为当前支撑云计算、大数据、移动互联网和工业互联网等新型计算和应用模型的关键“根技术”。

回顾虚拟化的发展历史，可得出一些重要启示。一是基础研究对于计算机系统非常重要，不少关键技术的突破首先来自学术界和工业界的前沿研究；二是最新的技术未必用于产品，成本和市场也是重要考量（ARM和RISC-V初期也不是可虚拟化硬件架构，如ARMv6有4大类24条敏感非特权指令）。因此，深入理解虚拟化技术，把握其内在的发展规律，对于虚拟化的创新发展有重要作用。

系统虚拟化作为物理硬件层的虚拟化，在计算机硬件和操作系统之间引入一个系统软件抽象层，向下管理硬件资源，向上对操作系统提供虚拟机接口，其目标可概括为“功能不缺失、性能不损失”。因此，系统虚拟化涉及操作系统和硬件接口，技术体系比较复杂，对初学者来说是一个很大的挑战。如果不了解虚拟化技术的基本原理而去直接翻阅源代码，例如开源的QEMU/KVM，很容易抓不住主线，迷失在庞大的代码中（QEMU源代码已超过150万行）。因此，本文将对系统虚拟化的理论体系进行一个整体性概述，在后续将对各部分逐个分析。

# 1 系统虚拟化基本概念

系统虚拟化 (System Virtualization) 已成为当前支撑云计算、大数据、移动互联网和工业互联网等新型计算和应用模型的关键技术，应用于从云数据中心到边缘智能终端等不同硬件尺度的“云-网-边-端”全场景中。

虚拟化 (Virtualization) 泛指将物理资源抽象成虚拟资源，并在功能和性能等方面接近物理资源的技术，即“功能不缺失、性能不损失”。虚拟化技术广泛应用于从硬件到软件的不同计算机系统层次，例如物理内存抽象成虚拟内存，设备抽象成文件，物理显示器抽象成窗口(Window)，Java应用运行于JVM（Java Virtual Machine，Java虚拟机）。一般而言，计算机系统自下而上被划分为多个层次（见下图）。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211320730.png" alt="image-20231121131949477" style="zoom: 50%;" />

> 系统/用户ISA

硬件向上对软件提供的指令集抽象ISA（Instruction Set Architecture，指令集架构）通常分为系统ISA (System ISA)和用户ISA (User ISA)。系统ISA提供特权操作（如切换进程页表），一般在操作系统内核态运行；用户ISA提供给普通应用程序使用（如进行加法操作），一般在用户态运行。例如，RISC-V的指令集手册提供第一卷——用户态ISA(User-Level ISA)和第二卷——特权（系统）架构(Privileged Architecture)，分别对应于用户ISA和系统ISA。因此，两者合起来构成完整的硬件编程接口，即 `ISA = System_ISA + User_ISA`。

> 操作系统OS

OS（Operating System，操作系统）运行于硬件之上，向下管理硬件资源、向上提供系统调用(System Calls)接口。操作系统提供的系统调用（上层应用可以通过系统调用请求操作系统执行特权操作，即执行系统ISA）以及用户ISA共同组成了应用程序的ABI（Application Binary Interface，应用程序二进制接口）。如果两个系统的ABI相同，意味着提供了一个可移植的基础运行环境，即 `ABI = System_Calls + User_ISA`。

> 用户库Lib

库程序调用 (Library Calls) 提供系统运行时 (Runtime)、系统公共服务 (Services) 和功能丰富的第三方程序（如数学库、图形库等），运行于用户态，以函数库的形式供应用程序调用。同时，应用程序一般通过库函数调用操作系统的系统调用接口。因此，对于应用程序而言，底层的系统ISA和操作系统的系统调用是透明的，它主要关心运行环境（包括运行时、服务和库等）提供的API（Application Program Interface，应用程序编程接口）。也就是说，确定了应用程序所使用的用户ISA和所调用的库函数（包括动态库和静态库），也就确定了应用程序的用户态编程接口，即 `API = Library_Calls + User_ISA`。

因此，如下图所示，硬件和软件资源在不同层次的抽象对应不同的虚拟化方法。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211321694.png" alt="image-20231121132032111" style="zoom: 50%;" />

> 物理硬件层的虚拟化

虚拟机监控器Hypervisor通过直接对硬件资源进行抽象和模拟，提供了虚拟硬件ISA(Virtual ISA)接口，包括虚拟系统ISA(Virtual System ISA)和虚拟用户ISA(Virtual User ISA)，即Virtual ISA=Virtual System ISA+Virtual User ISA。Hypervisor有时也称为VMM（Virtual Machine Monitor，虚拟机监控器），除非专门列出（例如Rust-VMM是用Rust语言编写的用户态虚拟机监控程序），本书统一采用Hypervisor表述VMM。由于虚拟化技术的多样性，操作系统还可以进一步细分为宿主机操作系统(Host OS)和客户机操作系统(Guest OS)，这将在后文中详细介绍。虚拟硬件ISA提供了完整的硬件资源抽象，从而使多个操作系统能够运行其上并共享硬件资源，例如VMware虚拟化产品VMware Workstation可以在Windows系统上运行一个Linux操作系统。

> 操作系统层的虚拟化

操作系统本身就是虚拟化技术的一种体现，它将CPU、内存和I/O等资源进行了抽象，最终以进程为单位使用这些资源，并进一步由命名空间 (Namespace) 和控制组 (Cgroups) 等容器 (Container) 技术实现对资源的隔离和限制。因此这一层次的虚拟化也称为轻量级虚拟化、进程虚拟化。有些文献把基于二进制翻译的方法运行相同或不同ISA的二进制程序称为进程虚拟化，例如在基于x86的Windows系统上运行ARM架构的程序。本书采用的术语容器虚拟化主要指基于同构指令集的轻量级虚拟化方法，它提供了虚拟ABI (Virtual ABI) 接口，即 Virtual ABI=虚拟系统调用(Virtual System Calls)+Virtual User ISA。近年比较流行的Docker技术便属于容器虚拟化方法。

> 应用层程序运行环境的虚拟化

应用程序本身作为虚拟机（也称为沙箱，Sandbox），提供用户态虚拟运行时 (Virtual Runtime) 支撑应用程序的运行，即虚拟API接口=虚拟库程序调用(Virtual Library Calls)+Virtual User ISA。比较常见的是高级编程语言的虚拟运行环境，例如Java虚拟机和Python虚拟机。注意，这里的用户态ISA有时采用高级编程语言自定义的字节码(Bytecode)抽象，用于屏蔽各个硬件指令集的差异，能够跨平台运行（最终会将字节码编译到某个具体硬件架构的用户态ISA）。

以上是三种基本的虚拟化抽象方法，根据指令集、操作系统是否同构等不同存在多种变体。例如库函数虚拟化：通过拦截用户态程序的系统调用，随后对调用进行仿真和模拟，能够在同一个硬件指令集上运行不同操作系统的程序。例如，Wine基于Linux等操作系统运行Windows的应用软件（通过提供兼容层将Windows的系统调用转换成与Linux对应的系统调用）。从ABI角度分析Wine，用户ISA同构（均为x86指令集），但系统调用异构（分别为Windows与Linux）。

近年日益流行的轻量级Unikernel是一种定制的库操作系统(Library OS)。Unikernel和应用程序一起编译打包，构成单地址空间的可执行二进制镜像，不再严格区分用户态和内核态。因此，Unikernel可以直接运行在硬件或Hypervisor上，就硬件接口而言，Unikernel的CPU和内存部分仍为（高度简化的）虚拟硬件ISA，而I/O部分则基本使用virtio或其他半虚拟化方案，降低了传统虚拟化方法由于逐层软件抽象而引入的性能开销，提供了轻量、高效的虚拟化抽象。

**本文主要介绍系统虚拟化，也就是上述物理硬件层的虚拟化，**特指在计算机硬件和操作系统之间引入一个系统软件抽象层，由其实际控制硬件，并向操作系统提供虚拟硬件ISA接口，从而可以同时运行多个“虚拟机”(Virtual Machine)。目前指令集同构的系统虚拟化是主流方法，因此本文着重介绍同构虚拟化方法。此外，容器虚拟化与操作系统和硬件密切相关，也是目前大规模使用的云计算的核心技术，本文将在后续简要介绍容器虚拟化方面的内容。应用层程序运行环境的虚拟化涉及与硬件平台无关的字节码和沙箱等技术，限于篇幅，不在本文讨论范围之内。

综上所述，系统虚拟化将物理机器抽象成虚拟机器，其抽象粒度包括CPU、内存和I/O在内的完整机器。从本质上来说，系统虚拟化抽象了硬件指令集架构，向上对操作系统提供了硬件特性的等效抽象。对操作系统而言，运行于虚拟机上与运行于物理机上没有区别，即虚拟机可视为真实物理机的高效并且隔离的复制品 (A virtual machine is taken to be an efficient,isolated duplicate of the real machine)。**因此，系统虚拟化的目标是使虚拟机的功能和性能等与物理机接近，即“功能不缺失、性能不损失”。**

此外，根据抽象类型的不同，系统虚拟化可以分为“一虚多”（见下图(a)）和“多虚一”（见下图(b)）。早期的系统虚拟化主要研究“一虚多”，即把单物理机抽象成若干虚拟机（如一个物理机可以抽象成10个虚拟机）。之后，跨越数量级的硬件性能提升为“多虚一”架构开辟了道路，即把多物理机抽象成单一虚拟机。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211323706.png" alt="image-20231121132134406" style="zoom:50%;" />

虚拟化技术通过对物理硬件在数量、功能和效果上进行逻辑化虚拟，能够提供高层次的硬件抽象、硬件仿真、服务器整合(Server Consolidation)、资源按需调配、资源聚合、柔性管理、在线迁移、系统高可用、安全隔离等功能，因此虚拟机也成为当前各类硬件平台上的主要载体。虚拟化能大幅提升资源利用率，因为信息系统是按照最大使用峰值设计的，利用率一般不到20%。采用虚拟化技术可以整合资源、削峰填谷和节能降耗，例如电信业信息系统虚拟化后，利用率能够提升到70%以上。根据VMware的报告，VMware vSphere可实现10∶1（10台虚拟机运行于1台物理机之上）或更高的整合率，将硬件利用率从5%~15%提高到80%以上。另外一个典型案例是，2007年《纽约时报》想把从1851年创刊开始到1922年的文章上传到网络，以供免费阅读和搜索，原预计费时数月及花费上百万美元，但租用亚马逊虚拟机，在24小时内将1851—1922年1100万篇文章的报纸扫描件整理成PDF，仅花费240美元。

综上所述，**系统虚拟化是把物理硬件资源通过 “一虚多/多虚一” 抽象成虚拟资源，并使得性能尽可能接近物理资源。**同时，与物理资源类似，虚拟资源的四大特性（可靠性、可用性、可维护性和安全性）也是系统虚拟化的重要技术指标，在后文中将会逐步对此展开详细介绍。自2009年起，全球新增的虚拟机数量已超过新增的物理机。因此，系统虚拟化技术成为当前支撑云计算、大数据、互联网和机器学习等新型计算和应用模型的核心技术之一。

# 2 系统虚拟化的发展历史和趋势展望

系统虚拟化与计算机硬件的发展息息相关，并且随着计算机软硬件技术的发展而不断完善。至今，虚拟化技术已经成为系统软件的核心技术。为了更好理解虚拟化的基本概念和技术演变，本节将简要叙述虚拟化技术的发展历史，并展望未来的发展趋势。

## 2.1 发展历史

### 早期虚拟化探索

虚拟化技术的发展历史与计算机技术一脉相承。早在1959年，牛津大学的计算机教授Christopher Strachey在信息处理国际大会(International Conference on Information Processing)发表了论文Time sharing in large fast computers，针对当时大型机器使用效率低下的问题（资源利用率低到现在也没有得到很好的解决，如何提升资源利用率贯穿整个计算机的发展历史），提出了具有虚拟化概念的高效分时复用方案，原文为：

> “各种程序和外设竞争获得控制权（分时共享），并竞争获得存储的使用权（空间共享）”（There would be various programs and pieces of peripheral equipment competing both for the use of control(time sharing)and also for the use of the store(space sharing)），这被认为是虚拟化技术的最早表述。

论文中提出的Director监控程序被放置在一个可快速访问并且不可删除的存储介质上（保证了自身运行的高效性和完整性）。Director提供了一个隔离的运行环境，控制各个程序的分时运行，并确保各个程序无法相互干扰。这里的Director可以看作 “一虚多” Hypervisor的雏形。同时，论文提出了虚拟机的概念（包括CPU和存储的完整状态），原文为：

> “多个操作（程序）同时在物理机上运行，似乎跑在单独的机器上（虚拟机），但每个虚拟机比实际的物理机小一些、慢一些”（In this way，during the normal running of the machine several operators are using the machine during the same time. To each of these operators the machine appears to behave as a separate machine(smaller,of course,and slower than the real machine)）。

由此可见，虚拟化技术提供的“弹性”和“隔离”概念已经孕育于早期的系统设计中。

因此早期的资源虚拟化主要研究 “一虚多”，即把单物理资源抽象成若干虚拟资源（如1个物理CPU可抽象成512个虚拟CPU）。在计算机技术发展的早期阶段，尚在探索对于计算资源的时分复用，以便多人能通过终端同时使用一台计算机，分享昂贵的计算资源，提高计算机的资源利用率。

1965年IBM M44/M44X实验项目实现了在同一台主机M44上模拟多个7044系统，突破了部分硬件共享(Partial Hardware Sharing)、分时共享(Time Sharing)和内存分页(Memory Paging)等关键技术，并首次使用术语“虚拟机”。这里模拟的虚拟机M44X并非完全与主机相同，与现在Xen采用的半虚拟化技术异曲同工。在此基础上，IBM研发了一系列虚拟化技术，如CP-40/CMS、CP-67/CMS和CP370/CMS等，构成了完整的CP/CMS大型机虚拟化系统软件，一直延续到后来的IBM虚拟化软件z/VM。

20世纪60年代中期，IBM的CSC（Cambridge Scientific Center，剑桥科学中心）开发了CP-40/CMS，CP-40是虚拟机控制程序(Virtual Machine Control Program)，其开发目标主要有4个：

> 1. 研究资源的分时共享；
> 2. 评估硬件对分时共享的需求；
> 3. 开发内部使用的分时共享系统；
> 4. 评估操作系统与硬件的交互行为。

CP-40提供的虚拟机数量最多为14个，是第一个支持全硬件虚拟化 (Full Hardware Virtualization) 的系统。CP-40的后继系统CP-67于1967年开发，其大小为80KB，可以运行于IBM的System/360-67大型机，提供多个虚拟的System/360机器实例，并可以通过CPREMOTE服务访问远程硬件资源，支持OS/360、DOS和RAX等客户机操作系统。此外，CP-67还有配套开发的专用客户机操作系统CMS（Cambridge Monitor System，剑桥监控系统），是一个不支持并发的单用户系统，以减小开销，其思想可以与今日的Unikernel类似。

CP-67/CMS这套系统组合甚至比大家熟知的Multics和UNIX系统出现更早，但以虚拟内存和抢占式调度为首要特点的多用户操作系统很快占据了学术界和工业界的主流，而在很长一段时间内，虚拟化技术都没有引起人们足够的重视。因此早期虚拟化技术主要用于大型机，以至于后来设计x86指令集时就没有考虑虚拟化的需求。面向嵌入式和PC（Personal Computer，个人计算机）市场，虚拟化的性能开销也是其重要限制因素。

### 虚拟化技术的普及

20世纪60至80年代，虚拟化技术主要用于大型主机。随着x86平台日益流行，特别是微软与英特尔的Wintel联盟主导全球PC市场，基于Linux的x86服务器也逐步侵蚀大型主机和小型机的市场份额，由此产生了对x86平台虚拟化的迫切需求，也推动了虚拟化技术的广泛普及。

20世纪80年代中期，Rod MacGregor创建的Insignia Solutions公司开发了x86平台的软件模拟器SoftPC，能在UNIX工作站上运行MS-DOS操作系统，1987年SoftPC被移植到苹果的Macintosh平台，并可以运行Windows操作系统。此后，一系列以软件模拟为主的PC虚拟化系统陆续发布，例如苹果开发的Virtual PC和开源的Bochs。

1997年，斯坦福大学Mendel Rosenblum教授等在计算机系统领域的重要国际会议SOSP（Symposium on Operating Systems Principles，操作系统原理研讨会）上发表了DISCO。DISCO运行在具有共享内存的可扩展多处理器系统上，可以同时运行多个客户机操作系统。不同于SoftPC这样的软件模拟器，DISCO基于全虚拟化(Full Virtualization)技术，大部分用户态指令是直接运行在物理CPU上，使得虚拟化的开销大为降低。基于该原型系统，1998年Mendel Rosenblum教授等成立了著名的VMware公司，该公司也是目前x86虚拟化软件的主流厂商之一。1999年，VMware推出了广受欢迎的桌面虚拟化产品VMware工作站 (VMware Workstation)，也间接推动了x86虚拟化技术的普及。

2003年，剑桥大学Ian Pratt等学者在SOSP上发表了基于PV（Para-Virtualization，半虚拟化）技术的代表性开源系统Xen。与全虚拟化技术不同，半虚拟化技术需要修改客户机操作系统。通过客户机操作系统主动调用超调用(HyperCall) 的虚拟化管理接口，能够进一步降低虚拟化的开销，并且由于Xen是开源系统，与Linux内核密切配合（半虚拟化技术需要修改操作系统，开源内核修改比较方便），Xen得到了广泛的应用，例如早期的亚马逊云计算平台EC2（Elastic Compute Cloud，弹性计算云）就是基于Xen构建的（EC2第一个采用半虚拟化的实例类型是m1.small）。

同年，法国天才程序员Fabrice Bellard发布了至今仍然使用的开源虚拟化软件QEMU（Quick EMUlator，快速仿真器），采用动态二进制翻译(Binary Translation)技术，能够支持多源多目标的跨平台仿真执行异构指令集，例如可以在x86平台上仿真运行ARM进程。QEMU是运行在用户态的仿真器，它不仅可以仿真异构指令集，还可以仿真整个机器，通过开源社区的不断改进，QEMU已成为目前使用最为广泛的开源软件仿真器。

自2005年，x86硬件厂商开始从体系结构层面解决早期x86架构不符合虚拟化准则的问题（也称为“虚拟化漏洞”），避免纯软件全虚拟化方式（如二进制翻译）带来的性能、安全和可靠性的缺陷。Intel和AMD相继发布了基于硬件辅助的全虚拟化技术（Hardware-assisted Full Virtualization）。Intel的硬件辅助VT（Virtualization Technology，虚拟化技术）和AMD的类似技术SVM（Secure Virtual Machine，安全虚拟机）扩展了传统的x86处理器架构，客户机操作系统不用修改源码就可以直接运行在支持虚拟化技术的x86平台上，原来由软件模拟的大量特权操作直接由硬件执行，基本达到了物理CPU和内存的性能，虚拟化开销大为减少。

---

2006年，AWS（Amazon Web Services，亚马逊云服务）发布了S3（Simple Storage Service，简单存储服务）和EC2，开始以虚拟机形式向企业提供IT（Information Technology，信息技术）基础设施服务（基于开源Xen搭建），开启了以IaaS（Infrastructure as a Service，基础设施即服务）为代表性技术的云计算时代。

2007年2月，Linux Kernel 2.6.20中加入了以色列公司Qumranet开发的基于硬件辅助虚拟化的内核模块KVM（Kernel-based Virtual Machine，基于内核的虚拟机）。KVM由虚拟化领域另外一个天才程序员Avi Kivity带领开发，并于2006年10月19日首次在Linux内核社区发布，发布不到半年时间，Linux就将其正式纳入内核官方版本，由此也可以看出KVM对Linux开源社区的重要性和迫切性。Avi Kivity等提出的KVM充分遵循UNIX系统的设计原则，仅实现Linux内核态的虚拟化模块，用户态部分由上述的开源QEMU实现。由此，**QEMU/KVM的虚拟化技术组合日益流行，成为目前主流的开源系统虚拟化方案，被各大虚拟化和云计算厂商采用。**

基于KVM/QEMU的系列年度技术论坛（例如KVM Forum）也成为虚拟化技术的重要权威论坛，从2007年开始，KVM/QEMU的重要技术进展和年度汇报均发布于KVM Forum，是Linux虚拟化技术发展的一个窗口。近年来，国内华为、阿里和腾讯等虚拟化技术方面的专家也积极参加KVM Forum，凸显国内对虚拟化技术的贡献日益增加。

### 虚拟化技术蓬勃发展

从2010年开始，虚拟化技术不断扩宽应用场景，在移动终端、嵌入式和车载设备等资源受限的平台也开始被引入。同时，新型硬件虚拟化、“多虚一”虚拟化和轻量级容器虚拟化也取得了长足的进展，下面分别展开介绍。

> 新型硬件虚拟化

近年来大量新型硬件得到迅速普及，例如拥有数千个核的GPU（Graphics Processing Unit，图形处理单元）、具有RDMA（Remote Direct Memory Access，远程内存直接访问）功能的高速网络、支持硬件事务内存和FPGA（Field Programmable Gate Array，现场可编程门阵列）加速的CPU处理器等。以计算能力为例，CPU/GPU/FPGA不断延续摩尔定律。自2005年Intel首次提供了针对CPU的硬件辅助VT-x（Virtualization Technology for x86，x86虚拟化技术）后，硬件辅助虚拟化成为主流的“一虚多”虚拟化方法。目前，基于硬件辅助的虚拟化方法在CPU、内存和网络等传统的硬件资源上获得了成功，特别是CPU和内存虚拟化性能已经接近物理性能。新型异构设备（如FPGA、GPU）逐步成为大数据和人工智能等专用计算系统的核心要素，是算力输出的重要甚至主要部分。但这些新型设备要么没有虚拟化，要么处于虚拟化的早期水平，导致云无法享受“新硬件红利”。直到2014年，各大厂商才提出了以Intel gVirt、GPUvm为代表的硬件辅助虚拟化方案，但该方案远未成熟。

**下面以GPU为典型代表介绍新型硬件虚拟化的发展历史。**

现代GPU的功能已经从原来的图形图像计算扩展到了视频编解码、高性能计算，甚至是GPGPU（General-Purpose Graphics Processing Unit，通用图形处理单元）。但GPU这类重要资源虚拟化的高性能、可扩展性和可用性相对于CPU仍处于滞后的阶段，例如2014年Intel的GPU虚拟化解决方案gVirt中，单个物理GPU仅支持3个vGPU（Virtual GPU，虚拟GPU），而同年发布的Xen 4.4已支持512个vCPU（Virtual CPU，虚拟CPU）。直到2016年，亚马逊等各大云服务提供商才陆续推出了商业化的GPU实例。

**传统GPU虚拟化通过API转发 (API Forwarding) 的方式将GPU操作由虚拟机发送到Hypervisor代理执行，**该方法被大量主流虚拟化产品所采用来支持图形处理，但并非真正意义上的完整硬件虚拟化技术，其性能和可扩展性均无法满足GPGPU等应用（如机器学习）的需要。

GPU虚拟化的软件模拟采用类似于CPU虚拟化中二进制转换方法。但相对于CPU，GPU的特性复杂，不同的设备提供商之间的GPU规格区别很大，GPU的资源很难被拆分，模拟的效率低。因此，典型的QEMU软件仅模拟了VGA设备的基本功能，它通过一个半虚拟化的图像缓冲区加速特定的2D图像访问，不符合高效、共享的虚拟化要求。

GPU虚拟化的设备直通 (Passthrough) 方法将物理GPU指定给虚拟机独占访问。设备直通有时也称为设备透传技术，直接将物理设备，通常是PCI（Peripheral Component Interconnect，外设组件互连）设备，配置给虚拟机独占使用（其他虚拟机无法访问该物理设备）。与API转发提供的良好GPU共享能力相比，设备直通方法通过独占使用提供了优异的性能。例如，基于Intel的VT-d/GVT-d技术，通过翻译DMA（Direct Memory Access，直接内存访问）所访问内存地址的方法将GPU分配给一个虚拟机使用，能够达到与原生物理GPU相近的性能，但牺牲了共享特性。NVIDIA的Tesla GPU也提供了类似的虚拟化方案Grid，虚拟机可以通过硬件直通的方式直接访问物理GPU。

在GPU虚拟化方面，由于架构复杂，既要支持常规显卡，又要支持GPGPU，直到2014年才发表了硬件支持的GPU全虚拟化方案（Intel早在2005年已增加对CPU虚拟化的硬件支持），即Intel提出的产品级开源方案gVirt和学术界提出的方案GPUvm（均发表于USENIX ATC 2014）。gVirt是第一个针对Intel平台的GPU全虚拟化开源方案，为每个虚拟机都提供了一个虚拟的GPU，并且不需要更改虚拟机的原生驱动。此后，在高性能、可扩展和可用性三个重要方面提出了一系列改进（如gHyvi、gScale和gMig等），为GPU虚拟化的广泛应用打下了良好基础。

> “多虚一” 虚拟化

单台物理主机已经能够拥有超过数百个CPU核、数千个GPU核、TB级内存以及超过100Gbps的网络带宽的硬件环境，由此产生了在单机上构建巨大规模/巨型虚拟机的迫切需求。资源扩展主要有资源横向扩展(Scale out)和资源纵向扩展(Scale up)两种方式。虚拟化资源横向扩展的好处是弹性分配、资源灵活使用且利用率高，但编程模型复杂；纵向扩展的好处是编程模型简单，避免了由于分布式系统和数据分区产生的软件复杂性，但硬件昂贵、灵活性差。针对内存计算等大规模计算需求，是否可以综合利用资源横向和纵向扩展的优势？通过虚拟化技术，可以在虚拟化层面聚合资源，使底层资源对上层客户机操作系统透明。

不同于传统的“一虚多”方法，**这种 “多虚一” 的跨物理节点虚拟化架构可以将计算资源、存储资源和I/O资源虚拟化，构建跨节点的虚拟化资源池，向上对客户机操作系统提供统一的硬件资源视图，并且无须修改客户机操作系统。**目前典型的跨节点虚拟化产品有以色列公司的ScaleMP和美国公司的TidalScale。其中TidalScale提出了一种软件定义服务器，通过超内核构建虚拟资源池，将主板上所有的DRAM（Dynamic Random Access Memory，动态随机存取存储器）内存抽象为虚拟化的L4缓存，并且引入了虚拟主板提供跨节点的虚拟设备连接，通过虚拟通用处理器、虚拟内存和虚拟网络构建虚拟资源池，并且资源可以迁移，用户可以灵活使用资源。通过复杂的缓存一致性算法和缓存管理算法，有效提升了性能。在一个包含1亿行、100列的数据库和128GB内存的服务器上，由于所需内存容量大于单个服务器硬件的内存容量，导致分页 (Paging) 频繁发生，以至于一个应用程序需要花费7小时才能完成MySQL的三次查询作业。这个现象称为 “内存悬崖(Memory Cliff)”，即由于应用所需内存超过服务器内存导致性能急剧下降。而该查询运行在采用两个96GB内存节点组成TidalScale服务器上只需要7分钟，性能提升60倍。

但ScaleMP和TidalScale都基于特定硬件和定制化闭源Type Ⅰ虚拟化平台。Type Ⅰ虚拟化在裸机上运行Hypervisor，然后加载客户机操作系统，技术生态和应用范围受限制，无法和主流的Type Ⅱ开源虚拟化系统KVM/QEMU兼容。不同于Type Ⅰ虚拟化，Type Ⅱ虚拟化是在宿主机操作系统上运行Hypervisor，然后加载客户机操作系统，是目前主流的虚拟化方法，**如KVM/QEMU被全球最大的亚马逊云服务和国内最大的阿里云等采用。**

基于KVM/QEMU，GiantVM架构围绕高速网络RDMA技术实现虚拟化层面的硬件聚合和抽象，是目前首个开源的Type Ⅱ“多虚一”架构。GiantVM以Libvirt为客户虚拟机上层接口，分布式QEMU提供跨节点虚拟机抽象，KVM为下层物理机提供管理接口，基于RDMA提供低延迟DSM（Distributed Shared Memory，分布式共享内存）。目前GiantVM可以将八台服务器虚拟成一台虚拟机，支持的客户机操作系统有Linux和瑞士的苏黎世联邦理工学院(ETH)Timothy Roscoe教授等提出的多内核操作系统Barrelfish。关于GiantVM的相关实现技术，将在后文进行介绍。

由于跨节点通信一般为亚微秒级，与常规内存访问有数量级差距（纳秒级），**虚拟化聚合“多虚一”的技术挑战是由于分布式共享内存同步需要跨节点通信，协议同步开销是性能损失的主要来源。**传统方法在DSM同步中引入了普林斯顿大学李凯教授等提出的基于顺序一致性(Sequential Consistency)的IVY协议，该协议的编程模型简单，但严格同步性能开销大。如何进一步降低DSM引入的“多虚一”性能开销是目前“多虚一”技术面临的重要技术挑战。

> 轻量级容器虚拟化

容器虚拟化技术最早可以追溯到1979年UNIX V7引入的chroot(change root)系统调用，通过将一个进程及其子进程的根目录（root目录）改变到原文件系统中的不同位置（虚拟根目录），使得这些进程只能访问该虚拟根目录下的文件和子目录。因此chroot为每个进程提供了相互隔离的虚拟文件系统（称为chroot Jail），被认为是轻量级进程隔离方法的雏形，标志着容器虚拟化的开始。

2000年发布的FreeBSD Jails比chroot提供了更完善的进程级容器（称为Jail）运行环境。每个Jail容器拥有各自的文件系统（基于chroot，并修正了传统chroot的安全漏洞）和独立的IP（Internet Protocol，网际协议）地址，对进程间通信也加以限制。因此，Jail容器中进程无法访问Jail之外的文件、进程和网络。此后，还发布了与FreeBSD Jails类似的进程隔离技术，如2001年的Linux VServer、2004年的Solaris Containers和2005年的Open VZ。

**2006年谷歌公司发布了进程容器 (Process Containers)，提供了一系列进程级资源（包括CPU、内存和I/O资源）隔离技术，后来改名为控制组(Cgroups)，并被合入Linux内核2.6.24。Cgroups技术沿用至今，也是目前的核心容器技术之一。**

2008年Linux社区整合Cgroups和命名空间(Namespace)，提出了完整的容器隔离方案LXC（Linux Containers，Linux容器）。LXC通过Cgroups隔离各类进程且同时控制进程的资源占用，并通过Namespace使每个进程组有独立的进程标识PID、用户标识、进程间通信IPC和网络空间，两者构成了完整的进程级隔离环境。**容器之所以被称为轻量级虚拟化技术，是因为LXC基于同一个宿主机操作系统，仅在操作系统层次通过软件隔离出类似虚拟机的效果（“欺骗”进程，使其认为自身运行在不同机器上），不需要虚拟整个ISA（“欺骗”操作系统，使其认为自身运行在物理机上）。因此容器虚拟化的缺点是只支持相同宿主机操作系统上的隔离，而系统虚拟化提供异构客户机操作系统的隔离，两者提供了不同层次的隔离，互为补充。**

2013年dotCloud公司（后更名为Docker）发布了基于LXC的开源容器引擎Docker，引入了分层镜像(Image)的概念，基于只读的文件系统提供容器运行时所需的程序、库和环境配置等，容器则作为一个轻量级应用沙箱，提供应用所需的Linux动态运行环境（包括进程空间、用户空间和网络空间等）。Docker引擎基于容器运行和管理各个用户态应用实例。Docker通过组合只读的静态镜像和可写的动态容器，部署方便且动态隔离，提供一整套容器的创建、注册、部署和管理的完整工具集。因此Docker问世后便迅速普及，成为容器技术的代名词。

2014年6月谷歌公司开源了基于容器的集群管理平台Kubernetes（简称K8S，名字来源于希腊语“舵手”）。Kubernetes是基于谷歌内部使用的大规模集群管理系统Borg实现的，其核心功能是自动化管理容器，解决大规模容器的编排、管理和调度问题。Kubernetes现已成为容器编排领域的事实标准，得到了广泛的应用。

此外，随着云原生(Cloud Native)和无服务器(Serverless)架构的日益成熟，更细粒度的FaaS（Function as a Service，函数即服务）将迎来更大的发展。虚拟机、容器和云函数作为资源抽象的不同层次，也会互为补充、相得益彰。

## 2.2 趋势展望

自2005年Intel首次提供了针对CPU的硬件辅助虚拟化技术VT-x后，硬件辅助虚拟化已经成为主流的虚拟化方法，在CPU、内存和网络等资源配置上获得了成功。自2014年人们开始研究新型异构设备，新型硬件设备虚拟化抽象效率进一步得到提升。目前新型的虚拟化架构（如亚马逊的Nitro架构、阿里云的神龙服务器和华为的ZERO架构）都基于单节点弹性资源（“一虚多”）横向扩展架构，然而随着新型计算、存储和网络硬件的不断出现（例如，Optical Fabric网络带宽为400Gbps且延迟仅为100ns，与内存总线的带宽差距缩小到一个数量级以内），虚拟化架构也在不断演进。Intel首席架构师Raja Koduri提出：“晶体管尺寸每缩小1/10，就会衍生出一种全新的计算模式”。跨**越数量级的硬件性能提升使得当初设计虚拟化系统的两大基本假设发生变化，孕育出虚拟化新架构，**具体体现如下。

> 基本假设一

单节点资源以CPU为中心。新型异构硬件作为CPU的附属设备，由CPU集中化控制，造成频繁上下文切换，不仅增加了虚拟化开销（例如GPU与CPU的虚拟化页表同步占据了24.43%~79.45%不等的开销），同时设备与CPU紧耦合也限制了新型异构硬件的可扩展性。当前硬件平台趋向于异构化和去CPU中心化，GPU、FPGA等新型硬件成为算力输出的重要甚至主要部分，例如单个Nvidia V100 GPU的性能可能是CPU性能的100倍。单台服务器可能同时配置CPU与GPU/FPGA处理器、具有RDMA特性的InfiniBand网卡、普通内存和NVM内存以及SSD等外存，同时，Gen-Z、CCIX等新的缓存一致互联协议可以让新型去中心化分散硬件资源 `Disaggregated Hardware Resource`（例如GPU、FPGA以及其他异构加速硬件）相互通信，并独立发起资源访问请求与CPU解耦。因此，各个分散硬件资源通过高速总线和网络相互通信，并具备一定的自主性，能够独立访问其他硬件资源，导致了**分散硬件资源的去CPU中心化。**

> 基本假设二

跨节点资源以横向扩展为主。早期由于硬件性能的限制，倾向于将各个节点虚拟化后在客户机操作系统上构建分布式系统，将计算和数据进行分区，虚拟化架构以多节点横向扩展为主。近年来，随着硬件性能跨数量级提升，出现了纵向扩展的硬件资源聚合趋势，例如内存聚合有Infiniswap［NSDI 2017（学术会议论文）］、Remote Regions[ATC 2018]、Leap[ATC 2020]、Fastswap[EuroSys 2020]、Semeru[OSDI 2020]和AIFM[OSDI 2020]等，I/O资源聚合有ReFlex[ASPLOS 2017]、PolarFS[PVLDB 2018]等。因此，**利用新型硬件实现多硬件多特性的虚拟化聚合与抽象，提升硬件性能甚至突破单一硬件的物理极限，**也是目前学术界和工业界的重要探索方向。

---

这两个基本假设的改变对虚拟化和系统软件的发展产生了深远影响。目前云业务对算力的需求越来越高，单个服务器往往已经无法满足算力需求，形成了机架规模 (Rack-Scale) 的分散式可组合基础设施。目前内存遭遇12nm的工艺瓶颈，工艺设计成本约占整体的2/3，新型持久性内存（如Intel傲腾）填补了内存纵向扩展的空白，高速网络互连使得远程内存访问不再是障碍，以内存为中心的系统虚拟化新架构正在兴起（如MemVerge分级大内存可以将深度学习性能提升20倍）。

2017年惠普公司基于Gen-Z总线，推出了The Machine原型系统，以内存语义访问远程NVM存储池，构建了160TB的共享内存池，并支持多种异构加速硬件共享该大内存资源。IBM公司在2020年8月发布的IBM POWER 10处理器借助Memory Inception突破性新技术，允许集群中任何基于IBM POWER10处理器的服务器跨节点访问和共享彼此的内存，从而创建PB级聚合内存资源，最多支持2PB内存，并且节点之间的延迟为纳秒级。

OSDI 2018最佳论文中提出了LegoOS分布式操作系统，将单一内核分解成独立的去中心化CPU、内存和I/O组件，并通过高速网络连接进行消息通信，在操作系统层面进行了前沿性探索。加州大学洛杉矶分校(UCLA)、麻省理工学院(MIT)等大学的学者在OSDI 2020论文中提出了基于分散内存(Disaggregated Memory)的Java运行时管理方法Semeru，能够通过统一的虚拟地址空间跨节点透明访问多个服务器的内存。而在虚拟化软件层面，高性能计算领域的先驱戈登·贝尔(Gordon Bell)等提出了硬件资源虚拟化聚合的方法，试图同时实现横向扩展的硬件成本优势和灵活性，以及纵向扩展固有的简单性。

因此，通过虚拟化构建弹性资源池（计算、存储和I/O相结合），实现纵向/横向灵活扩展，资源按需聚合/分散，面向领域垂直整合巨型虚拟机/微型虚拟机，能够便捷共享集约化硬件资源，高效抽象具有多样性的硬件设备，通过虚拟化提供更为轻量化、细粒度的资源和接口抽象，现有的硬件、操作系统、应用程序生态可以继续演化，实现软件和硬件的松耦合及协同优化是虚拟化技术不断发展的方向。

# 3 系统虚拟化的主要功能和分类

系统虚拟化向下管理硬件资源，向上提供硬件抽象。本节主要介绍系统虚拟化的基本功能（包括CPU、内存和I/O虚拟化），并根据Hypervisor与物理资源和操作系统交互方式的不同，介绍了两种基本的虚拟化分类。然后简要介绍三种虚拟化的实现方式，从而让大家在整体上了解虚拟化不同实现方式对功能和性能的影响。

## 3.1 虚拟化基本功能

系统虚拟化架构如下图所示（以经典的“一虚多”架构为例）：

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211325924.png" alt="image-20231121132455420" style="zoom: 50%;" />

底层是物理硬件资源，包括三大主要硬件：CPU、内存和I/O设备。Hypervisor运行在硬件资源层上，并为虚拟机提供虚拟的硬件资源。客户机操作系统 (Guest OS) 运行在虚拟硬件资源上，这与传统的操作系统功能一致，管理硬件资源并为上层应用提供统一的软件接口。总览整个架构，**Hypervisor应当具有两种功能：①管理虚拟环境；②管理物理资源。**

* **虚拟环境的管理**
  * Hypervisor需要为虚拟机提供虚拟的硬件资源，因此至少应当包括三个模块：①CPU虚拟化模块；②内存虚拟化模块；③I/O设备虚拟化模块；
  * Hypervisor能够允许多个虚拟机同时执行，那么应当具有一套完备的调度机制来调度各虚拟机执行。考虑到一个虚拟机可能具有多个vCPU，每个vCPU独立运行，那么Hypervisor调度的基本单位应当是vCPU而非虚拟机；
  * 在云环境下，用户有时需要查询虚拟机的状态或者暂停虚拟机的执行，这也通过Hypervisor实现。为了便于用户管理自己的虚拟机，云厂商往往会提供一套完整的管理接口（例如Libvirt）支持虚拟机的创建、删除、暂停和迁移等。

* **物理资源的管理**
  * 除了管理虚拟环境，在某种程度上，Hypervisor还担负着操作系统的职责，即管理底层物理资源、提供安全隔离机制，以及保证某个虚拟机中的恶意代码不会破坏整个系统的稳定性。

此外，一些Hypervisor还提供了调试手段和性能采集分析工具（例如KVM单元测试工具 `KVM-Unit-Tests`），便于开发者进行测试与评估，例如可以利用Linux提供的 `Perf` 工具查看由各种原因触发的VM-Exit（虚拟机退出）次数。

CPU、内存和I/O设备是现代计算机所必需的三大功能部件。如果系统虚拟化要构建出可运行的虚拟机，**CPU虚拟化、内存虚拟化和I/O虚拟化**是必要的，它们各自需要实现的功能与解决的问题简要概括如下：

> CPU虚拟化

在物理环境下，操作系统具有最高权限级别，可以直接访问寄存器、内存和I/O设备等关键的物理资源。但是，在虚拟环境下，物理资源由Hypervisor接管，并且Hypervisor处于最高特权级。这也就意味着客户机操作系统处于非最高特权级，无法直接访问物理资源。因此虚拟机对物理资源的访问应当触发异常，陷入Hypervisor中进行监控和模拟。

这些访问物理资源的指令称为敏感指令，上述处理方式称为**陷入-模拟 (Trap and Emulate)。**在Intel VT-x等硬件辅助虚拟化技术出现之前，软件虚拟化技术只能利用计算机体系中现有的特权级进行处理。这就要求所有的敏感指令都是特权指令，才能让Hypervisor截获所有的敏感指令。但系统中通常存在一些敏感非特权指令，它们称为 “虚拟化漏洞”，CPU虚拟化的关键就是消除虚拟化漏洞。

此外，中断和异常的模拟及注入也是CPU虚拟化应当考虑的问题。虚拟设备产生的中断无法像物理中断一样直接传送到物理CPU上，而需要在虚拟机陷入Hypervisor中时，将虚拟中断注入虚拟机中。虚拟机在恢复执行后发现自己有未处理的中断，从而陷入相应的中断处理程序中。

> 内存虚拟化

操作系统对于物理内存的使用基于两个假设：**①内存都是从物理地址0开始的；②物理内存都是连续的。**

对于一台物理机上的多个虚拟机而言，它们共享物理内存资源，因此无法满足假设①。对于假设②，可以采用将物理内存分区的方式保证每个虚拟机看到的内存是连续的，但这样牺牲了内存使用的灵活性。因此内存虚拟化引入了GPA（Guest Physical Address，客户机物理地址）供虚拟机使用。但是，当虚拟机需要访问内存时，是无法通过GPA找到对应的数据的，需要将GPA转换为HPA（Host Physical Address，宿主机物理地址）。

因此Hypervisor需要提供两个功能：**①维护GPA到HPA的映射关系，即GPA→HPA；②截获虚拟机对GPA的访问，并根据上述映射关系将GPA转换为HPA。**

> I/O虚拟化

在物理环境下，操作系统通过I/O端口访问特定的I/O设备，称为PIO（Port I/O，端口I/O），或者将I/O设备上的寄存器映射到预留的内存地址空间进行读写，称为MMIO（Memory-Mapped I/O，内存映射I/O）。因此Hypervisor需要截获所有的PIO和MMIO操作并对其进行模拟，再将结果告知虚拟机。设备中断也类似，Hypervisor需要将中断分发到不同的虚拟机中。

此外，当多个虚拟机运行在同一个物理机上时，由于I/O设备只有一份（如网卡）不可能同时供多个虚拟机使用。Hypervisor需要为每个虚拟机提供一个软件模拟的网卡（也可以采用硬件直通或者硬件辅助虚拟化的方式），这个网卡应当与现实设备具有完全相同的接口，从而允许用户无须修改客户机操作系统中原有的驱动程序就能使用这个虚拟设备。但软件模拟的方式性能较差，目前一般采用软硬件协同优化的I/O虚拟化提升性能。

## 3.2 虚拟化分类

1974年，Gerald J. Popek和Robert P. Goldberg在虚拟化方面的著名论文Formal Requirements for Virtualizable Third Generation Architectures中提出了一组称为**虚拟化准则**的充分条件，也称为波佩克与戈德堡虚拟化需求(Popek and Goldberg Virtualization Requirements)，即**虚拟化系统必须满足以下三个条件：**

* **资源控制(Resource Control)。**虚拟机对于物理资源的访问都应该在Hypervisor的监控下进行，虚拟机不能越过Hypervisor直接访问物理机资源，否则某些恶意虚拟机可能会侵占物理机资源导致系统崩溃。
* **等价(Equivalence)。**在控制程序管理下运行的程序（包括操作系统），除时序和资源可用性之外的行为应该与没有控制程序时完全一致，且预先编写的特权指令可以自由地执行，即物理机与虚拟机的运行环境在本质上应该是相同的。对于上层应用来说，在虚拟机和物理机中运行应该是没有差别的。
* **高效(Efficiency)。**绝大多数的客户虚拟机指令应该由主机硬件直接执行，而无须控制程序参与。

该论文为设计可虚拟化计算机架构给出了指导原则，遗憾的是，早期Intel x86架构并不全部符合这三个原则，也就是说，早期x86架构对虚拟化的支持是缺失的，因为其架构包含敏感非特权指令。Robert P. Goldberg还在其博士论文Architectural Principles for Virtual Computer Systems中介绍了**两种Hypervisor类型，分别是Type Ⅰ和Type Ⅱ虚拟化，**也就是沿用至今的虚拟化分类方法，如下图所示：

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211348625.png" alt="image-20231121134756354" style="zoom: 50%;" />

根据Hypervisor与物理资源和操作系统交互方式的不同，可以将Hypervisor分为两类：

* Type Ⅰ Hypervisor直接运行在物理硬件资源上，需要承担系统初始化、物理资源管理等操作系统职能。从某种程度上来说，**Type Ⅰ Hypervisor可以视为一个为虚拟化而优化裁剪过的内核，可以直接在Hypervisor上加载客户机操作系统。**它相当于在硬件和客户机操作系统之间添加了一个虚拟化层，避免了宿主机操作系统对Hypervisor的干预。Type Ⅰ Hypervisor包括Xen、ACRN等。
* 与Type Ⅰ Hypervisor不同，Type Ⅱ Hypervisor运行在宿主机操作系统中，只负责实现虚拟化相关功能，物理资源的管理等则复用宿主机操作系统中的相关代码。这种类型的Hypervisor更像是对操作系统的一种拓展。KVM就属于Type Ⅱ Hypervisor，它以模块的形式被动态地加载到Linux内核中。Type Ⅱ Hypervisor又名寄宿虚拟机监控系统，它将虚拟化层直接安装到操作系统之上，每台虚拟机就像在宿主机操作系统上运行的一个进程。一般而言，相对于Type Ⅱ Hypervisor，Type Ⅰ Hypervisor系统效率更高，具有更好的可扩展性、健壮性和性能。然而由于Type Ⅱ Hypervisor使用方便，与宿主机操作系统生态兼容，目前大多数桌面用户使用的都是该类型的虚拟机，目前常见的Type Ⅱ Hypervisor有KVM、VMware Fusion、VirtualBox和Parallels Desktop等。

## 3.3 系统虚拟化实现方式

系统虚拟化技术**按照实现方式可以分为基于软件的全虚拟化技术、硬件辅助虚拟化技术和半虚拟化技术。**一般来说，虚拟机上的客户机操作系统 “认为” 自己运行在真实的物理硬件资源上，但为了提升虚拟化性能，会修改客户机操作系统使之与Hypervisor相互协作共同完成某些操作。这种虚拟化方案称为PV（Para-Virtualization，半虚拟化），与之对应的是全虚拟化 (Full Virtualization)，无须修改客户机操作系统就可以正常运行虚拟机。下面分别从基于软件的全虚拟化、硬件辅助虚拟化和半虚拟化三个方面对上述虚拟化实现技术进行分析。

### 基于软件的全虚拟化

基于软件的全虚拟化技术采用**解释执行、扫描与修补、二进制翻译 (Binary Translation,BT)等模拟技术**弥补虚拟化漏洞。

* **解释执行**采用软件模拟虚拟机中每条指令的执行效果，相当于每条指令都需要“陷入”，这无疑违背了虚拟化的高效原则；
* **扫描与修补**为每条敏感指令在Hypervisor中生成对应的补丁代码，然后扫描虚拟机中的代码段，将所有的敏感指令替换为跳转指令，跳转到Hypervisor中执行对应的补丁代码；
* **二进制翻译**则以基本块为单位进行翻译，翻译是指将基本块中的特权指令与敏感指令转换为一系列非敏感指令，它们具有相同的执行效果。对于某些复杂的指令，无法用普通指令模拟出其执行效果，二进制翻译采用和类似扫描与修补的方案，将其替换为函数调用，跳转到Hypervisor进行深度模拟。

以如下x86指令集中 `cpuid` 指令的执行代码为例。cpuid指令是x86架构中用于获取CPU信息的敏感指令。经过二进制翻译后，将其替换为对`helper_cpuid` 的函数调用，即获取虚拟CPU的配置信息并返回，模拟出真实 `cpuid` 指令的执行效果。翻译前后代码如下：

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211349917.png" alt="image-20231121134941526" style="zoom: 50%;" />

---

对于内存虚拟化，前面提到虚拟化需要引入一层新的地址空间，即GPA。客户虚拟机中的应用使用的是GVA（Guest Virtual Address，客户机虚拟地址），而要访问内存中的数据，必须通过HVA（Host Virtual Address，宿主机虚拟地址）访问HPA。一种可能的解决方案是，将地址转换分为两部分，分别加载GPT（Guest Page Table，客户机页表）和HPT（Host Page Table，宿主机页表），完成GVA到GPA再到HPA的转换，其中转换GVA→GPA由GPT完成，而转换HVA→HPA由HPT完成，而中间转换GPA→HVA通常由Hypervisor维护，这样通过复杂的GVA→GPA→HVA→HPA多级转换，完成了客户虚拟机的内存访问，开销比较大。

因此基于软件的虚拟化技术引入了SPT（Shadow Page Table，影子页表），记录了从GVA到HPA的直接映射，只需要将SPT基地址加载到页表基地址寄存器（例如CR3）中即可完成从GVA到HPA的转换。由于每个进程有自己的虚拟地址空间，因此SPT的数目与虚拟机中进程数目相同。为了维护SPT与GPT的一致性，Hypervisor需要截获虚拟机对GPT的修改，并在处理函数中对SPT进行相应的修改。

物理机访问I/O设备一般是通过PIO或MMIO的方式，因此I/O虚拟化需要Hypervisor截获这些操作。在x86场景下，PIO截获十分简单，因为设备发起PIO一般需要执行IN、OUT、INS和OUTS指令，这四条指令都是敏感指令，可以利用CPU虚拟化中提到的方式进行截获。而对于MMIO而言，CPU是通过访问内存的方式发起的。因此Hypervisor采用一种巧妙的方式解决这一问题：在建立SPT时，Hypervisor不会为虚拟机MMIO所属的物理地址区域建立页表项，这样虚拟机MMIO操作就会触发缺页异常从而陷入Hypervisor中进行处理。

---

因此，**全虚拟化区分普通用户态指令和系统特权指令。前者直接执行即可，而对后者使用陷入和模拟技术返回到虚拟机监视器系统来模拟执行。**问题的关键在于对敏感指令的处理：全虚拟化实现了一个二进制系统翻译模块，该模块负责在二进制代码层面对代码进行转换，从而将敏感代码转换为可以安全执行的代码，避免了敏感指令的副作用。二进制翻译系统往往还带有缓存功能，显著提高了代码转换的性能。该翻译技术的主要使用者有VMware公司早期版本的系统虚拟化产品。

> 虽然全虚拟化无须对客户机操作系统进行任何修改，但却带来了二进制翻译的开销，导致了性能瓶颈。

### 硬件辅助虚拟化

为了解决软件在虚拟化引入的性能开销，Intel和AMD等CPU制造厂商都在硬件上加入了对虚拟化的支持，称为硬件辅助虚拟化 (Hardware Assisted Virtualization)。基于硬件辅助虚拟化，也可以实现全虚拟化，不用修改客户机代码。这里以典型的Intel VT技术为例进行简要说明。

前面提到，原本的x86架构存在虚拟化漏洞，并非所有敏感指令都是特权指令。最直接的解决方案就是让所有敏感指令都能触发异常，但是这将改变指令的语义，导致现有的软件无法正常运行。于是Intel VT-x引入了VMX（Virtual-Machine Extensions，虚拟机扩展）操作模式，包括根模式(root mode)和非根模式(non-root mode)，其中Hypervisor运行在根模式而虚拟机运行在非根模式。在非根模式下，所有敏感指令都会触发VM-Exit陷入Hypervisor中，而其他指令则可以在CPU上正常运行。VMX的引入使得Hypervisor无须大费周章地去识别所有的敏感指令，极大地提升了虚拟化的性能。

---

而对于内存虚拟化，前面提到可能需要两次地址转换，这就需要不断地切换页表寄存器CR3的值。因此软件全虚拟化技术引入了SPT，直接将GVA转换为HPA，而Intel VT-x引入了EPT（Extended Page Table，扩展页表）。原本的CR3装载客户页表将GVA转换为GPA，而EPT负责将GPA转换为HPA，直接在硬件上完成了两次地址转换。两次地址转换即GVA→GPA→HPA，其中转换GVA→GPA仍由客户机操作系统的GPT转换，不用修改；而第二次GPA→HPA由硬件EPT自动转换，对客户虚拟机透明。

虽然SPT是直接将GVA转换为HPA(GVA→HPA)，只有一次硬件转换。在没有页表修改的条件下，SPT更高效；然而客户机页表的修改需要通过VM-Exit陷出到Hypervisor进行模拟以保证页表同步，导致了SPT的性能问题。采用EPT后，客户机页表的修改不会导致EPT的同步（没有VM-Exit），因此EPT更为高效。此外，前面提到SPT的数量与虚拟机中进程数目相对应，而由于EPT是将GPA转换为HPA，所以理论上只需要为每个虚拟机维护一个页表即可，减少了内存占用。

---

在I/O虚拟化方面，Intel引入了Intel VT-d（Virtualization Technology for Direct I/O，直接I/O虚拟化技术）等硬件优化技术。相较于软件全虚拟化技术需要对设备进行模拟，Intel VT-d支持直接将某个物理设备直通给某个虚拟机使用，这样虚拟机可以直接通过I/O地址空间操作物理设备。但也引入了新问题，原本物理设备发起直接内存访问（DMA）需要的是宿主机物理地址，但将它分配给某个虚拟机后，该虚拟机只能为其提供客户机物理地址。因此Intel VT-d硬件上必须有一个单元（DMA重映射硬件）负责将GPA转换为HPA。这种设备直通分配有一个明显缺陷，即一个物理设备只能供一个虚拟机独占使用，这就需要更多的物理硬件资源。

SR-IOV（Single Root I/O Virtualization，单根I/O虚拟化）设备可以缓解这一问题。每个SR-IOV设备拥有一个PF（Physical Function，物理功能）和多个VF（Virtual Function，虚拟功能），每个VF都可以指定给某个虚拟机使用，这样从单个物理设备上提供了多个虚拟设备分配给不同的虚拟机使用。但SR-IOV的VF数量受硬件限制，限定了虚拟设备的可扩展性。

### 半虚拟化

半虚拟化打破了虚拟机与Hypervisor之间的界限，在某种程度上，虚拟机不再对自己的物理运行环境一无所知，它会与Hypervisor相互配合以期获得更好的性能。在半虚拟化环境中，虚拟机将所有的敏感指令替换为主动发起的超调用 (Hypercall)。Hypercall类似于系统调用，是由客户机操作系统在需要Hypervisor服务时主动发起的。通过Hypercall，客户机操作系统主动配合敏感指令的执行（这些指令受Hypervisor监控），大大减少了虚拟化开销。

相较于全虚拟化的暴力替换，半虚拟化则另辟蹊径。它对客户机操作系统中的敏感指令都进行了替换，取而代之的是Hypervisor新增的Hypercall。这些Hypercall实现了敏感指令本身的功能，同时在每次执行时都确保能够退回到Hypervisor。此外，不仅仅是敏感指令的处理，如果客户机操作系统能够意识到自身运行于虚拟机环境下，并与Hypervisor进行配合，修改代码进行针对性优化，就能提升性能。

Xen早期采用半虚拟化作为性能提升的主要手段，并大获成功。因此相较于CPU虚拟化，由于I/O半虚拟化性能提升明显，更受开发者的关注，如virtio、Vhost、Vhost-user和vDPA等一系列优化技术被提出来，并且不断演进。因此，相较于全虚拟化，半虚拟化能够提供更好的性能，但代价却是对客户机操作系统代码的修改。为了支持各种操作系统的各种版本，半虚拟化虚拟机监视器的实现者必须付出大量代价做适配工作（virtio提供了标准化的半虚拟化硬件接口，减少了适配难度），并且由于半虚拟化需要修改源代码，对不开源的系统（如Windows）适配比较困难（目前Windows设备驱动程序也广泛支持virtio接口）。

### 总结

下表总结了三种虚拟化实现方式对CPU、内存和I/O虚拟化等方面的影响。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311211353571.png" alt="image-20231121135308784" style="zoom:50%;" />

目前主流硬件架构（包括x86、ARM等）都对硬件辅助虚拟化提供了支持，RISC-V的虚拟化硬件扩展作为重要的架构规范也正在完善中。同时，半虚拟化驱动规范virtio接口标准正在普及，目前主流的操作系统都提供了virtio的支持。**因此，目前主流的虚拟化实现方式是硬件辅助虚拟化和半虚拟化，大幅降低了虚拟化性能的开销。**

在CPU、内存和I/O三类虚拟资源中，前两类都已经得到较好的解决。由于I/O设备抽象困难、切换频繁、非常态事件高发等原因，开销往往高达25%~66%（万兆网卡中断达70万次/秒，虚拟化场景下占用高达5个CPU核），因此**I/O成为需要攻克的主要效能瓶颈。**

此外，虚拟化可以嵌套(Nested Virtualization)，即在虚拟机（L1，第一层）中创建和运行虚拟机（L2，第二层），以此类推（物理机看作第零层，L0）。如果CPU支持嵌套的硬件辅助虚拟化（如Intel x86），则可以降低嵌套的虚拟机性能损失。

# 4 典型虚拟化系统

针对上述三种主要的虚拟化实现方式，本节主要介绍在虚拟化历史上起重要作用、并且目前仍在广泛使用的虚拟化系统。通过介绍典型虚拟化系统，加深对虚拟化方法的理解。

## 4.1 典型虚拟化系统简介

### VMware

斯坦福大学Mendel Rosenblum教授带领课题组研发了分布式操作系统Hive、机器模拟器SimOS和虚拟机监控器DISCO。基于这些技术积累，Mendel Rosenblum作为共同创始人在1998年创建了VMware公司，也是硅谷产学研结合的典型代表。由于当时x86架构在硬件上还不支持虚拟化，因此VMware公司采用动态二进制翻译技术与直接执行相结合的全虚拟化技术来优化性能，其虚拟机性能达到物理机的80%以上，CPU密集型的应用性能损失仅为3%~5%。具体而言，虚拟机用户态代码可直接运行（包括虚拟8086模式），虚拟机内核态代码基于动态二进制翻译执行。虽然动态二进制翻译增加了开销，但保证了敏感指令在Hypervisor监控下执行，符合波佩克与戈德堡虚拟化需求中第一条准则（资源控制），弥补了x86架构的虚拟化漏洞。同时，由于用户态代码直接运行，性能比采用纯二进制翻译的系统大为提高，取得了x86架构中虚拟化技术的突破，也被视为第一个成功商业化的虚拟化x86架构。

VMware公司在1999年发布了桌面虚拟化产品Workstation 1.0（见下图(a)），可以在一台PC上以虚拟机的形式运行多个操作系统，属于Type Ⅱ全虚拟化技术，Windows客户机操作系统不加修改就可以运行。2002年VMware公司发布了其第一代Type I虚拟化产品ESX Server 1.5（见下图(b)），采用服务器整合 (Server Consolidation) 的方式，支持将多个服务器整合到较少的物理设备中。通常可以将10台虚拟机整合到1台物理机中(10∶1)，这种“一虚多”的虚拟化方式大幅提升了服务器资源利用率，降低了数据中心的硬件成本，因此得到了广泛应用。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311212051898.png" alt="image-20231121205036533" style="zoom:50%;" />

> 注：①管理物理硬件；②实现虚拟化功能。

### Xen

2003年，剑桥大学Ian Pratt教授等发表了虚拟化领域的著名论文Xen and the art of virtualization，提出了以半虚拟化技术为基础的代表性开源项目Xen。由于当时x86硬件还没有支持虚拟化，如果不修改客户机操作系统，采用二进制翻译或软件模拟的全虚拟化技术，则内存和I/O的虚拟化开销比较大。以内存虚拟化为例，VMware采用影子页表技术，系统同时存在影子页表和客户机操作系统的原有页表，两套页表通过缺页异常(Page Fault)保持同步，引入了大量的同步开销。而Xen通过修改客户机操作系统，不再保留两套页表（消除同步开销），同时通过超调用主动更新页表。这样既保证了客户机操作系统可以快速访问页表，同时每次更新都需要经过Xen的监控，又保证了Hypervisor对内存资源的资源控制。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311212054853.png" alt="image-20231121205345029" style="zoom:50%;" />

> 注：①DomU前端驱动将数据写入I/O环；②Dom0后端驱动读取I/O环数据；③后端驱动调用设备驱动；④设备驱动操作物理设备。

Xen属于Type I Hypervisor（见上图），Hypervisor直接运行于物理硬件上，为了提供功能丰富的设备模型，提出了基于Dom0和DomU的资源管理架构。Dom0是修改后的特权Linux内核，专门提供访问物理设备的特权操作。并行存在多个DomU，但均没有访问物理设备的权限（设备直通除外），在客户机操作系统中提供了各种设备的前端驱动(Frontend Driver)，取代了原有的设备驱动（需要修改客户机操作系统）。前端驱动通过I/O环(I/O Ring)和后端驱动(Backend Driver)共享数据（基于共享内存避免数据复制），前端驱动和后端驱动的控制面通过事件通道(Event Channel)进行通信，并通过授权表(Grant Table)控制各个虚拟机的访问权限（事件通道和授权表在图中均未画出），后端驱动通过传统的设备驱动访问真实物理设备。

### QEMU/KVM

VMware和Xen均是在x86硬件不支持虚拟化的情况下，采用软件的手段弥补虚拟化漏洞，随着Intel和AMD等厂商相继提出了硬件支持的虚拟化扩展，基于硬件辅助的虚拟化技术应运而生，并由于其性能优势，逐渐占据主流地位。KVM是Linux内核提供的开源Hypervisor，也是目前主流的虚拟化技术，拥有活跃的社区和论坛(KVM Forum)。KVM自内核2.6.20起被合并进Linux，作为Linux的一个内核模块，在Linux启动时被动态加载。KVM利用了硬件辅助虚拟化的特性，能够高效地实现CPU和内存的虚拟化。

事实上，**KVM无法单独使用，因为它既不提供I/O设备的模拟，也不支持对整体虚拟机的状态进行管理。**它向用户态程序（如QEMU）暴露特殊的设备文件 `/dev/kvm` 作为接口，允许用户态程序利用它来实现最为关键的CPU和内存虚拟化，但还缺少I/O虚拟化需要的设备模型 (Device Model)，而QEMU正好可以弥补这块功能。QEMU是开源的软件仿真器，它能够通过动态二进制翻译技术来实现CPU虚拟化，同时提供多种I/O设备的模拟，因此可以作为低速的Type Ⅱ虚拟机监视器系统进行工作。然而，二进制翻译这种模拟方法带来了巨大的性能开销，导致虚拟机运行缓慢。为此**QEMU利用KVM暴露的 `/dev/kvm` 接口，以KVM作为“加速器”，从而极大地改善虚拟机的性能。**

由于QEMU通常和KVM配合使用，因此整体称为QEMU/KVM，其架构如下图所示：

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311212100179.png" alt="image-20231121205529082" style="zoom:50%;" />

QEMU通过打开设备文件 `/dev/kvm` 实现和KVM内核模块 (kvm.ko) 的交互。在创建虚拟机时，QEMU会根据用户配置完成创建vCPU线程、分配虚拟机内存、创建虚拟设备（包括磁盘、网卡等）等工作。在QEMU中，虚拟机的每个vCPU对应QEMU的一个线程，当QEMU完成了所有初始化工作后，会通过 `ioctl` 指令进入内核态的KVM模块中，由KVM模块通过虚拟机启动或恢复指令（如x86的 `VMLAUNCH/VMRESUME` 指令）切换到虚拟机运行，执行虚拟机代码。因此从宿主机操作系统的角度来看，虚拟机的每个CPU对应于系统中的一个线程，并且该线程受到QEMU的控制和管理。当CPU执行了特权指令或发生特定行为时，会触发虚拟机陷出事件退出到KVM，由KVM判断能否进行处理（比如对一个QEMU中模拟的I/O设备进行操作）。如果不能，则进一步返回到QEMU，由QEMU负责处理。

### ACRN

随着万物互联时代的到来，**虚拟化技术进一步扩展到移动终端、嵌入式和车载设备等资源受限的平台，**一系列嵌入式、轻量级虚拟化被提出来，例如QNX Hypervisor、Jailhouse、Xvisor、PikeOS和OKL4等。Linux基金会于2018年3月发布了开源的轻量级虚拟化平台ACRN。ACRN针对实时性和安全性（针对车载场景）进行了适配优化，并为关键业务提供了安全性隔离。ACRN通过GVT-g支持GPU虚拟化，可以在车载场景下共享GPU。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311212059446.png" alt="image-20231121205708684" style="zoom:50%;" />

> 注：①前端驱动将数据写入I/O环；②后端驱动读取I/O环数据；③后端驱动调用设备驱动；④设备驱动操作物理设备。

ACRN是Type I虚拟化架构（见上图），类似于Xen的Dom0，采用特权级的服务操作系统 (Service OS) 来管理物理I/O设备的使用；类似于Xen的DomU，用户操作系统(User OS)由ACRN Hypervisor进行创建和管理。设备分为前、后端驱动，两者之间的数据共享（环/队列）通过标准的virtio接口进行访问。

下表总结了上述4种典型虚拟化系统的特点，它们是2000年之后有代表性的虚拟化系统，至今仍广泛使用。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311212059467.png" alt="image-20231121205836414" style="zoom: 33%;" />

IBM、微软等主流操作系统厂商的虚拟化产品、亚马逊和阿里巴巴等主流云计算提供商的新型虚拟化技术（如亚马逊Nitro Hypervisor、阿里神龙服务器）和VirtualBox、Xvisor等开源系统也在市场中占据重要地位，限于篇幅，本书不做介绍。

## 4.2 openEuler的虚拟化技术

openEuler是一款开源操作系统。当前openEuler内核源于Linux，支持鲲鹏及其他多种处理器，是由全球开源贡献者构建的高效、稳定且安全的开源操作系统，适用于数据库、大数据、云计算和人工智能等应用场景。同时，openEuler是一个面向全球的操作系统开源社区，通过社区合作打造创新平台，构建支持多处理器架构、统一和开放的操作系统。openEuler社区还孵化了A-Tune和iSula两个开源子项目。A-Tune是智能性能优化系统软件，即通过机器学习引擎对业务应用建立精准模型，再根据业务负载智能匹配最佳操作系统配置参数组合，实现系统整体运行效率的提升；iSula是一种云原生轻量级容器解决方案，可以通过统一、灵活的架构满足ICT（Information Communications Technology，信息和通信技术）领域端、边与云场景的多种需求。**在全场景虚拟化方面，提出了基于Rust语言的下一代虚拟化平台StratoVirt，构建面向云数据中心的企业级虚拟化平台，实现了一套架构统一支持虚拟机、容器和Serverless三种场景。**

openEuler社区版本分为LTS（Long-Term Storage，长期支持版本）和创新版本，版本号按照交付年份和月份进行命名。

* 长期支持版本。发布间隔定为2年，提供4年社区支持。社区LTS版本openEuler 20.03于2020年3月正式发布。

* 社区创新版本。每隔6个月openEuler会发布一个社区创新版本，提供6个月社区支持。

openEuler 20.09于2020年9月发布。openEuler目前提供了支持AArch64和x86_64处理器架构的KVM虚拟化组件，支持鲲鹏处理器和容器虚拟化技术，与开源QEMU、StratoVirt和Libvirt等共同构成了完整的系统虚拟化运行环境。有关openEuler及虚拟化组件的安装、配置和管理请参考openEuler开源网站。

下表列出了openEuler软件包提供的虚拟化相关组件。

<img src="https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311212103508.png" alt="image-20231121210140537" style="zoom:33%;" />

# 5 总结

本章简要介绍了系统虚拟化的基本概念，回顾系统虚拟化的发展历史并展望其发展趋势，还介绍了系统虚拟化的主要功能、分类和目前使用广泛的典型系统虚拟化项目，以及openEuler操作系统及其虚拟化技术。通过本章，希望各位对虚拟化技术的来龙去脉有较为宏观的理解，为后续的学习打下基础。
