---
layout: post
title: "《Essence of Linear Algebra》系列笔记整理"
description: ""
categories: []
tags: [Essential and Professional Course]
redirect_from:
  - /2019/08/07/
typora-root-url: ..
---

# 《Essence of Linear Algebra》系列笔记整理

​	线性代数的本质这门课是我偶然发现的，主要从几何本质的的角度去理解香型代数，很形象，很容易去理解线性代数教材中的抽象概念，因此强推此系列课程用来辅助学习。我因为有其他安排，所以花了好几天，一天两个视频的看。其实，如果有时间，半天时间就能看完。简明而不简单！

> 原视频是YouTube大佬录制的，感谢3BlueBrown，他授权已经将此系列视频搬运到B站，需要的请自行检索

### 第00讲 序言

这一讲只是描述了线性代数的用途以及本课程的框架，没有实质的讲解，就当学英语了吧。

> | 相关关键词            | 美式发音                    | 中文翻译 |
> | --------------------- | --------------------------- | -------- |
> | Linear Algebra        | /'lɪnɪɚ  ældʒɪbrə/          | 线性代数 |
> | matrix multiplication | /'metrɪks  'mʌltəplə'keʃən/ | 矩阵乘法 |
> | deteminant            | /dɪ'tɝmɪnənt/               | 行列式   |
> | cross product         | /krɔs 'prɑdʌkt/             | 叉积     |
> | eigenvalue            | /'aɪgən,væljuː/             | 特征值   |
>
> | 应用学科               | 美式发音                  | 中文翻译   |
> | ---------------------- | ------------------------- | ---------- |
> | Computer Science       | /kəm'pjutɚ  'saɪəns/      | 计算机科学 |
> | Physics                | /'fɪzɪks/                 | 物理学     |
> | Electrical Engineering | /ɪ'lɛktrɪkl 'ɛndʒə'nɪrɪŋ/ | 电子工程   |
> | Mechanical Engineering | /mɪ'kænɪkəl 'ɛndʒə'nɪrɪŋ/ | 机械工程   |
> | Statistics             | /stə'tɪstɪks/             | 统计学     |

### 第01讲 向量究竟是什么

对于不同领域的学生有着不同的理解

![](/images/posts/2019-08-07/vectorstudent.png)

则向量的两个基本运算：向量加法、和数量积则是最基础，最重要的运算。

> Two fundamental operations:vector addition and scalar multiplication.

可形象的表示为

> 自己在ipad上画的。。。凑合吧

![](/images/posts/2019-08-07/operation.png)

> 数量积就更显而易见了，就没画。

为什么**v**和**w**(加粗代表向量)要首尾相接才能相加？为什么不是尾端都在原点。

可以把自己想象成一个人，先按照**v**走，走完站在原地再按照**w**走。就很容易理解了。

### 第02讲 线性组合，张成的空间与基

**线性组合**(Linear Combination)就很好理解了，即为两个向量线性相加

![](/images/posts/2019-08-07/linearcombinaton.png)

**基**(basis of coordinate system)就是两个向量，可以是任何两个线性无关的向量。上图所示的即是其中最特殊的一个标准正交基。

**向量空间**（span）就是这两个基所有的 线性组合

的集合

> The "span" of **v** and **w** is the set of all their linear combination a**v**+b**w** ,let a,b vary over all real number.

**线性相关**（linear dependent）**u**=a**v**+b**w** for some values of a and b

**线性无关**（linear independent）**u**不等于a**v**+b**w** for all values of a and b

### 第03讲 矩阵与线性变换

Linear Transformation 就相当于一个Input vector 通过一个函数运算输出一个Output vector

线性变换有两个原则

- Lines remain lines      坐标系从直线变到直线
- origin remain fixed     原点位置不变

下面就是一个线性变换的例子，从向量**u**变换到**u'**。

![](/images/posts/2019-08-07/transform.png)

其中坐标系的线性变换使基向量发生了线性变换，即变成了**a** 和**b**。按照原本的线性组合关系，**u**也变换到了**u'**。

而由变换后的基向量组成的2*2的矩阵即叫做**变换矩阵**

下面这两个例子也是一样。

![](/images/posts/2019-08-07/transformexample.png)

通过这一讲就很好的理解了矩阵与向量相乘的本质，就是对其进行线性变换。

### 第04讲 矩阵乘法与线性变换复合

例：Composition of a rotation and a shear

![](/images/posts/2019-08-07/composition.png)

一个向量进行两次线性变换，即在前面乘以两个变换矩阵。也等于在前面乘以一个只变换一次的变换矩阵。

则两个矩阵的乘法则就是两次线性变换的叠加。太神奇了！

这样的话就很好解释了为什么矩阵乘法不适用交换律

![](/images/posts/2019-08-07/jiaohuanlv.png)

从二维的理解很快的很直观的就能推广到三维、四维等等。

### 第05讲 行列式

行列式对于二维方阵来说就是向量围成的平行四边形的面积，对于三维来说就是围成的平行六面体的体积。

![](/images/posts/2019-08-07/deteminant.png)

### 第06讲 逆矩阵、列空间与零空间

一个变换矩阵的**逆矩阵**(inverse matrix)就是把这个线性变换反过来再变回来。

![](/images/posts/2019-08-07/inverse.png)

**秩**(Rank)的含义

- 若经线性变换到直线，则称这个变换矩阵秩为1
- 若经线性变换到平面，则称这个变换矩阵秩为2

**列空间**(column space)：不管是一条直线，一个平面，还是三维空间等，所有可能的变换结果的集合就是列空间。

> The colume space is the span of the columns of your matrix
>
> The rank is the number of dimensions in the column space.

**零空间**（null space or kernel）经变换后落在原点的向量的集合

#### 附注：非方阵 nonsquare matrices

![](/images/posts/2019-08-07/nonsquare.png)

### 第07讲 点积与对偶性

![](/images/posts/2019-08-07/dot.png)

上面表明了点积的几何意义与和其对偶的一个线性变换，即将一个向量变换到一维上。

### 第08讲上 叉积的标准介绍

叉积也在二维上也等于平行四边形的面积，正负代表方向。

叉积在二维上得到一个长度为平行四边形面积，方向垂直于两个向量。

> 这才叫灵魂画手！！！其实我想画右手定则的，哈哈哈哈

![](/images/posts/2019-08-07/cross.png)



### 第08讲下 以线性变换的眼光看待叉积

![](/images/posts/2019-08-07/cross2.png)

即利用线性变换与对偶性的思想，得到了叉积该有的样子。

### 第09讲 基变换

![](/images/posts/2019-08-07/basis.png)

即将基向量进行线性变换。主要看左下角的推导。

这里有两组基向量，一个是Standard，一个是Jennifer（只是个人名，为了区分而已）,Jennifer的基向量即不互相垂直，也不是单位向量，所以在他的基础上进行线性变换很难。

因此，先将Jennifer基向量通过线性变换到Standard标准基向量。然后再基向量坐标系下进行变换。变换完在变回Jennifer的基向量下。

> 有点绕，其实在视频里配合动画就很容易理解了

这就得到了相似矩阵的定义啦，相似矩阵就是为了简化这么绕的变换而生的。

### 第10讲 特征向量与特征值

![](/images/posts/2019-08-07/eigenvalue.png)

在一个线性变换中，可能有些向量方向不会发生变化，这个向量就叫特征向量，他的长度变化倍数就是特征值。太神奇了

最重要的还是与上述基变换结合起来。

![](/images/posts/2019-08-07/ji.png)

其中换基向量的变换矩阵就是特征向量组成的矩阵。得到的对角矩阵就是特征值组成的，得到对角矩阵的话如果要算一个矩阵的10000次方也轻而易举了。

### 第11讲 抽象向量空间

这一讲主要讲了抽象化的重要性。比如上述讲的所有东西都是基于二维或者三维矩阵的分析，怎么抽象为符号语言就是数学家干的事，其他领域的研究者则是使用这些符号语言（即公理Axioms）去进行计算。

![](/images/posts/2019-08-07/function.png)

### 第12讲 克莱姆法则 Cramer's rule

也是从线性变换的角度去理解克莱姆法则

![](/images/posts/2019-08-07/cramer.png)

x的求解亦然



### 学完的感想

听大佬说这些东西是高等代数里的知识，以后还是要学学高等代数。

这个视频是英文版的，我看是听着英文，看着中英文字母尽量去听懂。但正因为他是英文版，可能语速太快有些细节我没有捕捉到，目前的理解也仅限于此，能够帮助理解很多线性代数里面的概念知识。我学习的时候也是听着自己想着去理解着，这个视频确实很不错，大佬用动画的方式非常形象的展示了向量的变化，非常有助于理解。

这篇博客的目的也仅仅是我学习完了进行了一下系统复习，都是比较笼统的总结。