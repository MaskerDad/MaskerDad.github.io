---
title: 神龙虚拟化架构解析

date: 2023-11-12 17:00:00 +0800

categories: [kernel, virt]

tags: [virt, qemu, kvm]

description: 

---

# 0 引言

**项目名称：基于神龙软硬协同虚拟化的超大规模云计算关键技术与系统**

本项目围绕超大规模云计算关键技术，提出了面向大规模公共云的“云原生虚拟化”技术体系与结构，在计算与I/O  分离的高隔离与低损耗虚拟化、软硬协同I/O虚拟化加速、软硬协同设计虚拟化的RAS和负载率提升等方面取得技术突破，降低了云计算虚拟化成本，提升了云服务的性能和效率，项目成果应用于阿里云，服务全球四百多万用户。

从以下方面出发：

* **项目背景**
* **关键技术**
* **项目成果**
* **获奖原因**

[https://developer.aliyun.com/ebook/235](https://developer.aliyun.com/ebook/235?spm=a2c6h.12873639.article-detail.7.408a3721lOouOx&source=5176.11533457&userCode=b3pdrgck)

# 1 弹性裸金属服务器和神龙虚拟化

## 1.1 功能特点





## 1.2 适用场景







## 1.3 弹性裸金属技术







# 2 论文解读

[High-density Multi-tenant Bare-metal Cloud - AMiner](https://www.aminer.cn/pub/5e6cae3493d709897ccff2ed/high-density-multi-tenant-bare-metal-cloud) 

本文提出了BM-Hive的设计、实施和评估，它是一种高密度裸金属云服务。在BM-Hive中，每个bm-guest运行在一个带有自己的CPU和内存的PCIe卡上。它可以通过混合virtio接口与现有的云服务无缝交互。

## 2.1 设计需求

对于BM-Hive，我们有以下设计要求：

* 多租户：BM-Hive必须支持多租户，即一台物理服务器上可以容纳多个bm-guest，以实现云用户期望的精细化CPU核心。
* 安全性：多租户可能导致严重的安全问题。BM-Hive必须提供强大的基于硬件的bm-guest之间隔离，并保护自身免受恶意bm-guest的侵害。需要注意的是，与vm-guest相比，bm-guest的限制较少，因此更强大。
* 互操作性：BM-Hive必须与现有的云基础设施兼容，以便立即从云基础设施的可用性、可靠性、弹性和丰富功能中受益。互操作性要求bm-guest也可以在虚拟机中运行。我们将这个特性称为冷迁移（与热迁移相对）。冷迁移是云操作的基本要求。冷迁移的前提是bm-guest必须能够与云存储和高性能网络连接，因为大多数guests不允许访问本地存储。在虚拟化中，这可以通过诸如virtio之类的半虚拟化I/O来实现。BM-Hive通过其独特的硬件-软件混合virtio子系统解决了这个挑战。从用户的角度来看，他们只需要提供一个虚拟机镜像，可以作为虚拟机或bm-guest运行。
* 性能：bm-guest应具有本地CPU和内存性能，并接近本机I/O性能。bm-guest的性能变化应尽量小。
* 成本效益：与相似配置的vm-guest相比，bm-guest应具有更低的成本。

在这些要求中，互操作性对BM-Hive的系统设计具有决定性的影响。

## 2.2 系统概述

图2比较了虚拟化架构和BM-Hive服务器的结构。主要区别在于虚拟机客户机在虚拟化的CPU和内存上运行，而裸机客户机直接在计算板的物理CPU和内存上运行。这两个系统都利用virtio进行I/O操作。所有主要的操作系统都支持virtio作为内核中的设备驱动程序或可用的设备驱动程序。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311141518747.png)

bm-hypervisor，也是一个类似于vm-hypervisor的用户空间进程，负责管理bm-guests的生命周期（例如分配、创建和销毁），为virtio设备提供后端支持，并与云基础架构进行接口交互。由于bm-hypervisor支持与vm-hypervisor相同的云接口，因此可以无缝集成到现有的云基础架构中。与vm-hypervisor相比，bm-hypervisor的代码基础要简单得多，因为bm-hypervisor不需要虚拟化CPU、内存、总线等。

每个bm-hypervisor进程仅为一个bm-guest提供服务，以更好地隔离后端virtio资源。此外，bm-hypervisor仅通过virtio接口间接与bm-guest进行交互，而vm-hypervisor则通过虚拟组件和hypercalls直接暴露给（可能恶意的）vm-guests。因此，bm-hypervisor比vm-hypervisor更安全。

> ***使用场景：***云基础架构选择一个可用的裸金属服务器，并选择一个空闲的计算板并启动它（通过打开PCIe电源）。然后，该板上的固件（即BIOS）开始执行引导加载程序，该程序将进一步加载bm-guest内核。需要注意的是，云中的大多数客户机不被允许使用本地存储，而是必须通过网络接口使用远程云存储。因此，引导加载程序和内核（两者都是虚拟机镜像的一部分）被远程存储，并且只能通过virtio-blk接口访问。为了解决这个问题，我们扩展了计算板的（基于EFI的）固件，以在启动过程中识别和利用virtio。当内核加载时，它可以通过virtio设备访问云存储和网络。

## 2.3 系统架构

图3展示了BM-Hive的整体系统架构。**每个裸金属服务器由基础部分和若干计算板组成。**基础部分实际上是一个简化的基于Xeon的服务器，具有16个E5 CPU核心。每个计算板是基础部分的PCIe扩展板。其主要组件包括CPU、内存、PCIe总线和IO-Bond。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311141523987.png)

IO-Bond是一种在FPGA中实现的硬件。它与基础板和计算板上的PCIe总线进行交互。在计算板的PCIe总线上，它模拟了一些virtio设备。从bm-guest的角度来看，每个virtio设备都是一个普通的PCIe设备，可以被发现、配置和使用。只是恰好这些设备由virtio内核驱动程序支持。在基础板的PCIe总线上，IO-Bond暴露了virtio设备的后端，并提供了接口供bm-hypervisor控制计算板的执行。IO-Bond充当了bm-guest中的virtio前端和bm-hypervisor中的后端之间的桥梁。具体来说，它通过内置的DMA引擎将前端的控制和配置命令转发到后端，并在它们之间传输数据。目前，IO-Bond支持网络和存储（块）的virtio设备。只需进行微小的更改（因为新设备的主要逻辑包含在前端和后端中），就可以轻松扩展以支持其他virtio设备。IO-Bond还可以通过自定义ASIC芯片来实现。

从bm-hypervisor的角度来看，每个计算板都是一个特殊的PCIe设备，用于执行bm-guests。在我们当前的实现中，一个基本服务器可以支持最多16个计算板，即16个bm-guests可以安全地共存于同一台裸金属服务器上。**bm-hypervisor的主要功能有三个：**

1. 它管理bm-guests的生命周期。具体而言，它需要管理计算板的分配，并通过与IO-Bond通信来控制它们的执行。
2. bm-hypervisor为其bm-guests的所有virtio设备提供后端支持。它维护virtio设备的内部状态，并通过网络接口执行bm-guests的I/O请求。基础CPU拥有足够数量的CPU核心，以处理所有来自bm-guest的I/O请求，而不会丢失数据。
3. bm-hypervisor与现有的云基础设施进行接口交互，将自身整合到整个云操作中。与vm-hypervisor相比，bm-hypervisor不是一个更高权限的层，位于guests下方。它通过外部控制guests。如前所述，bm-hypervisor的代码基础要简单得多，无需创建虚拟计算机系统来运行guest。bm-hypervisor的其他组件，包括I/O后端和云接口，与vm-hypervisor大部分相同。实际上，它们可以共享相同的代码基础，以便进行简便的维护。

BM-Hive的架构不仅使其能够无缝集成到现有的云基础设施中，还使计算板的设计高度灵活 - 唯一的硬性要求是支持virtio接口。特别是，它可以使用适合其设计的任何CPU。我们已经进行了实验并生产了搭载Xeon E3和E5、Intel Core i7和Intel Atom处理器的计算板。从技术上讲，也可以使用AMD和ARM处理器。搭载Core i7/i9处理器的计算板可以实现比Xeon处理器更好的单线程性能，这对于股票交易等服务至关重要。

## 2.4 IO-Band

IO-Bond作为计算板的virtio设备的代理。Virtio被设计为一种通用的半虚拟化设备模型，可被不同的虚拟化监控程序使用。它由前端和后端组成。前端通过特定于虚拟化监控程序的通知机制（例如hypercalls）向后端发送（高级）I/O请求。请求被处理后，后端通过注入虚拟中断来通知前端。为了避免不必要的内存复制，I/O数据通过共享环形缓冲区交换。在虚拟化服务器上设置共享缓冲区很容易，因为前端和后端可以访问相同的内存。Virtio设备被建模为PCI设备。虚拟机客户机可以通过其虚拟PCI/PCIe总线发现、配置和使用virtio设备。这使得BM-Hive能够在硬件和软件混合设计中实现virtio。

### IO-Bond: Frontend

IO-Bond的前端连接到计算板的PCIe总线上。IO-Bond中的FPGA逻辑模拟了每个virtio设备的PCI接口（即PCI配置空间，BAR0，BAR1，PCIe Cap等）。这个PCI接口允许bm-guest发现、配置和驱动virtio设备。bm-guest对PCI接口的访问直接转发到后端进行处理。

虚拟化系统中，前端和后端通过共享的环形缓冲区交换数据。然而，在BM-Hive中，这种设计无法正常工作，因为IO-Bond的前后端不共享物理内存，而是各自拥有自己的内存。为了解决这个问题，IOBond在bm-hypervisor和bm-guest之间创建了一个环形缓冲区。bm-hypervisor的环形缓冲区（图4中的影子vring）与另一个环形缓冲区进行同步。当数据被添加到一个环形缓冲区时，IO-Bond中的DMA引擎会将其复制到另一个缓冲区中。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311142001025.png)

### IO-Bond: Backend

IO-Bond的后端通过打包和转发接收到的I/O请求来处理它们，并与相应的云设备进行连接。后端的设计关注如何将我们的通用云基础架构与高I/O性能相连接。类似于vm-guest，在BM-Hive中，所有的I/O请求都在用户空间中处理，使用vhost-user协议与云基础架构进行接口交互：自定义的DPDK虚拟交换机[1]和SPDK云存储[5]。我们同时使用DPDK和SPDK的轮询模式驱动程序（PMD）。PMD通过轮询virtio设备的I/O请求来代替依赖中断，可以显著提高I/O性能，尤其是当设备以全速运行时可以避免中断延迟。因此，所有的I/O请求完全在用户空间中处理，避免了在发送/接收网络接口数据包时在用户空间和内核缓冲区之间的额外内存拷贝。此外，BM-Hive支持VGA设备，供用户连接到bm-guest的控制台。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311142002431.png)

我们还实施了一些慢速I/O路径，以绕过云基础设施进行测试，例如通过Linux Tap设备发送数据包。由于性能低或无法访问云服务，这些路径并未在真实云中部署。只有具备DPDK和SPDK的快速I/O路径被部署在我们的裸金属服务器上。

### IO-Bond Implementation

IO-Bond是virtio PCI接口的硬件实现和相应的软件后端。计算板通过标准的virtio协议访问IO-Bond前端。每个模拟的virtio设备的virtio队列都有相应的后端阴影vring。bm-hypervisor通过一对用于PCI访问通知的邮箱寄存器和每个阴影vring的一对头/尾寄存器与IOBond进行通信。这些阴影vring实际上是IO-Bond和bm-hypervisor之间的共享缓冲区，它们以一系列缓冲区描述符的形式组织，并作为阴影vring映射到IO-Bond。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311142005940.png)

图6显示了一个典型的virtio Tx/Rx工作流程，说明了**IO-Bond如何响应guest。**Bm-guest通过写入其virtio通知寄存器来通知IO-Bond，并在Rx数据到达时获得MSI中断。在IOBond和其后端之间没有中断。在bm-hypervisor中的一个专用线程将拉取IO邮箱的更新值和vring头/尾寄存器的影子值。该示例展示了完成从bm-guest发送Tx和读取Rx的14个步骤。步骤1-6是标准virtio设备操作，包括IO-Bond如何更新vring的使用标志、获取描述符和间接描述符表。最后，IO-Bond通过更新其头寄存器来通知bm-hypervisor。接收Rx缓冲区的过程相反。

由于IO-Bond使用了成本低廉的FPGA，从bm-guest到IO-Bond前端的PCI读/写需要0.8微秒，从IO-Bond到其邮箱寄存器需要另外0.8微秒。因此，从bm-hypervisor模拟的典型PCI访问始终需要1.6微秒。IO-Bond为virtio网络和存储设备分别暴露了一个PCIe x4接口，它们由一个PCIe x8接口备份到bm-hypervisor。同时，IO-Bond内部DMA吞吐量约为50Gbps。因此，每个bm-guest的最大带宽为50Gbps（每个x4接口为32Gbps）。正如前面提到的，所有I/O请求最终都通过服务器的共享（100Gbit/s）网络接口转发到云服务中。

我们的云基础设施将平衡所有bm-guest的需求，动态调整每个bm-guest的带宽消耗以确保公平性。

## 2.5 成本效益

数据中心的盈利主要取决于可用于销售的虚拟中央处理器（vCPU）核心数，换句话说，可用vCPU核心的密度。现在，一个典型的基于虚拟机的服务器选择两个24核（48超线程）的E5处理器，其中8个超线程保留给 `hypervisor` 和 `host kernel`，因此只剩下88个超线程供用户使用。而在相同的机架空间下，BM-Hive 可以为每个bm-guest提供32个超线程，最多可提供8个bm-guest，总计256个超线程。即使我们为每个bm-guest添加了额外的FPGA（英特尔Arria低成本FPGA）和一个更便宜的基础服务器处理器（16个超线程的E5处理器），BMHive的每个vCPU成本效益仍然远远超过vm-guest。我们的销售价格显示，具有相同配置的bm-guest比vm-guest低10%。

功耗是与成本效益相关的另一个方面。然而，由于BMHive和基于虚拟机的服务器提供不同的可用vCPU，很难进行比较。BM-Hive配置中最接近基于虚拟机的服务器的是一个单计算板，销售96个超线程（如表3所示），而基于虚拟机的服务器只销售88个超线程。我们的TDP（热设计功耗）估计显示：单个计算板的BM-Hive每个vCPU的功耗为3.17w，而基于虚拟机的服务器根据英特尔处理器的TDP[4]为3.06w/每个vCPU。BM-Hive的额外功耗来自FPGA硬件和基础服务器的处理器。

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311141507336.png)



## 2.6 性能测试

### CPU/Memory



### I/O



### Application

![](https://cdn.jsdelivr.net/gh/MaskerDad/BlogImage@main/202311150938431.png)

一个更重要的实验是BM-Hive在云端常用应用中的表现如何。为此，我们在bmguest中运行了NGINX、MariaDB和Redis服务器，并测量它们的QPS。

* 我们使用Apache HTTP基准测试来测试禁用KeepAlive功能的NGINX服务器。结果如图12所示。**当客户端数量增加时，bm-guest相对于vm-guest始终能够每秒处理大约50%至60%更多的请求。每个请求的平均响应时间约短了30%。**
* MariaDB的测试数据库包含16个表，每个表中有100万条记录。我们使用128个线程的sysbench-1.0.17同时查询数据库。结果如图13和14所示。**对于只读查询，bm-guest每秒可维持195K个查询（QPS），而具有相同配置的vm-guest仅达到170K QPS，即bm-guest比vm-guest快大约14.7%。此外，在仅写入查询方面，bm-guest比vm-guest快大约42%，在读/写混合查询方面快大约55%。**
* Redis是一个缓存型存储库。它支持许多数据结构，例如哈希、列表和集合。它经常被用作云中数据库的快速缓存。在这个实验中，我们比较了在运行Redis服务器时，bm-guest和vm-guest的性能。我们使用标准的Redis基准测试作为客户端，并配置了具有10M随机键值条目的服务器。在每个测试中，我们查询服务器1M次以获取/设置数据。每个测试重复进行10次。结果如图15和16所示。**具体而言，图15显示了服务器在不同客户端数量（从1,000到10,000）下的性能表现。bm-guest的性能（每秒请求数）比vm-guest的性能好20%到40%；图16显示了服务器在不同数据大小（从4B到4KB）下的性能表现。bm-guest不仅每秒处理更多的请求，而且吞吐量更加稳定。vm-guest性能的波动可能是由缓存引起的。请注意，图16的y轴从80K请求/秒开始，以提高可见性。**

> **总结：对于云中常用的应用程序，BM-Hive的性能明显优于基于虚拟化的云服务。**



## 2.7 Hypervisor相关工作 (对比其他虚拟化架构)

在本节中，我们将BM-Hive与密切相关的工作进行比较，包括减少虚拟化开销和提供裸机和高密度服务。

### 减少上下文切换次数

虚拟化开销主要由上下文切换、内存和设备虚拟化以及虚拟化管理程序本身引起。上下文切换是vm-guest和虚拟化管理程序之间的切换。由于需要保存和恢复大量状态，这是一项开销很大的操作。上下文切换可以由许多事件引起，例如内存虚拟化和定时器。

目前已经提出了许多方法来减少上下文切换的数量和频率。例如，大多数现代虚拟化管理程序拦截所有硬件中断（即使是通过客户机传递的设备）。高速设备如网络接口卡可以产生大量中断。为此，ELI（Exit-Less Interrupt）提议从中断处理路径中删除虚拟化管理程序，并让客户机直接且安全地处理中断[18]。通过这样做，ELI可以显着提高I/O密集型工作负载的吞吐量并减少延迟。

在减少上下文切换方面，还有努力改善客户机的响应能力。例如，KVM最近引入了halt_polling特性，在CPU让出并进入睡眠状态之前，轮询唤醒条件。这避免了客户机被置于睡眠状态并立即被待处理的唤醒条件唤醒的问题[20]。另一个例子是使用共享调度[6, 32]来解决锁持有者抢占问题[33]，即在持有锁的情况下客户机被抢占。所有这些问题在BM-Hive中不存在。

### 内存虚拟化

内存通常通过两级页表进行虚拟化 - 客户机页表（GPT）将客户机虚拟地址映射到客户机物理地址，扩展（或嵌套）页表（EPT）将客户机物理地址映射到物理地址。因此，在客户机中缺少一个TLB（转换后备缓冲区）的处理成本要比在bm-guest中高得多，因为它需要遍历两级页表（最多24次内存访问）[31]。已经提出了解决方案来增加TLB覆盖范围（例如使用大页）或减少TLB惩罚。例如，POM-TLB提出通过使用作为内存一部分的非常大的TLB来减少地址转换的成本[31]。Chang等人提出了一种硬件软件混合设计，将连续的页面合并为页表条目的子集，从而增加TLB条目的覆盖范围[29]。Amit提出通过跟踪页面访问来减少不必要的TLB清除，即维护（每个核心的）TLB缓存的一致性[12]。这些问题不会影响BM-Hive，因为它的内存是以本地方式访问的。它可以达到内存的本地性能。

### 设备虚拟化

现在大多数设备通过半虚拟化来实现虚拟化，其中一个自定义设备驱动程序被加载到客户机内核中以直接处理I/O操作。Virtio是最流行的半虚拟化I/O接口，它在Linux上得到了本地支持。此外，一些硬件设备原生支持虚拟化，例如SRIOV网络适配器，这些设备可以直接传递给客户机。**BM-Hive通过混合方法独特地实现了virtio接口，以实现与现有云基础设施的互操作性。**

### 极简hypervisor

还有一些努力在创建最小型虚拟机监控程序，通常是通过在虚拟机之间分割系统资源来实现。例如，英特尔的ACRN是用于嵌入式物联网系统的最小型虚拟机监控程序[25]。它直接将CPU核心和内存分割给虚拟机，以减少CPU和内存开销。NoHype提议通过硬件对资源进行分割，从而消除虚拟机监控程序的存在[23]。**BM-Hive的Bm-hypervisor比传统的虚拟机监控程序要简单得多，因为它不进行任何虚拟化，并且与bm-guests没有直接的交互。**

### 嵌套虚拟化支持

BM-Hive的目标之一是允许用户运行自己的虚拟化监控程序，这是云计算中越来越重要的应用场景。这理论上可以通过嵌套虚拟化[13, 19]来实现，其中客户机虚拟化监控程序在虚拟机内运行。然而，嵌套虚拟化存在较高的性能开销[13]。BM-Hive提供了更高效的解决方案，因为用户的hypervisor直接运行在物理CPU上，并对硬件虚拟化支持具有完全控制权。

### 其他裸金属服务

用户长期以来一直可以在数据中心租用物理机器。**<font color='red'>然而，机器租赁并不被视为裸金属服务，因为它没有集成到云基础设施中，因此缺乏云服务的优势。例如，设置租赁机器可能需要几个小时甚至几天的时间，因为用户必须提供要安装在机器上的物理硬盘。在BM-Hive中，bm-guest可以同时为裸金属和虚拟化服务使用相同的虚拟机映像。该服务的调度是即时的。</font>**

大多数公共云供应商都部署了单租户裸金属服务，具有第1节中提到的限制。与单租户裸金属服务相比，BM-Hive具有更高的密度，更具成本效益，并且可以使用具有高单线程性能的CPU（这在云中通常不可用）。

一些公共云供应商还开发了专用的虚拟机网络设备。例如，Azure在其数据中心部署了SmartNIC [17]，将大部分软件定义网络功能卸载到硬件中。据我们所知，SmartNIC专注于低层云基础设施的优化，而BM-Hive不涉及云基础设施层。

还有一些具有类似总体架构的系统。例如，Intel的媒体编解码处理器VCA2是一张带有三个Xeon E3 v1258L处理器的PCIe卡[9]。每个服务器最多可以安装八张VCA2卡来加速媒体编解码。BM-Hive是为云服务而构建的，其硬件设计针对此目的。例如，它具有混合virtio接口，允许bm-guest与各种云服务进行交互。

