<!DOCTYPE html>
<html lang="zh-CN">
<head>

    <!--Google Adsense, details at: "https://www.google.com/adsense/new/u/0/pub-3980406043325966/home" 
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-3980406043325966",
            enable_page_level_ads: true
        });
    </script>-->


    <meta http-equiv="content-type" content="text/html; charset=utf-8">


    <!-- Global site tag (gtag.js) - Google Analytics 
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-130856615-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-130856615-1');
    </script>-->


    <!-- Google Tag Manager 
    <script>(function (w, d, s, l, i) {
        w[l] = w[l] || [];
        w[l].push({
            'gtm.start':
                new Date().getTime(), event: 'gtm.js'
        });
        var f = d.getElementsByTagName(s)[0],
            j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : '';
        j.async = true;
        j.src =
            'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
        f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-PW3MXPD');</script>
    <!-- End Google Tag Manager -->


    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <meta content="yes" name="apple-mobile-web-app-capable">
    <meta content="black-translucent" name="apple-mobile-web-app-status-bar-style">
    <meta content="telephone=no" name="format-detection">

    <title>Convex optimization notes from Yuanming Shi | Personal Blog</title>

    <!--live2d function-->
    <script src="/assets/live2dw/lib/L2Dwidget.min.js"></script>
    <script>L2Dwidget.init({
        "pluginRootPath": "live2dw/",
        "pluginJsPath": "lib/",
        "pluginModelPath": "assets/",
        "tagMode": false,
        "debug": false,
        "model": {"jsonPath": "/assets/live2dw/assets/koharu.model.json"},
        "display": {"position": "right", "width": 65, "height": 90, "hOffset": 60, "vOffset": -10,},
        "mobile": {"show": true, "scale": 1,},
        "log": false
    });</script>


    <meta name="keywords"
          content="yzp">
    <meta name="description"
          content="  这篇博客只记录每章我认为需要记忆的一些名词定义和公式。这套课件是我导师自己写的，吸收了各家之长，知识更广泛，更贴近于科研需要的知识。  注意：这个笔记只用来整理知识架构，备忘，所以里面有些公式因为在Markdown里面不能完美复现，比如向量和矩阵的加粗显示，所以本笔记里很多公式里是向量但是我没加粗。使用的时候...">
    <!--<meta name="baidu-site-verification" content="fjE6fa4d3J"/>-->
    <meta name="author" content="Yang Zhanpeng">

    <link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png">
    <link rel="apple-touch-icon" href="/favicon.png">
    <link rel="apple-touch-icon-precomposed" href="/favicon.png">
    <link rel="alternate" type="application/rss+xml" title="Zhan Peng’s Bolg" href="
    /feed.xml">

    <link rel="stylesheet" type="text/css" href="
    /assets/css/normalize.css">
    <link rel="stylesheet" type="text/css" href="
    /assets/css/pure-min.css">
    <link rel="stylesheet" type="text/css" href="
    /assets/css/grids-responsive-min.css">
    <link rel="stylesheet" type="text/css" href="
    /assets/css/style.css?v=1514870841">

    <!--use mathJax to edit formulas in LaTex grammar.-->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
              inlineMath: [['$','$']]
            }
          });




    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>


    <script type="text/javascript" src="/assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/js/tongji.baidu.js"></script>

</head>

<body>

<!-- Google Tag Manager (noscript) 
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PW3MXPD"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
</noscript>
<!-- End Google Tag Manager (noscript) -->


<div class="body_container">
    <div id="header">
        <div class="site-name">
            <h1 class="hidden">Convex optimization notes from Yuanming Shi</h1>
            <a id="logo" href="/.">Zhan Peng’s Bolg</a>
            <p class="description">Personal Blog</p>
        </div>
        <div id="nav-menu">
            
            <a href="/."><i class="icon-home"> 首页</i></a>
            
            <a href="/archives/"><i class="icon-archive"> 归档</i></a>
            
            <a href="/tags/"><i class="icon-tags"> 标签</i></a>
            
            <a href="/guestbook/"><i class="icon-guestbook"> 留言版</i></a>
            
        </div>
        <script type="text/javascript">
            var menus = document.getElementById('nav-menu');
            var ishome = true;
            for (var i = 1; i < menus.childElementCount; i++) {
                var path = location.href;
                var current = path.startsWith(menus.children[i].href);
                if (current) {
                    menus.children[i].className = 'current';
                    ishome = false;
                    break;
                }
            }
            if (ishome) {
                menus.children[0].className = 'current';
            }
        </script>
    </div>

    <div id="layout" class="pure-g">
        <div class="pure-u-1 pure-u-md-3-4">
            <div class="content_container">
                <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Convex optimization notes from Yuanming Shi</h1>
        <p class="post-meta">
            <time datetime="2019-11-13T00:00:00-08:00" itemprop="datePublished">
                <i class="icon-post-date">Nov 13, 2019</i>
            </time>
             |
            <span class="post-category">
            <i class="icon-post-category">
            
                <a href="/category/#ConvexOptimization">ConvexOptimization</a>
            
            </i>
        </span>   |
            <span class="post-tag">
            <i class="icon-post-tag">
            
                <a href="/tags/#Essential and Professional Course">Essential and Professional Course</a>
            
            </i>
        </span>


            <!-- 阅读量统计， id 将作为查询条件 -->
            <span id=/2019/11/13/Convex-optimization-notes-from-Yuanming-Shi/ class="leancloud-visitors" data-flag-title=Convex optimization notes from Yuanming Shi>
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-text">阅读量 </span>
                <span class="leancloud-visitors-count"></span>
            </span>
        </p>

    </header>

    <div class="post-content" itemprop="articleBody">
        <blockquote>
  <p>这篇博客只记录每章我认为需要记忆的一些名词定义和公式。这套课件是我导师自己写的，吸收了各家之长，知识更广泛，更贴近于科研需要的知识。</p>

  <p>注意：这个笔记只用来整理知识架构，备忘，所以里面有些公式因为在Markdown里面不能完美复现，比如向量和矩阵的加粗显示，所以本笔记里很多公式里是向量但是我没加粗。使用的时候一定要注意。可以再在书里查看一下。</p>
</blockquote>

<ul class="toc" id="markdown-toc">
  <li><a href="#convex-set" id="markdown-toc-convex-set">Convex set</a></li>
  <li><a href="#convex-function" id="markdown-toc-convex-function">Convex function</a></li>
  <li><a href="#convex-optimization-problems" id="markdown-toc-convex-optimization-problems">Convex Optimization Problems</a></li>
  <li><a href="#lagrange-duality" id="markdown-toc-lagrange-duality">Lagrange Duality</a></li>
  <li><a href="#constructive-convex-analysis-and-disciplined-convex-programming" id="markdown-toc-constructive-convex-analysis-and-disciplined-convex-programming">Constructive Convex Analysis and Disciplined Convex Programming</a></li>
  <li><a href="#gradient-method" id="markdown-toc-gradient-method">Gradient method</a>    <ul>
      <li><a href="#gradient-methords-for-unconstrained-problem" id="markdown-toc-gradient-methords-for-unconstrained-problem">Gradient methords for unconstrained problem</a></li>
      <li><a href="#gradient-methods-for-constrained-problems" id="markdown-toc-gradient-methods-for-constrained-problems">Gradient methods for constrained problems</a></li>
    </ul>
  </li>
  <li><a href="#subgradient-methods" id="markdown-toc-subgradient-methods">Subgradient methods</a></li>
  <li><a href="#proximal-gradient-methods" id="markdown-toc-proximal-gradient-methods">Proximal gradient methods</a></li>
  <li><a href="#accelerated-gradient-methods" id="markdown-toc-accelerated-gradient-methods">Accelerated gradient methods</a></li>
  <li><a href="#smoothing-for-nonsmooth-optimization" id="markdown-toc-smoothing-for-nonsmooth-optimization">Smoothing for nonsmooth optimization</a></li>
  <li><a href="#dual-and-primal-dual-methods" id="markdown-toc-dual-and-primal-dual-methods">Dual and primal-dual methods</a></li>
  <li><a href="#alternating-direction-method-of-multipliers-admm" id="markdown-toc-alternating-direction-method-of-multipliers-admm">Alternating direction method of multipliers (ADMM)</a></li>
  <li><a href="#quasi-newton-methods" id="markdown-toc-quasi-newton-methods">Quasi-Newton methods</a></li>
  <li><a href="#stochastic-gradient-methods" id="markdown-toc-stochastic-gradient-methods">Stochastic gradient methods</a></li>
</ul>
<h2 id="convex-set">Convex set</h2>

<ul>
  <li>Affine set : $x_1,x_2 \in C,\theta \in R\Rightarrow x=\theta x_1+(1-\theta x_2)\in C$ (Line)</li>
  <li>Convex set : $x_1,x_2 \in C,\theta \in [0,1]\Rightarrow x=\theta x_1+(1-\theta x_2)\in C$ (Line segment)</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Hyperplane : ${x</td>
          <td>a^Tx=b}$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Polyhedra :  ${ {\mathbf{x} }</td>
          <td>{\mathbf{A} }{\mathbf{x} }\preceq{\mathbf{b} }},{\mathbf{C} }{\mathbf{x} }={\mathbf{d} }}$ ($A\in R^{m\times n},C\in R^{p\times n},\preceq$ is componentwise inequality)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Euclidean Ball :$B(x_c,r)={x</td>
          <td>{\left| {x - {x_c} } \right|_2} \le r}={x_c+ru</td>
          <td>{\left| u\right|_2} \le 1}$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Ellipsoid : $E(x_c,P)={x</td>
          <td>(x-x_c)^TP^{-1}(x-x_c) \le 1}={x_c+Au</td>
          <td>{\left| u\right|<em>2} \le 1}$, $P\in S^n</em>{++}$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Norm: a function $\left| {\cdot } \right|$ that satifies
    <ul>
      <li>$\left| {x} \right| \ge 0$; $\left| {x} \right| = 0$ if and only if $x=0$</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$\left| {tx} \right| =</td>
              <td>t</td>
              <td>\left| {x} \right|$ for $t\in R$</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>$\left| {x+y} \right| \le \left| {x} \right|+\left| {y} \right|$</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>Norm ball : ${x</td>
              <td>\left| {x-x_c} \right|\le r}$</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>Norm cone: ${(x,t)\in R^{n+1}</td>
              <td>\left| {x} \right|\le t}$ (Euclidean norm cone, second-order cone, ice-cream cone)</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>Positive semidefinite
    <ul>
      <li>$S^n$ is set of symmetric $n\times n$ matrices</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$S^n_+={\mathbf{X}\in S^n</td>
              <td>\mathbf{X} \succeq 0}$: positive semidefinite $n \times n$ matrices。$\mathbf{X}\in S^n_+\Leftrightarrow z^T \mathbf{X} z \ge 0 {\rm \ for\  all\  z }$</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$S^n_{++}={\mathbf{X}\in S^n</td>
              <td>\mathbf{X} \succ 0}$: positive definite $n \times n$ matrices。</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>Operation that Preserve Convexity
    <ul>
      <li>intersection</li>
      <li>affine function</li>
      <li>perspective function</li>
      <li>linear-fractional function</li>
    </ul>
  </li>
  <li>Proper cone:
    <ul>
      <li>$K$ is closed (contains its boundary)</li>
      <li>$K$ is solid (has nonempty interior)</li>
      <li>$K$ is pointed (contains no line $x\in K,-x \in K\Rightarrow x=0$)</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Dual Cones $K^*={y</td>
          <td>y^Tx \ge0 {\rm \ for\  all\ }x \in K}$</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>self-dual</li>
      <li>$y \succeq_{K^*}0 \Leftrightarrow y^Tx \ge 0 {\rm\  for\ all\ }x \succeq_K0$</li>
    </ul>
  </li>
</ul>

<h2 id="convex-function">Convex function</h2>

<ul>
  <li>Convex function : $f(\theta x+(1-\theta)y\le \theta f(x)+(1-\theta)f(y)$ for $0\le\theta\le1$</li>
  <li>Example on $R^n$
    <ul>
      <li>Affine function $f(\mathbf{x})=\mathbf{a}^T\mathbf{x}+b$ is convex and convave</li>
      <li>Norms $\left |\mathbf{x} \right|$ are convex</li>
      <li>Quadratic function $f(\mathbf{x})=\mathbf{x}^T\mathbf{P}\mathbf{x}+2\mathbf{q}^T\mathbf{x}+r$ is convex if and only if  $\mathbf{P}\succeq0$</li>
      <li>Geometric mean $f(\mathbf{x})=(\prod\nolimits_{i = 0}^n x_i )^{1/n}$ is convave</li>
      <li>log-sum-exp $f(\mathbf{x})=\log \sum_ie^{x_i}$ is convex</li>
      <li>Quadratic over linear $f(\mathbf{x},y)=\mathbf{x}^T\mathbf{x}/y$ is convex on $R^n \times R_{++}$</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Epigraph ${\rm epi\ } f ={(x,t)\in R^{n+1}</td>
          <td>x\in {\rm dom \ }f, f(x)\le t} $</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Restriction of a Convex Function to a Line
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$f:R^n \rightarrow R$ is convex if and only if the function $g:R \rightarrow R, g(t)=f(\mathbf{x}+t\mathbf{v}), {\rm dom\ }g={t</td>
              <td>\mathbf{x}+t\mathbf{v}\in {\rm dom\ }f}$ is conve for any $\mathbf{x}\in {\rm dom\ }f,\mathbf{v} \in R^n$</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>arbitrary line</li>
    </ul>
  </li>
  <li>First-order condition:$f(\mathbf{y})\ge f(\mathbf{x})+\nabla f(\mathbf{x})^T(\mathbf{y}-\mathbf{x})\ \ \ \forall \mathbf{x},\mathbf{y}\in {\rm dom} f$</li>
  <li>Second-order condition: $\nabla^2f(\mathbf{x})\succeq0 \ \ \ \forall \mathbf{x}\in {\rm dom} f $</li>
  <li>Operations that preserve convexity
    <ul>
      <li>nonnegative weight sum $\alpha_1f_1+\alpha_2f_2$</li>
      <li>composition with affine function $f(\mathbf{A}\mathbf{x}+\mathbf{b})$</li>
      <li>composition with scalar function $f(x)=h(g(\mathbf{x}))$</li>
      <li>pointwise maximum $f:=\max{f_1,\cdots,f_n}$</li>
      <li>supermum,minimization  $g(x)=\mathop {\rm sup}\limits_{\mathbf{y}\in A}f(x,y) ,g(x)=\mathop {\rm inf}\limits_{\mathbf{y}\in A}f(x,y)  $</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>perspection $g(x,t) =tf(x,t), {\rm dom\ } g= {(x,t)\in R^{n+1}</td>
              <td>x/t \in {\rm  dom\ }f,t &gt;0}$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Quasi-convexity : the sublevel sets $S_\alpha={\mathbf{x}\in {\rm dom\ }f</td>
          <td>f(\mathbf{x})\le \alpha} $ is all convex</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Log-convexity : $\log f$ is convex</li>
</ul>

<h2 id="convex-optimization-problems">Convex Optimization Problems</h2>

<ul>
  <li>stationary point  $\nabla f(\mathbf{x})=0$
    <ul>
      <li>local minimum and gobal minimum</li>
      <li>Saddle point: if $\mathbf{x}$ is a stationary opint and for any neighborhood $B \subseteq R^n $ exist $\mathbf{y},\mathbf{z} \in B$ such that $f(\mathbf{z})\le f(\mathbf{x})\le f(\mathbf{y})$ and $\lambda_{\min}(\nabla^2f(x))\le0$</li>
    </ul>
  </li>
  <li>Convex Optimization problem
    <ul>
      <li>$\begin{array}{*{20}{c} }
{ {\rm{minimize} } }&amp;{ {f_0} (x)}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{ {f_i}(\mathbf{x}) \le 0}&amp;{i = 1, \cdots ,m}<br />
{}&amp;{ {\bf{Ax} } = {\bf{b} } }&amp;{}
\end{array}$</li>
      <li>$f_0,f_1,\cdots,f_m$ are convex and equality constraints are affine</li>
    </ul>
  </li>
  <li>Oprimality Criterion
    <ul>
      <li>Minimum principle: $\nabla f_0(\mathbf{x^<em>})^T(\mathbf{y}-\mathbf{x}^</em>)\ge0 $ for all feasible $\mathbf{y}$</li>
      <li>Unconstrained problem:  $\nabla f_0(\mathbf{x^<em>})=0,x^</em>\in {\rm dom\ }f$</li>
      <li>Equality constrained problem $\mathbf{A}\mathbf{x^<em>}=\mathbf{b},\nabla f_0(\mathbf{x^</em>})+\mathbf{A}\mathbf{v}=0$</li>
      <li>minimization over nonnegative orthant:$x \succeq0,\left{ {\begin{array}{*{20}{c} }
{ {\nabla _i}{f_0}(x) \ge 0}&amp;{ {x_i} = 0}<br />
{ {\nabla _i}{f_0}(x) = 0}&amp;{ {x_i} &gt; 0}
\end{array} } \right.$</li>
    </ul>
  </li>
  <li>Equivalent Reformulations
    <ul>
      <li>Introducing slack variables for linear inequalities$\begin{array}{*{20}{c} }
{\mathop {\rm{minimize} }\limits_{x,s} }&amp;{ {f_0}(x)}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{ {\mathbf{a}_i}\mathbf{x} +s_i=b_i}&amp;{i = 1, \cdots ,m}<br />
{}&amp;{s_i\ge0}&amp;{}
\end{array}$</li>
      <li>epigraph form$\begin{array}{*{20}{c} }
{\mathop {\rm{minimize} }\limits_{x,t} }&amp;{t}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{f_0(x)-t\le0}&amp;{}\{}&amp;{ {f_i}(\mathbf{x}) \le 0}&amp;{i = 1, \cdots ,m}<br />
{}&amp;{\bf{Ax} } = {\bf{b} }&amp;{}
\end{array}$</li>
      <li>minimizing over some variables$\begin{array}{*{20}{c} }
{\mathop {\rm{minimize} }\limits_{x} }&amp;{ { {\tilde f}_0}(\mathbf{x})}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{ {f_i}(\mathbf{x}) \le 0}&amp;{i = 1, \cdots ,m}
\end{array}$ where ${ { {\tilde f}_0}(\mathbf{x})}={\rm inf}_yf_0(\mathbf{x},\mathbf{y})$</li>
      <li>Quasi-convex  optmization: $f_0$ is quasiconvex, and $f_1,\cdots,f_m$ are convex</li>
    </ul>
  </li>
  <li>Classes of Convex Problem
    <ul>
      <li>LP(Linear Programming) : objective and constraint functions are affine</li>
      <li>QP(Quadratic Programming): convex quadratic objective and affine constraint function</li>
      <li>QCQP(Quadratically Constrained Quadratic Programming), inequality constraint function is quadratic</li>
      <li>SOCP(Second-Order Cone Programming): linear objective and second-order cone inequality constrains $\left| A_ix+b\right|_2\le c_i^Tx+d_i\ \ \ i=1,\cdots,m$</li>
      <li>SDP(Semi-Definite Programming)$\begin{array}{*{20}{c} }
{\mathop {\rm{minimize} }\limits_{x} }&amp;{\mathbf{c}^T\mathbf{x} }&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{x_1\mathbf{F}_1+\cdots+x_n\mathbf{F}_n\preceq\mathbf{G} }&amp;{}\{}&amp;{\mathbf{A}\mathbf{x}=\mathbf{b} }\end{array}$</li>
    </ul>
  </li>
</ul>

<h2 id="lagrange-duality">Lagrange Duality</h2>

<ul>
  <li>
    <p>Primal problem:  $\begin{array}{*{20}{c} }
{\mathop {\rm{minimize} }\limits_{x} }&amp;{f_0(\mathbf{x})}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{f_i(x)\le0}&amp;{i = 1, \cdots ,m}\{}&amp;{ {h_i}(\mathbf{x}) == 0}&amp;{i = 1, \cdots ,p}
\end{array}$</p>
  </li>
  <li>
    <p>Lagrangian $L(\mathbf{x},\mathbf{\lambda},\mathbf{v})=f_0(\mathbf{x})+\sum\limits_{i = 1}^m { { {\bf{\lambda } }<em>i}{f_i}({\bf{x} })}+\sum\limits</em>{i = 1}^p { { {\bf{v} }_i}{h_i}({\bf{x} })}$</p>
  </li>
  <li>
    <p>Dual function: $g(\mathbf{\lambda},\mathbf{v})=\mathop{\rm inf}\limits_{x\in D}L(\mathbf{x},\mathbf{\lambda},\mathbf{v})$, concave</p>

    <p>$f_0(x)\ge L(\mathbf{x},\mathbf{\lambda},\mathbf{v}) \ge g(\mathbf{\lambda},\mathbf{v})$</p>
  </li>
  <li>
    <p>Lagrange dual problem: $\begin{array}{*{20}{c} }
{\mathop {\rm{maximize} }\limits_{\mathbf{\lambda},\mathbf{v} } }&amp;{g(\mathbf{\lambda},\mathbf{v})}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{\mathbf{\lambda}\succeq 0}&amp;{}
\end{array}$</p>
  </li>
  <li>
    <p>Duality</p>

    <ul>
      <li>week duality:  $d^<em>\le p^</em>$(对偶问题的最优解小于原问题的最优解)</li>
      <li>strong duality: $d^<em>= p^</em>$, is very desirable, does not hold in general, usually holds for convex problem</li>
      <li>duality gap: $p^<em>-d^</em>$</li>
    </ul>
  </li>
  <li>
    <p>slater’s Constraint Qualification: conditions that guarantee strong duality in convex problem</p>

    <ul>
      <li>$\exists x \in {\rm int\  }D),f_i(x)&lt;0$</li>
    </ul>
  </li>
  <li>
    <p>Complementary slackness $\lambda_i^<em>f_i(x^</em>)=0$（互补松弛性）</p>
  </li>
  <li>
    <p>KKT condition</p>

    <ol>
      <li>primial feasibility: $f_i(x)\le0,i=1,\cdots,m,h_i(x)=0,i=1,\cdots,p$</li>
      <li>dual feasibility: $\lambda\succeq0$</li>
      <li>complementary slackness: $\lambda_i^<em>f_i(x^</em>)=0$ for $i=1,\cdots,m$</li>
      <li>zero gradient of Lagrangian with respect to $\mathbf{x}$ : $\nabla f_0(\mathbf{x})+\sum\limits_{i = 1}^m { { {\bf{\lambda } }<em>i}{\nabla f_i}({\bf{x} })}+\sum\limits</em>{i = 1}^p { { {\bf{v} }_i}{\nabla h_i}({\bf{x} })}=0$</li>
    </ol>

    <ul>
      <li>We already known that if strong duality holds and $x,\lambda,v$ are optimal, then they must satisfy the KKT condition.</li>
      <li>If $x^<em>,\lambda ^</em>,v^*$ satisfy the KKT conditions for a convex problem, then they are optimal.</li>
    </ul>
  </li>
</ul>

<h2 id="constructive-convex-analysis-and-disciplined-convex-programming">Constructive Convex Analysis and Disciplined Convex Programming</h2>

<ul>
  <li>
    <p>Conic program: $\begin{array}{*{20}{c} }
{ {\rm{minimize} } }&amp;{c^Tx}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{Ax=b}&amp;{x\in K}
\end{array}$,where $K$ is convex cone.</p>

    <ul>
      <li>the modern canonical form</li>
      <li>there are well developed solvers for cone programs</li>
    </ul>

    <blockquote>
      <p>如何求解一个凸问题</p>

      <ol>
        <li>用一个现存的定制的的求解器去求解特定的问题</li>
        <li>利用现存的流行方法研发一个针对你的问题的新的求解器</li>
        <li>把你的问题转换为一个cone program (CVX)，然后用一个标准cone program 求解器(SDPT3, MDSEK)</li>
      </ol>
    </blockquote>
  </li>
  <li>
    <p>Basic example</p>

    <table>
      <thead>
        <tr>
          <th>Convex</th>
          <th>Concave</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$x^p(p\ge0 {\rm \ or\ } p\le0)$<br />$e^x$<br />$x\log x$<br />$a^Tx+b$<br />$x^TPx(P\succeq0)$<br />$\left|x\right|$(any norm)<br />$\max{x_1,\dots,x_n}$</td>
          <td>$x^p(0\le p\le1)$<br />$\log x$<br />$\sqrt{xy}$<br />$x^TPx(P\preceq0)$<br />$\min{x_1,\dots,x_n}$</td>
        </tr>
        <tr>
          <td>$x^2/y(y&gt;0),x^Tx/y(y&gt;0),x^TY^{-1}x(Y\succ0)$<br />$\log(e^{x_1}+\cdots+e^{x_n})$<br />$f(x)=x_{[1]}+\cdots+x_{[k]}$(sum of lagrest $k$ entries)<br />$f(x,y)=x\log(x/y)$   (x,y&gt;0)<br />$\lambda_{\max}(X)(X^T=X)$</td>
          <td>$\log\det X,(\det X)^{1/n}$  $(X\succ 0)$<br /> $\log\Phi(x)$  ($\Phi$is Gaussian CDF)<br />$\lambda_{\min}(X)(X^T=X)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Calculus rules</p>

    <ul>
      <li>nonnegative scaling: $\alpha f$</li>
      <li>sum : $f+g$</li>
      <li>affine composition : $f(Ax+b)$</li>
      <li>pointwise maximum : $\max_if_i(x)$</li>
      <li>composition: $h(f(x))$,  $h$ convex increasing, $f$ convex</li>
    </ul>
  </li>
  <li>
    <p>Disciplined convex programing(DCP)</p>

    <blockquote>
      <p>描述凸优化问题的框架；基于Constructive Convex Analysis；凸性是充分而不必要条件；基于 domain specific languages (DSL) and tools</p>
    </blockquote>

    <ul>
      <li>zero or one objective, with form
        <ul>
          <li>minimize {scalar convex expression} or</li>
          <li>maximize {scalar convex expression}</li>
        </ul>
      </li>
      <li>zero or more constraints, with form
        <ul>
          <li>{convex expression} &lt;={concave expression} or</li>
          <li>{concave expression} &gt;={convex expression} or</li>
          <li>{affine expression} &gt;={affine expression}</li>
        </ul>
      </li>
    </ul>

    <blockquote>
      <p>很容易构建一个DCP分析器；不难将DCP转化为cone problem</p>
    </blockquote>
  </li>
  <li>
    <p>Modeling languages</p>

    <ul>
      <li>CVX       Matlab       Grant,Boyd            2005</li>
      <li>CVXPY   Python       Diamond, Boyd    2013</li>
    </ul>

    <blockquote>
      <p>slides 中有示例代码，这里不再赘述</p>
    </blockquote>
  </li>
</ul>

<h2 id="gradient-method">Gradient method</h2>

<h3 id="gradient-methords-for-unconstrained-problem">Gradient methords for unconstrained problem</h3>

<ul>
  <li>
    <p>Dradient descent (GD) $x^{t+1}=x^{t}-\eta_t\nabla f(x^t)$, a.k.a. steepest descent</p>
  </li>
  <li>
    <p>Exact Line Search : $\eta_t=\arg \mathop \min \limits_{\eta \ge 0} f(x^t- \eta \nabla f(x^t))$</p>
  </li>
  <li>strongly convex and smooth
    <ul>
      <li>A twice-differentiable function $f$ is said to be $\mu$-strongly  convex and $L$-smooth if $0 \preceq \mu I \preceq\nabla^2f(x)\preceq LI, \forall x$</li>
      <li>more on strong convexity
        <ol>
          <li>$f(y)\ge f(x)+\nabla f(x)^T(x-y)+\frac{\mu}{2}\left |x-y \right|^2_2, \forall x,y$</li>
          <li>$f(\lambda x+(1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y)-\frac{\mu}{2}\lambda(1-\lambda)\left |x-y \right|^2_2, \forall x,y,0\le\lambda\le1$</li>
          <li>$\langle \nabla f(x)-\nabla f(y),x-y\rangle \ge\mu\left |x-y \right|^2_2, \forall x,y$</li>
          <li>$\nabla^2 f(x)\succeq \mu I, \forall x$</li>
        </ol>
      </li>
      <li>more on smoothness
        <ol>
          <li>$f(y)\le f(x)+\nabla f(x)^T(x-y)+\frac{L}{2}\left |x-y \right|^2_2, \forall x,y$</li>
          <li>$f(\lambda x+(1-\lambda)y)\ge \lambda f(x)+(1-\lambda)f(y)-\frac{L}{2}\lambda(1-\lambda)\left |x-y \right|^2_2, \forall x,y,0\le\lambda\le1$</li>
          <li>$\langle \nabla f(x)-\nabla f(y),x-y\rangle \ge \frac{1}{L}\left |\nabla f(x)-\nabla f(y)\right|^2_2, \forall x,y$</li>
          <li>$\left |\nabla f(x)-\nabla f(y)\right|_2\le L\left |x-y \right|_2, \forall x,y$ (L-Lipschitz gradient)</li>
          <li>$\left | \nabla^2 f(x)\right| \le L, \forall x$</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>
    <p>Backtracking line search</p>

    <ol>
      <li>Initialize $\eta=1,0&lt;\alpha\le1/2,0&lt;\beta&lt;1$</li>
      <li>while $f(x^t-\eta\nabla f(x^t))&gt;f(x^t)-\alpha\eta\left | \nabla f(x^t)\right|^2_2$ do:</li>
      <li>​           $\eta \leftarrow\beta\eta$</li>
    </ol>

    <ul>
      <li>$f(x^t)-f(x^<em>) \le \left( 1-\min{2\mu\alpha,2\beta\alpha\mu/L}\right)^{t}(f(x^0)-f(x^</em>))$</li>
    </ul>
  </li>
  <li>
    <p>Local strong convexity</p>

    <table>
      <tbody>
        <tr>
          <td>Let $f$ be locally $\mu$-strongly  convex and $L$-smooth such that  $\mu I \preceq\nabla^2f(x)\preceq LI, \forall x\in B_0$, where $B_0:={x</td>
          <td>\left|x-x^<em>\right|_2\le \left|x^0-x^</em>\right|_2}$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Regularity condition: $\langle \nabla f(x),x-x^<em>\rangle \ge\frac{\mu}{2}\left |x-x^</em> \right|^2_2+\frac{1}{2L}\left |\nabla f(x)\right|^2_2, \forall x$</p>

    <ul>
      <li>$\left|x^t-x^<em>\right|^2_2\le \left( 1- \frac{\mu}{L}\right)^t \left|x^0-x^</em>\right|^2_2$</li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>problem</th>
      <th>algorithm</th>
      <th>stepsize rule</th>
      <th>convergence rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Quadratic minimization<br />${\rm minimize}_x f(x):=\frac{1}{2}(x-x^<em>)^TQ(x-x^</em>)$</td>
      <td>GD</td>
      <td>$\eta_t=\frac{2}{\lambda_1(Q)+\lambda_n(Q)}$(constant stepsize)</td>
      <td>$\left|x^t-x^<em>\right|_2\le \left( \frac{\lambda_1(Q)-\lambda_n(Q)}{\lambda_1(Q)+\lambda_n(Q)}\right)^t \left|x^0-x^</em>\right|_2$</td>
    </tr>
    <tr>
      <td>Quadratic minimization<br />${\rm minimize}_x f(x):=\frac{1}{2}(x-x^<em>)^TQ(x-x^</em>)$</td>
      <td>GD</td>
      <td>Exact Line Search</td>
      <td>$f(x^t)-f(x^<em>) \le \left( \frac{\lambda_1(Q)-\lambda_n(Q)}{\lambda_1(Q)+\lambda_n(Q)}\right)^{2t}(f(x^0)-f(x^</em>))$</td>
    </tr>
    <tr>
      <td>Strongly convex and smooth functions</td>
      <td>GD</td>
      <td>$\eta_t=\frac{2}{\mu+L}$(constant stepsize)</td>
      <td>$\left|x^t-x^<em>\right|_2\le \left( \frac{\kappa-1}{\kappa+1}\right)^t \left|x^0-x^</em>\right|_2$<br />$\kappa:=L/\mu$</td>
    </tr>
    <tr>
      <td>Convex and smooth functions</td>
      <td>GD</td>
      <td>$\eta_t=\frac{1}{L}$</td>
      <td>$f(x^t)-f(x^<em>) \le \frac{2L\left|x^0-x^</em>\right|_2^2}{t}$</td>
    </tr>
  </tbody>
</table>

<h3 id="gradient-methods-for-constrained-problems">Gradient methods for constrained problems</h3>

<ul>
  <li>
    <p>Frank-Wolfe/ conditional gradient algorithm</p>

    <ol>
      <li>for $t=0,1,\cdots$ do</li>
      <li>​      $y^t:=\arg\min_{x\in C}\langle \nabla f(x^t),x \rangle$                              (direction finding)</li>
      <li>​      $x^{t+1}=(1-\eta_t)x^t+\eta_ty^t$                                     (line search and update)</li>
    </ol>

    <ul>
      <li>stepsize $\eta_t$ determined by line search or $\eta_t=\frac{2}{t+2}$</li>
      <li>Frank-Wolfe can also be applied to nonconvex problems</li>
    </ul>
  </li>
  <li>
    <p>Optimizing over Atomic Sets</p>

    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>Gauge function $U_D(x):={\rm inf}_{t\ge0}{t</td>
              <td>x\in tD}$</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>Support function $U^*<em>D:={\rm sup}</em>{s\in D}\langle s,y \rangle$</li>
    </ul>
  </li>
  <li>
    <p>Projected gradient methods</p>

    <ol>
      <li>
        <p>for $t=0,1,\cdots$ do</p>
      </li>
      <li>
        <p>​      $x^{t+1}=\mathcal{P}_{\mathcal{C} }(x^t-\eta_t\nabla f(x^t))$</p>

        <p>where $\mathcal{P}<em>{\mathcal{C} }(x)=\arg\min</em>{z\in \mathcal{C} }\left|x-z\right|_2$ is Euclidean projection onto $\mathcal{C}$</p>
      </li>
    </ol>

    <ul>
      <li>Projection theorem
        <ul>
          <li>Let  $\mathcal{C}$ be closed convex set, Then $x_\mathcal{C}$ is projection of $x$ onto $\mathcal{C}$ iff $(x-x_\mathcal{C})^T(z-x_\mathcal{C})\le0,\forall z\in \mathcal{C}$</li>
          <li>$-\nabla f(x^t)^T(x^{t+1}-x^t)\ge 0$</li>
          <li>Nonexpansicness of projection: $\left|\mathcal{P}<em>{\mathcal{C} }(x)- \mathcal{P}</em>{\mathcal{C} }(z)\right|_2\le \left|x-z\right|_2$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>problem</th>
      <th>algorithm</th>
      <th>stepsize rule</th>
      <th>convergence rate</th>
      <th>iteration complexity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>convex &amp; smooth</td>
      <td>Frank-Wolfe</td>
      <td>$\eta_t=\frac{2}{t+2}$</td>
      <td>$f(x^t)-f(x^*) \le \frac{2Ld_C^2}{t+2}$<br />$d_C={\rm sup}_{x,y\in C}\left|x-y\right|_2$</td>
      <td>$O(\frac{1}{\epsilon})$</td>
    </tr>
    <tr>
      <td>strongly convex &amp; smooth<br />($x\in {\rm int}(\mathcal{C})$)</td>
      <td>Projected GD</td>
      <td>$\eta_t=\frac{2}{\mu+L}$</td>
      <td>$\left|x^t-x^<em>\right|_2\le \left( \frac{\kappa-1}{\kappa+1}\right)^t \left|x^0-x^</em>\right|_2$<br />$\kappa:=L/\mu$</td>
      <td> </td>
    </tr>
    <tr>
      <td>strongly convex &amp; smooth</td>
      <td>Projected GD</td>
      <td>$\eta_t=\frac{1}{L}$</td>
      <td>$\left|x^t-x^<em>\right|_2\le \left( 1- \frac{\mu}{L}\right)^t \left|x^0-x^</em>\right|_2$<br />$O((1-\frac{1}{\kappa})^t$</td>
      <td>$O(\kappa\log\frac{1}{\epsilon})$</td>
    </tr>
    <tr>
      <td>convex &amp; smooth</td>
      <td>Projected GD</td>
      <td>$\eta_t=\frac{1}{L}$</td>
      <td>$f(x^t)-f(x^<em>) \le \frac{3L\left|x^0-x^</em>\right|_2^2+f(x^0)-f(x^*)}{t+1}$<br />$O(\frac{1}{t})$</td>
      <td>$O(\frac{1}{\epsilon})$</td>
    </tr>
  </tbody>
</table>

<h2 id="subgradient-methods">Subgradient methods</h2>

<ul>
  <li>
    <p>generalizing steepest descent</p>

    <ol>
      <li>$d^t\in \arg\min_{\left|d \right|<em>2\le1}f’(x^t,d)$,  where $f’(x^t,d)=\lim</em>{\alpha \downarrow0} \frac{f(x+\alpha d)-f(x)}{\alpha}$</li>
      <li>$x^{t+1}=x^t+\eta_td^t$</li>
    </ol>

    <blockquote>
      <p>问题：1. Finding steepest descent direction involves expensive computation, 2，stepsize rule 不好选择，可能收敛不到最优解，特别是不可导的点。</p>
    </blockquote>
  </li>
  <li>
    <p>(projected) subgradient method</p>

    <blockquote>
      <p>主要就是为了解决不可导的点的问题的</p>
    </blockquote>

    <ul>
      <li>$x^{t+1}=\mathcal{P}_{\mathcal{C} }(x^t-\eta_tg^t)$ , where $g^t$ is any subgradient of $f$ at $x^t$</li>
    </ul>
  </li>
  <li>
    <p>Subgradient</p>

    <ul>
      <li>We say $g$ is subgradient of $f$ at point $x$ if $f(z)\ge f(x)+g^T(z-x), \forall z$
        <ul>
          <li>set of all subgradients of $f$ at $x$ is called subdifferential of $f$ at $x$, denoted by $\partial f(x)$</li>
        </ul>
      </li>
      <li>Basic rules
        <ul>
          <li>scaling: $\partial (\alpha f)=\alpha\partial (f)$</li>
          <li>summation: $\partial (f_1+f_2)=\partial (f_1)\partial (f_2)$</li>
          <li>affine transformation : $\partial (f(Ax+b))=A^T\partial f(Ax+b)$</li>
          <li>chain rule: $\partial (g\circ f)=g’(f(x))\partial f(x)$</li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>composition: suppose $f(x)=h(f_1(x),\cdots,f_n(x)),q=\nabla h(y)</td>
                  <td>_{y=[f_1(x),\cdots,f_n(x)]},g_i\in \partial f_i(x)$. then   $q_1g_1+\cdots+q_ng_n\in \partial f(x)$</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>pointwise maximum: $f(x)=\max_{1\le i\le k}f_i(x)$, then $\partial f(x)={\rm conv}{ \cup {\partial f_i(x)</td>
                  <td>f_i(x)=f(x)}}$</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>pointwise supremum :  $f(x)=\sup_{\alpha\in \mathcal{F} }f_\alpha(x)$, then $\partial f(x)={\rm closure}({\rm conv}{ \cup {\partial f_\alpha(x)</td>
                  <td>f_\alpha(x)=f(x)}})$</td>
                </tr>
              </tbody>
            </table>
          </li>
        </ul>
      </li>
      <li>negative subgradent is not necessarily descent direction (lack of continuity)
        <ul>
          <li>$f^{ {\rm best},t}:=\mathop \min\limits_{1\le i\le t} f(x^i)$</li>
          <li>$f^{\rm opt}:=\min_x f(x)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Lipschitz function : $</td>
          <td>f(x)-f(z)</td>
          <td>\le L_f|x-z|_2, \forall x,z$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Projected subgradient update：$x^{t+1}=\mathcal{P}_{\mathcal{C} }(x^t-\eta_tg^t)$</p>
  </li>
  <li>
    <p>majorizing function: $|x^{t+1}-x^<em>|_2^2\le|x^{t}-x^</em>|_2^2-2\eta_t(f(x^t)-f^{\rm opt})+\eta^2_t|g^t|_2^2$</p>

    <ul>
      <li>
        <p>Polyak’s stepsize rule: $\eta_t=\frac{f(x^t)-f^{\rm opt} }{|g^t|_2^2}\Rightarrow |x^{t+1}-x^<em>|_2^2\le|x^{t}-x^</em>|_2^2-\frac{(f(x^t)-f^{\rm opt})^2}{|g^t|_2^2}$</p>
      </li>
      <li>
        <blockquote>
          <p>必须知道$f^{\rm opt}$, 所以并不实用</p>
        </blockquote>
      </li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>problem</th>
      <th>algorithm</th>
      <th>stepsize rule</th>
      <th>convergence rate</th>
      <th>iteration complexity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>convex &amp; Lipschitz problem</td>
      <td>projected subgradient method</td>
      <td>$\eta_t=\frac{1}{\sqrt{t} }$</td>
      <td>$f^{ {\rm best}，t}\mathbin{\lower.3ex\hbox{$\buildrel&lt;\over{\smash{\scriptstyle\sim}\vphantom{_x} }$} } \frac{|x^0-x^*|_2^2+L_f\log t}{\sqrt{t} }$<br />$O(\frac{1}{\sqrt{t} })$</td>
      <td>$O(\frac{1}{\epsilon^2})$</td>
    </tr>
    <tr>
      <td>strongly convex &amp; Lipschitz problem</td>
      <td>projected subgradient method</td>
      <td>$\eta_t=\frac{2}{\mu(t+1)}$</td>
      <td>$f^{ {\rm best},t}-f^{\rm opt}\le \frac{2L_f^2}{\mu}\cdot\frac{1}{t+1}$<br />$O(\frac{1}{t})$</td>
      <td>$O(\frac{1}{\epsilon})$</td>
    </tr>
  </tbody>
</table>

<h2 id="proximal-gradient-methods">Proximal gradient methods</h2>

<ul>
  <li>
    <p>composite models : $\begin{array}{*{20}{c} }
{ {\rm{minimize} }_x}&amp;{F(x):=f(x)+h(x)}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{x\in R^n}&amp;{}
\end{array}$, f convex and smooth, h convex$F^{\rm opt}:=\min_x F(x)$</p>

    <ul>
      <li>$\mathcal{l}_1 $regularized minimization $\begin{array}{*{20}{c} }
{ {\rm{minimize} }_x}&amp;{f(x)+|x|_1}
\end{array}$,use $\mathcal{l}_1 $ regularization to promote sparsity.</li>
      <li>nuclear norm  regularized minimization $\begin{array}{<em>{20}{c} }
{ {\rm{minimize} }_x}&amp;{f(x)+|x|_</em>}
\end{array}$,use nuclear norm regularization to promote low-rank structure.</li>
    </ul>
  </li>
  <li>
    <p>Proximal gradient descent</p>

    <ul>
      <li>$x^{t+1}=\arg \min_x \left { f(x^t)+\langle \nabla f(x^t),x-x^t\rangle+\frac{1}{2\eta_t}|x-x^t|_2^2\right }$</li>
    </ul>
  </li>
  <li>
    <p>projected proximal gradient  descent</p>

    <ul>
      <li>
        <p>$x^{t+1}=\arg \min_x \left { f(x^t)+\langle \nabla f(x^t),x-x^t\rangle+\frac{1}{2\eta_t}|x-x^t|<em>2^2\right }+\mathbb{L}</em>\mathcal{C}(x)$</p>

        <p>​         $=\arg \min _x \left{ \frac{1}{2}|x-(x^t-\eta_t \nabla f(x^t))|_2^2+\eta_t c(x) \right}$</p>

        <p>where $\mathbb{L}_\mathcal{C}(x)=\left{ {\begin{array}{*{20}{c} }
  0&amp;{ {\rm{if\ } }x \in \mathcal{C} } \ 
  \infty &amp;{ {\text{else} } } 
\end{array} } \right.$</p>
      </li>
    </ul>
  </li>
  <li>
    <p>proximal operator</p>

    <ul>
      <li>
        <p>${\rm prox}_h(x):=\arg\min_z \left{ \frac{1}{2}|z-x|_2^2 +h(z)\right}$ , for any convex function $h$, may be non-smooth</p>
      </li>
      <li>
        <p>Projected GD update $x^{t+1}={\rm prox}<em>{\eta_t \mathbb{L}</em>\mathcal{C} }(x^t-\eta_t \nabla f(x^t))$</p>
      </li>
      <li>
        <p>accommodate more general $h$</p>

        <ol>
          <li>for $t=0,1,\cdots$ do</li>
          <li>$x^{t+1}={\rm prox}_{\eta_t h}(x^t-\eta_t \nabla f(x^t))$</li>
        </ol>
      </li>
      <li>
        <blockquote>
          <p>在一般环境下定义良好，比如非光滑凸函数；能够被常用的函数有效评估，比如正则化函数；这个概念很简单，覆盖了很多众所周知的优化算法</p>
        </blockquote>
      </li>
      <li>
        <p>basic rules</p>

        <ul>
          <li>if $f(x)=ag(x)+b$ with $a&gt;0$, then ${\rm prox}<em>f(x)={\rm prox}</em>{ag}(x)$</li>
          <li>affine addition: $f(x)=g(x)+a^Tx+b$ , then ${\rm prox}<em>f(x)={\rm prox}</em>{g}(x-a)$</li>
          <li>quadratic addition:  $f(x)=g(x)+\frac{\rho}{2}|x-a|<em>2^2$, then ${\rm prox}_f(x)={\rm prox}</em>{\frac{1}{1+\rho}g}(\frac{1}{1+\rho}x+\frac{\rho}{1+\rho}a)$</li>
          <li>scaling and translation : $f(x)=g(ax+b)$  with $a \ne 0$ , then ${\rm prox}<em>f(x)=\frac{1}{a}({\rm prox}</em>{a^2g}(ax+b)-b)$</li>
          <li>orthogonal mapping: $f(x)=g(Qx)$ with $Q^TQ=QQ^T=I$, then ${\rm prox}<em>f(x)=Q^T{\rm prox}</em>{g}(Qx)$</li>
          <li>orthogonal affine mapping:  $f(x)=g(Qx+b)$ with $QQ^T=\alpha^{-1}I$, then  ${\rm prox}<em>f(x)=(I=\alpha Q^TQ)x+\alpha Q^T({\rm prox}</em>{\alpha^{-1}g}(Qx+b)-b)$</li>
          <li>norm composition: $f(x)=g(|x|<em>2)$, then ${\rm prox}_f(x)={\rm prox}</em>{g}(|x|_2)\frac{x}{|x|_2}, \forall x \ne 0$</li>
        </ul>
      </li>
      <li>
        <p>firm nonexpansiveness(非膨胀性) $\langle {\rm prox}<em>h(x_1)-{\rm prox}</em>{h}(x_2),x_1-x_2\rangle \ge |{\rm prox}<em>h(x_1)-{\rm prox}</em>{h}(x_2)|^2_2$</p>
      </li>
      <li>
        <p>nonexpansiveness $|{\rm prox}<em>h(x_1)-{\rm prox}</em>{h}(x_2)|_2\le |x_1-x_2|_2$</p>
      </li>
      <li>
        <p>interpret prox via resolvant of subdifferential operator $z={\rm prox}_f(x) \Leftrightarrow z=(\mathcal{I}+ \partial f)^{-1}(x)$ , where $\mathcal{I} $ is identity mapping  ($\mathcal{I}(z)=z$)</p>
      </li>
      <li>
        <p><strong>Moreau decomposition</strong></p>

        <ul>
          <li>Suppose $f$ is closed convex, and $f^<em>(x):= \sup_z{ \langle x,z\rangle -f(z)}$ is <strong>convex conjugate</strong> of $f$, then $x={\rm prox}_f(x)+{\rm prox}_{f^</em>}(x)$</li>
        </ul>

        <blockquote>
          <p>proximal mapping 和 duality的关键联系</p>

          <p>generalization of orthogonal decomposition</p>
        </blockquote>

        <ul>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>$x=\mathcal{P}<em>\mathcal{K}(x)+\mathcal{P}</em>\mathcal{K^{\circ} }(x)$, where $K^{\circ}:={x</td>
                  <td>\lang x,z\rang \le 0, \forall z \in \mathcal{K}}$</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>
            <p>extended Moreau decomposition</p>

            <p>Suppose $f$ is closed convex and $\lambda &gt;0 $, then $x={\rm prox}<em>{\lambda f}(x)+\lambda {\rm prox}</em>{\frac{1}{\lambda}f^*}(x/\lambda)$</p>
          </li>
        </ul>
      </li>
      <li>
        <blockquote>
          <p>slides 中有很多函数的分解，还没仔细看</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>
    <p>Backtracking line search for proximal gradient methods</p>

    <p>Let $\tau_L(x):={\rm prox}_{\frac{1}{L}h}(x-\frac{1}{L}\nabla f(x))$</p>

    <ol>
      <li>Initialize $\eta=1,0&lt;\alpha\le 1/2,0&lt;\beta&lt;1$</li>
      <li>while $f(\tau_{L_t}(x^t))&gt;f(x^t)-\lang \nabla f(x^t),x^t-\tau_{L_t}(x^t)\rang+\frac{L_T}{2}|\tau_{L_t}(x^t)-x^t|_2^2$ do</li>
      <li>​     $L^t\leftarrow \frac{1}{\beta}L^t ({\rm or\ } \eta_t\leftarrow \beta \eta_t)$</li>
    </ol>

    <p>here, $\frac{1}{L_t}$ correspond to $\eta_t$, and $\tau_{L_t}(x^t)$ generalizes $x^{t+1}$</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>problem</th>
      <th>algorithm</th>
      <th>stepsize rule</th>
      <th>convergence rate</th>
      <th>iteration complexity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>convex &amp; smooth problem</td>
      <td>proximal GD</td>
      <td>$\eta_t=\frac{1}{L}$</td>
      <td>$F(x^t)-F^{\rm opt}\le \frac{L|x^0-x^*|_2^2}{2t}$<br />$O(\frac{1}{ {t} })$</td>
      <td>$O(\frac{1}{\epsilon})$</td>
    </tr>
    <tr>
      <td>strongly convex &amp; smooth problem</td>
      <td>projected subgradient method</td>
      <td>$\eta_t=\frac{1}{L}$</td>
      <td>$|x^t-x^<em>|_2^2\le \left( 1- \frac{\mu}{L}\right)^t |x^0-x^</em>|_2^2$<br />$O((1-\frac{1}{\kappa})^t)$</td>
      <td>$O(\kappa\log\frac{1}{\epsilon})$</td>
    </tr>
  </tbody>
</table>

<h2 id="accelerated-gradient-methods">Accelerated gradient methods</h2>

<blockquote>
  <p>问题：1. GD专注于每次迭代时改善损失，可能会目光短浅。2. GD可能有时候是锯齿形瞬变的</p>

  <p>解决方法：1. 从历史迭代中探索信息 2. 增加缓冲器（比如momentum）产生更平滑的轨迹</p>
</blockquote>

<ul>
  <li>
    <p>Heavy-ball method</p>

    <ul>
      <li>$x^{t+1}=x^{t}-\eta_t \nabla f(x^t)+\theta_t(x^t-x^{t-1})$</li>
      <li>add inertia to the “ball ” (i.e. include momentum term) to mitigate zigzagging</li>
      <li>System matrix$\left[ \begin{gathered}
  {x^{t + 1} } - {x^<em>} \hfill <br />
  {x^t} - {x^</em>} \hfill \ 
\end{gathered}  \right] = \left[ {\begin{array}{<em>{20}{c} }
  {(1 + \theta ){\mathbf{I} } - {\eta _t}\int_0^1 { {\nabla ^2}f({x_\tau }){\text{d} }\tau } }&amp;{ - {\theta _t}{\mathbf{I} } } \ 
  {\mathbf{I} }&amp;0 
\end{array} } \right]\left[ \begin{gathered}
  {x^t} - {x^</em>} \hfill <br />
  {x^{t - 1} } - {x^*} \hfill \ 
\end{gathered}  \right]$</li>
    </ul>
  </li>
  <li>
    <p>Nesterov’s accelerated gradient methods</p>

    <ul>
      <li>
        <p>$x^{t+1}=y^t-\eta_t\nabla f(y^t)$</p>

        <p>$y^{t+1}=x^{t+1}+\frac{t}{t+3}(x^{t+1}-x^t)$</p>
      </li>
      <li>
        <p>alternates between gradient updates and proper extrapolation (not a descent method(i.e. may not have $f(x^{t+1})\le f(x^t)$ ))</p>
      </li>
      <li>
        <p>one of most beautiful and mysterious results in optimization</p>
      </li>
      <li>
        <blockquote>
          <p>可以用微分方程解释，具体看slides</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>
    <p>FISTA (Fast iterative shrinkage-thresholding algorithm(快速迭代收缩阈值算法))</p>

    <ul>
      <li>
        <p>$x^{t+1}={\rm prox}_{\eta_t h}(y^t-\eta_t\nabla f(y^t))$</p>

        <p>$y^{t+1}=x^{t+1}+\frac{\theta_t-1}{\theta_{t+1} }(x^{t+1}-x^t)$</p>

        <p>where $y^0=x^0,\theta_0=1$ and $\theta_{t+1}=\frac{1+\sqrt{1+4\theta_t^2} }{2}$ (momentum coefficient)</p>
      </li>
      <li>
        <p>Rippling behavior: take $y^{t+1}=x^{t+1}+\frac{1-\sqrt{q} }{1+\sqrt{q} }(x^{t+1}-x^t)$ $q^*=1/\kappa$</p>

        <ul>
          <li>
            <p>when $q&gt;q^*$ : we underestimate momentum $\rightarrow$ slower convergence</p>
          </li>
          <li>
            <p>when $q&lt;q^*$ : we overestimate momentum ($\frac{1-\sqrt{q} }{1+\sqrt{q} }$ is large) $\rightarrow$ overshooting/ rippling behavior</p>
          </li>
          <li>
            <blockquote>
              <p>$q=q^<em>$是最好的，但是由于$u,L$很难计算，导致$\kappa,p^</em>$很难计算，所以实际使用时需要自己调试</p>
            </blockquote>
          </li>
        </ul>
      </li>
      <li>
        <p>Adaptive restart</p>

        <ul>
          <li>When certain criterion is met , restart running FISTA with $\begin{array}{*{20}{c} }
  { {x^0} \leftarrow {x^t} } \ 
  { {y^0} \leftarrow {x^t} } \ 
  { {\theta _0} \leftarrow 1} 
\end{array}$</li>
          <li>function scheme: restart when $f(x^t)&gt;f(x^{t-1})$</li>
          <li>gradient scheme: restart when $\lang\nabla f(y^{t-1}),x^t-x^{t-1}&gt;0\rang$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>problem</th>
      <th>algorithm</th>
      <th>stepsize rule</th>
      <th>convergence rate</th>
      <th>iteration complexity</th>
      <th> </th>
      <th> </th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>strong convex &amp; smooth problem</td>
      <td>Heavy-ball method</td>
      <td>$\eta_t=\frac{4}{(\sqrt{L}+\sqrt{\mu})^2}$<br />$\theta_t=\max{</td>
      <td>1-\sqrt{\eta_tL}</td>
      <td>,</td>
      <td>1-\sqrt{\eta_t\mu}</td>
      <td>}^2$</td>
      <td>$\left| \left[ \begin{gathered}  {x^{t + 1} } - {x^<em>} \hfill \  {x^t} - {x^</em>} \hfill \ \end{gathered}  \right] \right |_2 \leqslant {\left( {\frac{ {\sqrt \kappa   - 1} }{ {\sqrt \kappa   + 1} } } \right)^t}\left | \left[ \begin{gathered}{x^1} - {x^<em>} \hfill \  {x^0} - {x^</em>} \hfill \ \end{gathered}  \right]\right |_2$</td>
      <td>$O(\sqrt{\kappa}\log\frac{1}{\epsilon})$</td>
    </tr>
    <tr>
      <td>convex &amp; smooth problem</td>
      <td>Nesterov’s accelerated gradient methods</td>
      <td>$\eta_t=\frac{1}{L}$</td>
      <td>$f(x^t)-f^{\rm opt}\le  \frac{2L|x^0-x^*|_2^2}{(t+1)^2}$<br />$O(\frac{1}{t^2})$</td>
      <td>$O(\frac{1}{\sqrt{\epsilon} })$</td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>convex &amp; smooth problem</td>
      <td>FISTA</td>
      <td>$\eta_t=\frac{1}{L}$</td>
      <td>$F(x^t)-F^{\rm opt}\le  \frac{2L|x^0-x^*|_2^2}{(t+1)^2}$<br />$O(\frac{1}{t^2})$</td>
      <td>$O(\frac{1}{\sqrt{\epsilon} })$</td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>strong convex &amp; smooth problem</td>
      <td>FISTA</td>
      <td>$\eta_t=\frac{1}{L}$</td>
      <td>$F(x^t)-F^{\rm opt}\le\left( 1-\frac{1}{\sqrt{\kappa} }\right )^t\left( F(x^0)-F^{\rm opt}+\frac{\mu |x^0-x^*|_2^2}{2}\right ) $<br />$O((1-\frac{1}{\sqrt{\kappa} })^t)$</td>
      <td>$O(\sqrt{\kappa}\log\frac{1}{\epsilon})$</td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="smoothing-for-nonsmooth-optimization">Smoothing for nonsmooth optimization</h2>

<blockquote>
  <p>非光滑导致输出subgradient成为一个first order oracle (black box model), 所以所以不能提升收敛速度</p>

  <p>解决方法： approximate a nonsmooth objective  function by a smooth function</p>
</blockquote>

<ul>
  <li>
    <p>Smooth approximation</p>

    <p>A convex function $f$ is called $(\alpha,\beta)$-smoothable if , for any $\mu&gt;0,\exist$ convex function$f_\mu$ s.t.</p>

    <ul>
      <li>$f_\mu(x)\le f(x)\le f_\mu(x)+\beta\mu, \forall x$</li>
      <li>$f_\mu$ is $\frac{\alpha}{\mu}$-smooth</li>
    </ul>

    <p>here, $f_\mu$ is called $\frac{1}{\mu}$-smooth approximation of f with parameter $(\alpha,\beta)$, $\mu$ is tradeoff between approximation accuracy and smoothness</p>

    <ul>
      <li>Examples
        <ul>
          <li>$|x|<em>1 \Rightarrow f</em>\mu(x):=\sum\nolimits_{i = 1}^n { {h_\mu }({x_i})} ,{h_\mu }(z) = \left{ {\begin{array}{*{20}{c} }
  { {z^2}/2\mu }&amp;{ {\text{if } }|z|\le \mu} \ 
  {|z| - \mu /2}&amp;{ {\text{else} } } 
\end{array} } \right.$(Huber function)</li>
          <li>$|x|<em>2 \Rightarrow f</em>\mu(x):=\sqrt{|x|_2^2+\mu^2}-\mu$</li>
          <li>$\max_i{x_i}\Rightarrow f_\mu(x):=\mu \log (\sum\nolimits_{i = 1}^ne^{x^i/\mu})-\mu \log n$</li>
        </ul>
      </li>
      <li>basic rules
        <ul>
          <li>addition $\lambda_1f_1+\lambda_2 f_2 \Rightarrow \lambda_1f_{\mu,1}+\lambda_2 f_{\mu,2}$ with parameters $(\lambda_1\alpha_1+\lambda_2 \alpha_2,\lambda_1\beta_1+\lambda_2 \beta_2)$</li>
          <li>affine transformation  $h(Ax+b) \Rightarrow h_\mu(Ax+b)$ with parameters $(\alpha_1+\lambda_2 \alpha|A|^2,\beta)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Smoothing via Moreau envelope</p>

    <p>Moreau envelope (or Moreau-Yosida regularization) of convex function $f$ with parameter $\mu&gt;0$ is defined as</p>

    <p>$M_{\mu f}(x):=\inf_z\left{f(z)+\frac{1}{2\mu}|x-z|_2^2\right}$</p>

    <ul>
      <li>$M_{\mu f}$ is smoothed or regularized form of $f$</li>
      <li>minimizers of $f$=minimizers of $M_f$ $\Rightarrow$ minimizing $f$ and $M_f$ are equivalent</li>
      <li>Connection with proximal operater
        <ul>
          <li>$M_{\mu f}=f({\rm prox_{\mu f}(x)})+\frac{1}{2\mu}|x-{\rm prox_{\mu f}(x)}|$</li>
          <li>${\rm prox_{\mu f}(x)}=x-\mu\nabla M_{\mu f}(x)$</li>
        </ul>
      </li>
      <li>Properties of Moreau envelope
        <ul>
          <li>$M_{\mu f}$ is convex</li>
          <li>$M_{\mu f}$ is $\frac{1}{\mu}$-smooth</li>
          <li>if $f$ is $L_f$-Lipschitz, then $M_{\mu f}$ is $\frac{1}{\mu}$-smooth approximation of $f$ with parameters $(1,L_f^2/2)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Smoothing via conjugation</p>

    <p>Suppose $f=g^<em>$, namely, $f(x)=\mathop \sup \limits_{z}{\lang z,x\rang-g(z)}:=g^</em>(x)$</p>

    <p>$f_\mu(x)=\mathop \sup \limits_{z}{\lang z,x\rang-g(z)-\mu d(z)}=(g+\mu d)^*(x)$</p>

    <p>for some 1-strong convex and continuous function $d$ , called proximity function(近邻函数)</p>

    <ul>
      <li>Properties
        <ul>
          <li>$g+\mu d$ is $\mu$-strongly convex $\Rightarrow$ $f_\mu$ is$\frac{1}{\mu}$-smooth</li>
          <li>$f_\mu(x)\le f(x)\le f_\mu(x)+\mu D$ with $D:=\sup_xd(x)$ $\Rightarrow$ $f_\mu$ is$\frac{1}{\mu}$-smooth approximation of $f$ with parameters $(1,D)$
| problem                                               | algorithm | parameter                     | iteration complexity    |
| —————————————————– | ——— | —————————– | ———————– |
| $\frac{1}{\mu}$-smooth with $(\alpha, \beta)$ problem | FISTA     | $\mu=\frac{\epsilon}{2\beta}$ | $O(\frac{1}{\epsilon})$ |</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>iteration complexity 从subgradient的$O(\frac{1}{\epsilon^2})$提升到了$O(\frac{1}{\epsilon})$</p>
</blockquote>

<h2 id="dual-and-primal-dual-methods">Dual and primal-dual methods</h2>

<ul>
  <li>
    <p>Dual formulation $\begin{array}{<em>{20}{c} }
{\mathop {\rm{minimize} }\limits_{x} }&amp;{f(x)+h(Ax)}&amp;{}<br />
\end{array} \Rightarrow \begin{array}{</em>{20}{c} }
{\mathop {\rm{minimize} }\limits_{x,z} }&amp;{f(x)+h(z)}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{Ax=z}&amp;{}<br />
\end{array} \Rightarrow \begin{array}{*{20}{c} }
{ {\rm{maximize} }<em>\lambda \mathop \min \limits</em>{x,z} }&amp;{f(x)+h(z)+\lang \lambda, Ax-z\rang}&amp;{}
\end{array}$</p>

    <p>$\Rightarrow 
{ {\rm{maximize} }<em>\lambda \mathop \min \limits</em>{x,z} }{\lang A^T\lambda, x\rang+f(x)}+{\mathop \min \limits_{z} }{h(z)-\lang \lambda, z\rang} $</p>

    <p>$\Rightarrow \begin{array}{<em>{20}{c} }
{ {\rm{minimize} }_\lambda}&amp;{f^</em>(-A^T\lambda)-h^*(\lambda)}
\end{array}$</p>

    <ul>
      <li>primal : $\begin{array}{*{20}{c} }
{\mathop {\rm{minimize} }\limits_{x} }&amp;{f(x)+h(Ax)}&amp;{}<br />
\end{array}$</li>
      <li>dual: $\Rightarrow \begin{array}{<em>{20}{c} }
{ {\rm{minimize} }_\lambda}&amp;{f^</em>(-A^T\lambda)-h^*(\lambda)}
\end{array}$
        <ul>
          <li>if $f^*$ is smooth or strongly convex</li>
          <li>proximal operator w.r.t. h is cheap</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Dual proximal gradient algorithm</p>

    <ol>
      <li>for $t=0,1,\cdots$ do</li>
      <li>​      $\lambda^{t+1}={\rm prox}_{\eta_th^<em>}(\lambda^t+\eta_tA\nabla f^</em>(-A^T\lambda^t))$</li>
    </ol>
  </li>
  <li>
    <p>Primal representation of dual proximal gradient algorithm</p>

    <ol>
      <li>for $t=0,1,\cdots$ do</li>
      <li>​      $x^t=\arg \min_x{f(x)+\lang A^T\lambda^t, x\rang}$</li>
      <li>​       $\lambda^{t+1}=\lambda^t+\eta_tAx^t-\eta_t{\rm prox}_{\eta_t^{-1}h^*}(\eta_t^{-1}\lambda^t+Ax^t)$</li>
    </ol>
  </li>
  <li>
    <p>Accelerated dual proximal gradient algorithm</p>

    <ol>
      <li>for $t=0,1,\cdots$ do</li>
      <li>​      $\lambda^{t+1}={\rm prox}_{\eta_th^<em>}(w^t+\eta_tA\nabla f^</em>(-A^Tw^t))$</li>
      <li>​      $\theta_{t+1}=\frac{1+\sqrt{1+4\theta^3_t} }{2}$</li>
      <li>​       $w^{t+1}=\lambda^{t+1}+\frac{\theta_t-1}{\theta_{t+1} }(\lambda^{t+1}-\lambda^{t})$</li>
    </ol>
  </li>
  <li>
    <p>Primal representation of accelerated  dual proximal gradient algorithm</p>

    <ol>
      <li>for $t=0,1,\cdots$ do</li>
      <li>​    $x^t=\arg \min_x{f(x)+\lang A^T\lambda^t, x\rang}$</li>
      <li>​    $\lambda^{t+1}=w^t+\eta_tAx^t-\eta_t{\rm prox}_{\eta_t^{-1}h^*}(\eta_t^{-1}w^t+Ax^t)$</li>
      <li>​    $\theta_{t+1}=\frac{1+\sqrt{1+4\theta^3_t} }{2}$</li>
      <li>​     $w^{t+1}=\lambda^{t+1}+\frac{\theta_t-1}{\theta_{t+1} }(\lambda^{t+1}-\lambda^{t})$</li>
    </ol>
  </li>
  <li>
    <p>Primal-dual proximal gradient method</p>

    <p>$\begin{array}{*{20}{c} }
{\mathop {\rm{minimize} }\limits_{x} }&amp;{f(x)+h(Ax)}&amp;{}<br />
\end{array}$where $f$ and $h$ are closed and convex, both $f$ and $h$ might be non-smooth, both $f$ and $h$ admit inexpensive proximal operators</p>

    <blockquote>
      <p>我们仅仅已经讨论了proximal method (resp. dual proximal method) ,但是他们只能更新primal(resp. dual )变量。我们能利用${\rm prox}_f$ 和${\rm prox}_h$从而同时更新primal和dual变量吗？</p>
    </blockquote>

    <ul>
      <li>
        <p>Saddle-point problem ${\rm minimize}<em>x \max</em>\lambda f(x)+\lang \lambda ,Ax\rang-h^*(\lambda)$</p>

        <blockquote>
          <p>saddle points : $\forall x\in X,y\in Y, f(x^<em>,y)\le f(x^</em>,y^<em>)\le f(x,y^</em>)$</p>
        </blockquote>

        <ul>
          <li>
            <p>optimality condition $0 \in \left[ {\begin{array}{<em>{20}{c} }
  {}&amp;{ {A^T} } \ 
  { - A}&amp;{} 
\end{array} } \right]\left[ \begin{gathered}
  x \hfill <br />
  \lambda  \hfill \ 
\end{gathered}  \right] + \left[ \begin{gathered}
  \partial f(x) \hfill <br />
  \partial {h^</em>}(\lambda ) \hfill \ 
\end{gathered}  \right]: =\mathcal{F} (x,\lambda )$</p>
          </li>
          <li>
            <p>fixed-point iteration/resolvent iteration $x^{t+1}={(\mathcal{I}+\eta\mathcal{F})^{-1}(x^t)}$</p>

            <blockquote>
              <p>issue: 计算$(\mathcal{I}+\eta\mathcal{F})^{-1}$太难了</p>
            </blockquote>
          </li>
          <li>
            <p>$\mathcal{A}(x,\lambda):=\left[ {\begin{array}{<em>{20}{c} }
  {}&amp;{ {A^T} } \ 
  { - A}&amp;{} 
\end{array} } \right]\left[ \begin{gathered}
  x \hfill <br />
  \lambda  \hfill \ 
\end{gathered}  \right]$, $\mathcal{B}(x,\lambda):=\left[ \begin{gathered}
  \partial f(x) \hfill <br />
  \partial {h^</em>}(\lambda ) \hfill \ 
\end{gathered}  \right]$</p>

            <blockquote>
              <p>solution: 将$(\mathcal{I}+\eta\mathcal{F})^{-1}$拆分成$(\mathcal{I}+\eta\mathcal{A})^{-1}$和$(\mathcal{I}+\eta\mathcal{B})^{-1}$分别解线性问题和$\rm prox$</p>
            </blockquote>
          </li>
          <li>
            <p>operator splitting via Cayley operators</p>

            <p>Let $\mathcal{R}<em>\mathcal{A}:=(\mathcal{I}+\eta\mathcal{A})^{-1}$, $\mathcal{R}</em>\mathcal{B}:=(\mathcal{I}+\eta\mathcal{B})^{-1}$ be resolvents and $\mathcal{C}<em>\mathcal{A}:=(2\mathcal{R}</em>\mathcal{A}-\mathcal{I})$, $\mathcal{C}<em>\mathcal{B}:=(2\mathcal{R}</em>\mathcal{B}-\mathcal{I})$ be Cayley operators</p>

            <p>then</p>

            <p>$0\in \mathcal{A}(x)+\mathcal{B}(x) \Leftrightarrow \mathcal{C}<em>\mathcal{A}\mathcal{C}</em>\mathcal{B}(z)=z$ with $x=\mathcal{R}_\mathcal{B}(z)$</p>

            <blockquote>
              <p>这个的证明。。。没看懂</p>

              <p>issue： 怎么求解$\mathcal{C}<em>\mathcal{A}\mathcal{C}</em>\mathcal{B}(z)$呢 直接用$z^{t+1}=\mathcal{C}<em>\mathcal{A}\mathcal{C}</em>\mathcal{B}(z^t)$可能不会收敛</p>
            </blockquote>
          </li>
          <li>
            <p>Douglas-Rachford splitting（damped fixted-point iteration）</p>

            <p>$z^{t+1}=\frac{1}{2}(\mathcal{I}+\mathcal{C}<em>\mathcal{A}\mathcal{C}</em>\mathcal{B})(z^t)$</p>

            <blockquote>
              <p>expilcit expression (更清晰的阐释)</p>

              <p>$x^{t+\frac{1}{2} }=\mathcal{R}_\mathcal{B}(z^t)$</p>

              <p>$z^{t+\frac{1}{2} }=2x^{t+\frac{1}{2} }-z^t$</p>

              <p>$x^{t+1}=\mathcal{R}_\mathcal{A}(z^{t+\frac{1}{2} })$</p>

              <p>$z^{t+1}=\frac{1}{2}(z^t+2x^{t+1}-z^{t+\frac{1}{2} })=z^t+x^{t+1}-x^{t+\frac{1}{2} }$</p>

              <p>其中$x^{t+\frac{1}{2} },z^{t+\frac{1}{2} }$都是辅助变量</p>
            </blockquote>

            <p>applying Douglas-Rachford splitting to optimality condition</p>

            <p>$x^{t+\frac{1}{2} }={\rm prox}_{\eta f}(p^t)$</p>

            <p>$\lambda^{t+\frac{1}{2} }={\rm prox}_{\eta h^*}(q^t)$</p>

            <p>$\left[ \begin{gathered}
    {x^{t + 1} } \hfill <br />
    {\lambda ^{t + 1} } \hfill \ 
  \end{gathered}  \right] = {\left[ {\begin{array}{*{20}{c} }
    I&amp;{\mu {A^T} } \ 
    { - \eta A}&amp;I 
  \end{array} } \right]^{ - 1} }  \left[ \begin{gathered}
    2{x^{t + \frac{1}{2} } } - {p^t} \hfill <br />
    2{\lambda ^{t + \frac{1}{2} } } - {q^t} \hfill \ 
  \end{gathered}  \right]$</p>

            <p>$p^{t+1}=p^t+x^{t+1}-x^{t+\frac{1}{2} }$</p>

            <p>$q^{t+1}=q^t+\lambda^{t+1}-\lambda^{t+\frac{1}{2} }$</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="alternating-direction-method-of-multipliers-admm">Alternating direction method of multipliers (ADMM)</h2>

<ul>
  <li>
    <p>Two-block problem</p>

    <p>$\begin{array}{*{20}{c} }
{\mathop {\rm{minimize} }\limits_{x,z} }&amp;{F(x,z):=f_1(x)+f_2(z)}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{Ax+Bz=b}&amp;{}<br />
\end{array}$</p>

    <blockquote>
      <p>这也可以用Douglas-Rachford splitting求解</p>
    </blockquote>
  </li>
  <li>
    <p>Augmented Lagrangian method</p>

    <ul>
      <li>
        <p>Dual problem : ${\rm minimize}_\lambda f_1^<em>(-A^T\lambda)+f_2^</em>(-B^T\lambda)+\lang\lambda,b\rang$</p>
      </li>
      <li>
        <p>Proximal point method $\lambda^{t+1}=\arg \min_\lambda \left{ f_1^<em>(-A^T\lambda)+f_2^</em>(-B^T\lambda)+\lang\lambda,b\rang +\frac{1}{2\rho}|\lambda -\lambda^t|_2^2\right}$</p>
      </li>
      <li>
        <p>Augmented Lagrangian method (or method for multipliers)</p>

        <p>$ (x^{t+1},z^{t+1})=\arg \min_{x,z}\left{ f_1(x)+f_2(z)+\frac{\rho}{2}|Ax+Bz-b+\frac{1}{\rho}\lambda^t|_2^2\right} $ (primal step)</p>

        <p>$\lambda^{t+1}=\lambda^{t}+\rho(Ax^{t+1}+Bz^{t+1}-b)$   (dual step)</p>

        <blockquote>
          <p>primal step通常是expensive，就像解决原问题一样</p>

          <p>$x,z$的最小化不能分开进行</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>
    <p>Alternating direction method of multipliers</p>

    <p>$ x^{t+1}=\arg \min_{x}\left{ f_1(x)+\frac{\rho}{2}|Ax+Bz^t-b+\frac{1}{\rho}\lambda^t|_2^2\right} $</p>

    <p>$ z^{t+1}=\arg \min_{z}\left{f_2(z)+\frac{\rho}{2}|Ax^{t+1}+Bz-b+\frac{1}{\rho}\lambda^t|_2^2\right} $</p>

    <p>$\lambda^{t+1}=\lambda^{t}+\rho(Ax^{t+1}+Bz^{t+1}-b)$</p>

    <blockquote>
      <p>混合了对偶分解(dual decomposition )和增强拉格朗日( Augmented Lagrangian method)方法的好处</p>

      <p>$x,z$是接近对称的但是不是一个整体</p>
    </blockquote>
  </li>
  <li>
    <p>robust PCA</p>

    <p>$M=L$ (low-rank) $+S$(sparse)</p>

    <blockquote>
      <p>怎么将一个矩阵$M$分解成一个低秩矩阵和系数矩阵的叠加(superposition)</p>
    </blockquote>

    <ul>
      <li>
        <p>convex programing</p>

        <p>$\begin{array}{<em>{20}{c} }
{\mathop {\rm{minimize} }\limits_{L,S} }&amp;{|L|_</em>+\lambda|S|_1}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{L+S=M}&amp;{}<br />
\end{array}$</p>

        <table>
          <tbody>
            <tr>
              <td>where $|L|<em>*:=\sum\nolimits</em>{i = 1}^n { {\sigma <em>i}(L)} $ is nuclear norm, and $|S|_1:=\sum\nolimits</em>{i,j} {</td>
              <td>S_{i,j}</td>
              <td>} $ is enteywise $l_1$ norm</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>ADMM for solving</p>

        <p>$ L^{t+1}=\arg \min_{L}\left{ |L|_*+\frac{\rho}{2}|L+S^t-M+\frac{1}{\rho}\Lambda^t|_F^2\right} $</p>

        <p>$ S^{t+1}=\arg \min_{S}\left{\lambda|S|_1+\frac{\rho}{2}|L^{t+1}+S-M+\frac{1}{\rho}\Lambda^t|_F^2\right} $</p>

        <p>$\Lambda^{t+1}=\Lambda^{t}+\rho(L^{t+1}+S^{t+1}-M)$</p>

        <ul>
          <li>
            <p>is equivalent to</p>

            <p>$ L^{t+1}={\rm SVT}_{\rho^{-1} }\left(M-S^t-\frac{1}{\rho}\Lambda^t\right) $    (singular value thresholding)</p>

            <p>$ S^{t+1}={\rm ST}_{\lambda \rho^{-1} }\left(M-L^{t+1}-\frac{1}{\rho}\Lambda^t\right) $   (soft thresholding)</p>

            <p>$\Lambda^{t+1}=\Lambda^{t}+\rho(L^{t+1}+S^{t+1}-M)$</p>

            <p>where for any $X$ with ${\rm SVD} X=U \Sigma  V^T (\Sigma={\rm diag}({\sigma_i}))$, one has</p>

            <p>${\rm SVT}<em>{\tau}(X)=U{\rm diag}({(\sigma_i-\tau)</em>+})V^T$ and</p>

            <p>${({\text{S} }{ {\text{T} }<em>\tau }(X))</em>{i,j} } = \left{ {\begin{array}{*{20}{c} }
  { {X_{i,j} } - \tau }&amp;{ {\text{if } }{X_{i,j} } &gt; \tau } \ 
  0&amp;{ {\text{if |} }{X_{i,j} }| \leqslant \tau } \ 
  { {X_{i,j} } + \tau }&amp;{ {\text{if } }{X_{i,j} } &lt;  - \tau } 
\end{array} } \right.$</p>
          </li>
        </ul>
      </li>
      <li>
        <p>other examples</p>

        <ul>
          <li>graphical lasso $\begin{array}{*{20}{c} }
{\mathop {\rm{minimize} }\limits_{\Theta} }&amp;{-\log \det\Theta+\lang\Theta ,S\rang+\lambda|\Theta|_1}&amp;{}<br />
{ {\rm{subject\ to} } }&amp;{\Theta \succeq 0}&amp;{}<br />
\end{array}$(估计稀疏高斯图形模型)</li>
          <li>consensus optimization（共识优化）</li>
        </ul>
      </li>
      <li>
        <p>conbergence rate: $O(1/t)$</p>

        <p>iteration complexity $O(1/\epsilon)$</p>

        <blockquote>
          <p>收敛性证明部分抽空一步一步推导一下</p>
        </blockquote>
      </li>
    </ul>
  </li>
</ul>

<h2 id="quasi-newton-methods">Quasi-Newton methods</h2>

<ul>
  <li>Newton’s method
    <ul>
      <li>$x^{t+1}=x^t-(\nabla^2f(x^t))^{-1}\nabla f(x^t)$</li>
      <li>Quadratic convergence $O(\log\log\frac{1}{\epsilon})$</li>
    </ul>
  </li>
  <li>
    <p>Quasi-Newton method $x^{t+1}=x^t-\eta_tH_t\nabla f(x^t)$</p>
  </li>
  <li>
    <p>BFGS(Broyden-Fletcher-Goldfarb-Shanno) method</p>

    <ol>
      <li>
        <p>for $t=0,1,\cdots$ do</p>
      </li>
      <li>
        <p>​      $x^{t+1}=x^t-\eta_tH_t\nabla f(x^t)$ (line search to determin $\eta_t$)</p>
      </li>
      <li>
        <p>​      $H_{t+1}=(I-\rho_ts_ty_t^T)H_t(I-\rho_ty_ts_t^T)+\rho_ts_ts_t^T$</p>

        <p>where $s_t=x^{t+1}-x^t,y_t=\nabla f(x^{t+1})-\nabla f(x^{t}),\rho_t=\frac{1}{y_t^T s_t}$</p>
      </li>
    </ol>

    <ul>
      <li>iteration cost:  gradient method &lt;$O(n^2)$&lt;newton method $O(n^3)$</li>
      <li>no magic formula for initization; possinle choices: approximate inverse Hessian at $x^0$, or identity matrix.</li>
    </ul>
  </li>
  <li>
    <p>Computational complexity = iteration cost $\times$ iteration complexity</p>

    <p>iteration complexity : gradient method $O(\log(\frac{1}{\epsilon}))$ &gt;BFGS&gt;Newton method $O(\log\log(\frac{1}{\epsilon}))$</p>
  </li>
  <li>
    <p>L-BFGS (Limited-memory BFGS)</p>

    <p>$H_t^L=V_{t-1}^T\cdots V_{t-m}^TH^L_{t,0}V_{t-m}\cdots V_{t-1}$</p>

    <p>​            $+\rho_{t-m}V_{t-1}^T\cdots V_{t-m+1}^Ts_{t-m}s_{t-m}^TV_{t-m+1}\cdots V_{t-1}$</p>

    <p>​            $+\rho_{t-m+1}V_{t-1}^T\cdots V_{t-m+2}^Ts_{t-m+1}s_{t-m+1}^TV_{t-m+2}\cdots V_{t-1}$</p>

    <p>​             $+\cdots+\rho_{t-1}s_{t-1}s_{t-1}^T$</p>

    <p>​    where $V_t=(I-\rho_t y_ts_t^T)$</p>

    <ul>
      <li>can be computed recursively (递归的)</li>
      <li>intialization $H^L_{t,0}$ may vary from iteration to iteration.</li>
      <li>only needs to store ${(s_i,y_i)}_{t-m\le i&lt;t}$</li>
    </ul>
  </li>
</ul>

<h2 id="stochastic-gradient-methods">Stochastic gradient methods</h2>

<ul>
  <li>
    <p>stochastic approximation / stochastic gradient descent (SGD)</p>

    <p>$x^{t+1}=x^t-\eta_t g(x^t;\xi^t)$</p>

    <p>where $g(x^t;\xi^t)$ is unbiased estimate of $\nabla F(x^t)$, i.e. $\mathbb{E}[g(x^t;\xi^t)]=\nabla F(x^t)$</p>

    <ul>
      <li>$\nabla F(x^t)=0 \Rightarrow$ finding roots of $G(x):=\mathbb{E}[g(x^t;\xi^t)]$</li>
      <li>Examples
        <ul>
          <li>SGD for empirical risk minimization $x^{t+1}=x^t-\eta_t \nabla_xf_{i_t}(x^t;{a_i,y_i})$ (choose $i_t$ uniformly at random)</li>
          <li>temporal difference (TD) learning: Reinforcement learning studies Markov decision process (MDP) with unknown model.</li>
          <li>Q-learning: solve Bellman equation</li>
        </ul>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$\mathbb{E}[| g(x^t;\xi^t)|_2^2]\le \sigma_g^2+c_g |\nabla F (x)</td>
              <td>_2^2$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>
    <p>iterate averaging</p>

    <blockquote>
      <p>没看懂它推导出来的结果是用来干什么的，slides中说是减轻震荡和减少方差，但是我没看懂怎么减少的，下一个sildes应该会讲。</p>
    </blockquote>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>problem</th>
      <th>algorithm</th>
      <th>stepsize rule</th>
      <th>convergence rate</th>
      <th>iteration complexity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>strong convex &amp; smooth problem</td>
      <td>SGD</td>
      <td>$\eta_t&lt;\frac{1}{Lc_g}$</td>
      <td>$\mathbb{E}[F(x^t)-F(x^<em>)] \le \frac{\eta L \sigma_g^2}{2\mu}+(1-\eta \mu)^t(F(x^0)-F(x^</em>))$</td>
      <td>$O(\sqrt{\kappa}\log\frac{1}{\epsilon})$</td>
    </tr>
    <tr>
      <td>strongly convex problem</td>
      <td>SGD</td>
      <td>$\eta_t=\frac{\theta}{t+1}$ for $\theta &gt;\frac{1}{2\mu}$</td>
      <td>$\mathbb{E}[|x^t-x^<em>|_2^2] \le \frac{c_\theta}{t+1}$<br />where $c_\theta=\max{\frac{2\theta^2 \sigma_g^2}{2\mu\theta-1},|x^0-x^</em>|_2^2}$</td>
      <td>$O(\frac{1}{\sqrt{\epsilon} })$</td>
    </tr>
  </tbody>
</table>

<p>待整理：</p>

<ol>
  <li>norm</li>
  <li>线性收敛，log收敛速度,算法复杂度</li>
  <li></li>
</ol>

<p>$\max x_i \le f(\mathbf{x})\le \max x_i+\log n, f(\mathbf{x})=(\prod\nolimits_{i = 0}^n x_i )^{1/n}$</p>

<p>Gradient: $\nabla f(\mathbf{x})=[\frac{ {\delta f(\mathbf{x})} }{ {\delta x_1} } \cdots \frac{ {\delta f(\mathbf{x})} }{ {\delta x_n} }]^T\in R^n$</p>

<p>Hessian: $\nabla^2f(\mathbf{x})=\left (\frac{ {\delta^2 f(\mathbf{x})} }{ {\delta x_i\delta x_j} }\right )_{ij}\in R^{n\times n}$</p>

<p>Taylor series $f(\mathbf{x}+ \mathbf{\delta})=f(x)+\nabla f(\mathbf{x})^T\mathbf{\delta}+\frac{1}{2}\mathbf{\delta}^T\nabla^2f(\mathbf{x})\mathbf{\delta}+o(\left|\mathbf{\delta}\right|^2)$</p>

<table>
  <tbody>
    <tr>
      <td>$\left| I-\eta Q\right|=max{</td>
      <td>1-\eta\lambda_1(Q)</td>
      <td>,</td>
      <td>1-\eta\lambda_n(Q)</td>
      <td>}$</td>
    </tr>
  </tbody>
</table>

<p>w.r.t 关于，iff 当且仅当, a.k.a. 又名 resp. 分别的，分开的</p>

    </div>
<!--
    <div id="donate" >
        <a href="/donate/" style="text-decoration: none" title="点击直达打赏页面～" >
            <button>客官~对小店满意的话，<mark>赏个铜板</mark>再走啊🐼</button>
        </a>
    </div>
-->

    <div class="post-nav">
        
        <a href="/2019/11/07/%E5%8C%BA%E5%9D%97%E9%93%BE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-%E5%9F%BA%E4%BA%8EUbuntu/" class="pre"><i class="icon-post-pre"> 区块链环境搭建Ubuntu+Nodejs+truffle+testrpc+anaconda</i></a>
         
        <a href="/2019/11/13/%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%87%B8%E4%BC%98%E5%8C%96%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/" class="next">中科大凸优化视频笔记 <i
                class="icon-post-next"></i></a>
        
    </div>


    <!--valine comment function-->
    <script src="https://cdnjs.loli.net/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine/dist/Valine.min.js"></script>

    <p><br/>对本文有任何问题，请在下面评论：</p>
    <div id="valine_comment" class="fb_comments_container"></div>
    <script>
        var notify = 'false' === true;
        var verify = 'false.>' === true;
        var visitor = 'true.>' === true;

        new Valine({
            av: AV,
            el: '#valine_comment',
            notify: notify,
            verify: verify,
            // smiles_url: '/smiles',
            visitor: true,
            app_id: 'yPxItE7bwrQBFHzHIQIfFXCG-MdYXbMMI',
            app_key: 'rrUOPcAJyjIaA7iMYSIXepE8',
            placeholder: 'say something~~\n - 昵称 为你所要显示的评论的ID；\n - 邮箱 只有后台可见，仅供讨论过程中是否需要交流邮件而设置，请放心填写;\n - 网址 为你的评论的昵称的超链接。\n以上都可以不写，评论依然会显示……\n这里支持MarkDown语法哦！回复前可以预览一下。',
            avatar: 'retro',
        });
    </script>


</article>

            </div>
        </div>

        <div class="pure-u-1-4">
            <div id="sidebar">
                <div class="widget">
    <form id="search-form" class="search-form">
      <input id="query" type="text" placeholder="Search" class="search-form-input">
    </form>
</div>
<script type="text/javascript">
$(document).ready(function() {

    function find_posts(words, posts_data) {
        var matched_posts = [];

        posts_data.forEach(function(post) {
            var content = (new Array(post["title"], post["keywords"])).join(" ");
            words.forEach(function (word) {
                if (word === null || word === undefined || word == "") {
                    return;  // continue
                }
                var regex = new RegExp(word, 'i');
                if (content.match(regex) != null) {
                    matched_posts.push(post);
                }
            });
        });

        return matched_posts;
    }

    function show_posts(posts) {
        var html = '';
        if (posts.length > 0) {
            for (var i = 0; i < posts.length; i++) {
                post = posts[i];
                html += '<div class="post">';
                html += '<h2 class="post-title">';
                html += '<a href="' + post.url + '">' + post.title + '</a>';
                html += '</h2>';
                html += '<div class="post-meta"><i class="icon-post-date">';
                html += ' ' + post.date;
                html += '</i></div>';
                html += '<div class="post-content">' + post.summary + '</div>';
                html += '<p class="readmore">';
                html += '<a href="' + post.url + '">阅读更多</a>';
                html += '</p>';
                html += '</div>';
            }
        } else {
            html += "<h4>Whoops,您输入的关键词没有搜索结果，请更换关键词！ O(∩_∩)O~</h4>"
        }
        $('.content_container').html(html);
        $('.content_container').show();
    }

    $('#search-form').submit(function() {
        var query = $('#query').val();
        var words = query.split(" ")

        $('#query').blur().attr('disabled', true);
        $('.content_container').hide();

        $.ajax({
            url: "/assets/data/posts.json",
            dataType: "json",
            timeout: 6000,  // 6 sec
            success: function(data) {
                matched_posts = find_posts(words, data);
                show_posts(matched_posts);
            }
        });

        $('#query').blur().attr('disabled', false);

        return false;
    });
});
</script>

				
                <!-- <div class="widget">
    <div class="widget-title"><i class="icon-edit"> 心情随笔</i></div><ul></ul>
        我梦扬州,便想到扬州梦我。<br>

        幸会。<br><br>
<td style="text-align: center"><a style="color: slategrey"></a>

        微信公众号:

<a href="https://mp.weixin.qq.com/s/dCxGcuv5ngyR6U-uBYVI9Q"
                   style="color: darkorange;"
target="_blank" title="点击直达微信公众号～～"><u><b>璇珠杂俎</b></u></a>
            </td>
</div>
 -->

                <div class="widget">
    <div class="widget-title"><i class="icon-category"> 文章分类</i></div>
    <ul class="category-list">
      
        <li class="category-list-item">
          <a class="category-list-link" href="/category/#文献管理与信息分析">文献管理与信息分析 (13)</a>
        </li>
      
        <li class="category-list-item">
          <a class="category-list-link" href="/category/#GeneralLecture">GeneralLecture (3)</a>
        </li>
      
        <li class="category-list-item">
          <a class="category-list-link" href="/category/#PythonManuals">PythonManuals (2)</a>
        </li>
      
        <li class="category-list-item">
          <a class="category-list-link" href="/category/#PaperReview">PaperReview (1)</a>
        </li>
      
        <li class="category-list-item">
          <a class="category-list-link" href="/category/#OriginManuals">OriginManuals (2)</a>
        </li>
      
        <li class="category-list-item">
          <a class="category-list-link" href="/category/#LaTeXManuals">LaTeXManuals (3)</a>
        </li>
      
        <li class="category-list-item">
          <a class="category-list-link" href="/category/#ConvexOptimization">ConvexOptimization (5)</a>
        </li>
      
        <li class="category-list-item">
          <a class="category-list-link" href="/category/#LinuxManuals">LinuxManuals (1)</a>
        </li>
      
        <li class="category-list-item">
          <a class="category-list-link" href="/category/#BlockChainLearning">BlockChainLearning (1)</a>
        </li>
      
    </ul>
</div>


                <!--<div class="widget">
    <div class="widget-title"><i class="icon-star"> 推荐文章</i></div>
    <ul class="post-list">
        <li class="post-list-item"><a class="post-list-link" href="https://www.oukohou.wang/2018/10/11/S3FD-Single-Shot-Scale-invariant-Face-Detector/">S3FD:Single Shot Scale-invariant Face Detector</a></li>
    </ul>

    <ul class="post-list">
        <li class="post-list-item"><a class="post-list-link" href="https://www.oukohou.wang/2018/11/29/live2D_installation/">给自己的jekyll博客安装live2D看板娘</a></li>
    </ul>

    <ul class="post-list">
        <li class="post-list-item"><a class="post-list-link" href="https://www.oukohou.wang/2018/11/20/process_of_blog_setting-up/">博客搭建历程</a></li>
    </ul>

    <ul class="post-list">
        <li class="post-list-item"><a class="post-list-link" href="https://www.oukohou.wang/2018/12/06/autumn_poems/">寒事今牢落，人生亦有初</a></li>
    </ul>
</div>
-->
				<div class="widget">
    <div class="widget-title">标签云</div>
    <div class="tagcloud">
        
    
    
    <a href="/tags/#Essential Skill" rel="nofollow" style="font-size: 14pt; color: #444;">Essential Skill</a>
    
    
    <a href="/tags/#Science Paper" rel="nofollow" style="font-size: 18pt; color: #000;">Science Paper</a>
    
    
    <a href="/tags/#Essential and Professional Course" rel="nofollow" style="font-size: 14pt; color: #444;">Essential and Professional Course</a>
    
    
    <a href="/tags/#Python" rel="nofollow" style="font-size: 10pt; color: #888;">Python</a>
    
    
    <a href="/tags/#Efficient Application" rel="nofollow" style="font-size: 9.5pt; color: #888;">Efficient Application</a>
    
    
    <a href="/tags/#Broaden Horizons" rel="nofollow" style="font-size: 9pt; color: #999;">Broaden Horizons</a>
    
    
    <a href="/tags/#LaTeX" rel="nofollow" style="font-size: 10pt; color: #888;">LaTeX</a>
    
    
    <a href="/tags/#Comprehensive Quality" rel="nofollow" style="font-size: 9pt; color: #999;">Comprehensive Quality</a>
    
    
    <a href="/tags/#Linux" rel="nofollow" style="font-size: 10pt; color: #888;">Linux</a>
    
    
    <a href="/tags/#BlockChain" rel="nofollow" style="font-size: 9.5pt; color: #888;">BlockChain</a>
    
    </div>
</div>


                <!-- <div class="widget">
  <div class="widget-title"><i class="icon-code-hub"> 代码仓库</i></div>
  <ul class="codehub-list">
  
    <li class="codehub-list-item"><a href="" title="" target="_blank"></a></li>
  
  </ul>
</div>
 -->

                <!--<div class="widget">
  <div class="widget-title"><i class="icon-link"> 友情链接</i></div>
  <ul class="link-list">
  
    <li class="link-list-item"><a href="http://jekyllcn.com/" title="Jekyll blog system" target="_blank">Jekyll blog system</a></li>
  
  </ul>
</div>
-->

                <div class="widget post-toc hide">
    <div class="widget-title"><i class="icon-post-toc"> 文章目录</i></div><ul></ul>
</div>
<script type="text/javascript">
function TOCize(toc, content, matchHeightTo) {
    if (!(toc && content && matchHeightTo)) return false

    var cnt = 0;

    var make = function(tag) {
        return document.createElement(tag)
    }

    var aniscroll = {
        to: function(top) {
            aniscroll.target = top;
            if (aniscroll.running) return;
            aniscroll.running = setInterval(aniscroll.tick, 20);
        },
        target: 0,
        running: 0,
        getTop: function() {
            return window.scrollY || window.pageYOffset || document.documentElement.scrollTop;
        },
        setTop: function(value) {
            (window['scrollTo'] && window.scrollTo(window.scrollX, value))
        },
        tick: function() {
            var oldST = aniscroll.getTop(), newST = ~~((oldST + aniscroll.target) / 2);
            aniscroll.setTop(newST);
            if (aniscroll.getTop() < newST || Math.abs(newST - aniscroll.target) < 10) {
                aniscroll.setTop(aniscroll.target);
                clearInterval(aniscroll.running)
                aniscroll.running = 0
            }
        }
    }

    function scrollToHeader(header, hash, ev) {
        var y = header.getBoundingClientRect().top + aniscroll.getTop();
        if (window.history['pushState']) {
            window.history.pushState({}, header.textContent, "#" + hash);
            aniscroll.to(y);
            ev.preventDefault();
        } else {
            var y2 = aniscroll.getTop();
            setTimeout(function() {
                aniscroll.setTop(y2);
                aniscroll.to(y);
            }, 0);
        }
    }

    var generateLink = function(h) {
        var q = make('a');
        cnt++;
        var hash = h.getAttribute('id');
        if (!hash) {
            hash = ('generated-hash-' + cnt);
            h.setAttribute('id', hash);
        }
        q.textContent = h.textContent;
        q.setAttribute('href', '#' + hash );
        q.addEventListener('click', scrollToHeader.bind(this, h, hash), false);
        return q;
    };

    var hs = content.querySelectorAll('h1, h2, h3, h4, h5, h6');
    var cul = null, plevel = 1;
    var uls = [make('ul')];
    for (var i=0;i<hs.length;i++) {
        var level = +hs[i].tagName.substr(1);
        var hi = hs[i];
        var ti = make('li');
        ti.appendChild(generateLink(hi));
        if (plevel < level) {
            do {
                uls.push(make('ul'));
                uls[uls.length-2].appendChild(uls[uls.length-1]);
            } while (++plevel < level);
        } else if (plevel > level) {
            do {
                cul = uls.pop();
            } while (--plevel > level);
        }
        cul = uls[uls.length-1];
        cul.appendChild(ti);
    }
    while(true) {
        var chs = uls[0].children;
        if (chs.length == 1 && chs[0].tagName == 'UL')
            uls.shift();
        else
            break;
    }

    if (!cnt) return false;

    var scrolldummy=make('div');
    toc.appendChild(scrolldummy);
    toc.appendChild(uls[0]);
    toc.style.display = 'block';

    var maxHeightTOC = '';
    var ppc = document.querySelector('.post');
    var s1 = function(){
        var scrollTop=aniscroll.getTop(), dummyClientTop=scrolldummy.getBoundingClientRect().top,
            margin = 10,c,d; // c = dummyHeight, d = TOC.maxHeight (+'px')
        if ((c = -dummyClientTop + margin) < 0) c = 0;
        if (c) {
            var wh = window.innerHeight
                || document.documentElement.clientHeight
                || document.body.clientHeight,
            cbox = matchHeightTo.getBoundingClientRect(),
            vq = cbox.bottom - dummyClientTop - uls[0].offsetHeight;
            if (c>vq) c=vq;
            d = (wh - (margin<<1)) + 'px';
        } else {
            d = "";
        }
        if (d != maxHeightTOC) { //status lock.
            maxHeightTOC = d;
            if (d) {
                uls[0].setAttribute('style', 'max-height:' + d + '; width:' + (toc.offsetWidth-20) + "px" );
            } else {
                uls[0].setAttribute("style","");
            }
        }
        scrolldummy.style.height = (c+'px');
    };
    window.addEventListener('scroll', s1, false);
    window.addEventListener('resize', s1, false);
}

TOCize(
    document.querySelector('.post-toc'),
    document.querySelector('.post-content'),
    document.querySelector('.post')
);
</script>

            </div>
        </div>
    </div>


    <div id="footer">
        Copyright &copy; 2015-<span id="cur_year"></span> Albert - All Rights
        Reserved.
        Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> @ Github.com
        <script type="text/javascript">var cur_date = new Date();
        document.getElementById("cur_year").innerHTML = cur_date.getFullYear();</script>
        
    </div>

    <a id="rocket" href="#top" class="show"></a>
    <script src="/assets/js/totop.js" type="text/javascript"></script>
</div>

<!--<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
-->

<script type="text/javascript">
/* 鼠标点击特效 */
var a_idx = 0;
jQuery(document).ready(function($) {
    $("body").click(function(e) {
        var a = new Array("❤Yang❤","❤Zhan❤","❤Peng❤","❤欢迎你❤","❤加油❤","❤努力❤","❤奋斗❤","❤拼搏❤","❤Albert❤","❤聪明❤","❤机智❤","❤活泼❤","❤可爱❤");
        var $i = $("<span></span>").text(a[a_idx]);
        a_idx = (a_idx + 1) % a.length;
        var x = e.pageX,
        y = e.pageY;
        $i.css({
            "z-index": 999999999999999999999999999999999999999999999999999999999999999999999,
            "top": y - 20,
            "left": x,
            "position": "absolute",
            "font-weight": "bold",
            "color": "rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"
        });
        $("body").append($i);
        $i.animate({
            "top": y - 180,
            "opacity": 0
        },
        1500,
        function() {
            $i.remove();
        });
    });
});
</script>

<section ></section>
<script type='text/javascript'>
/* 鼠标环绕特效 */
!function () { function o(w, v, i) { return w.getAttribute(v) || i } function j(i) { return document.getElementsByTagName(i) } function l() { var i = j("script"), w = i.length, v = i[w - 1]; return { l: w, z: o(v, "zIndex", -1), o: o(v, "opacity", 0.5), c: o(v, "color", "0,0,0"), n: o(v, "count", 99) } } function k() { r = u.width = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth, n = u.height = window.innerHeight || document.documentElement.clientHeight || document.body.clientHeight } function b() { e.clearRect(0, 0, r, n); var w = [f].concat(t); var x, v, A, B, z, y; t.forEach(function (i) { i.x += i.xa, i.y += i.ya, i.xa *= i.x > r || i.x <0 ? -1 : 1, i.ya *=i.y>n || i.y <0 ? -1 : 1, e.fillRect(i.x - 0.5, i.y - 0.5, 1, 1); for (v=0; v < w.length; v++) { x = w[v]; if (i !== x &&null !== x.x &&null !== x.y) { B = i.x - x.x, z = i.y - x.y, y = B * B + z * z; y <x.max && (x=== f && y>= x.max / 2 &&(i.x -= 0.03 * B, i.y -= 0.03 * z), A = (x.max - y) / x.max, e.beginPath(), e.lineWidth = A / 2, e.strokeStyle = "rgba(" + s.c + "," + (A + 0.2) + ")", e.moveTo(i.x, i.y), e.lineTo(x.x, x.y), e.stroke()) } } w.splice(w.indexOf(i), 1) }), m(b) } var u = document.createElement("canvas"),  s = l(), c = "c_n" + s.l, e = u.getContext("2d"), r, n, m = window.requestAnimationFrame || window.webkitRequestAnimationFrame || window.mozRequestAnimationFrame || window.oRequestAnimationFrame || window.msRequestAnimationFrame || function (i) { window.setTimeout(i, 1000 / 45) }, a = Math.random, f = { x: null, y: null, max: 20000 }; u.id = c; u.className = "hidden-xs hidden-sm my_canvas"; u.style.cssText = "position:fixed;top:0;left:0;z-index:" + s.z + ";opacity:" + s.o; j("body")[0].appendChild(u); k(), window.onresize = k; window.onmousemove = function (i) { i = i || window.event, f.x = i.clientX, f.y = i.clientY }, window.onmouseout = function () { f.x = null, f.y = null }; for (var t = [], p = 0; s.n > p; p++) { var h = a() * r, g = a() * n, q = 2 * a() - 1, d = 2 * a() - 1; t.push({ x: h, y: g, xa: q, ya: d, max: 6000 }) } setTimeout(function () { b() }, 100) }();
</script>

</body>


</html>
